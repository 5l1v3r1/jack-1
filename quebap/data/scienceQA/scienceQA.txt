created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['application', 'performance', 'algorithms', 'algorithm', 'method', 'discussion', 'dataset', 'fisher', 'paper', 'simulation study', 'el', 'simulation', 'study', 'iris', 'procedure', 'section', 'concept', 'clustering algorithms']	Section 2 discusses necessary preliminaries related to the EL concept and introduces the proposed algorithm. Section 3 investigates the performance of the suggested procedure in a comprehensive simulation study where the proposed EL method is compared with other popular clustering algorithms. An illustrative showcase example is also provided with a discussion. Section 4 considers the application of the proposed algorithm to the celebrated Iris dataset (Anderson, 1935; Fisher, 1936). The paper concludes with a discussion provided in Section 5.	computational_statistics_&_data_analysis/S0167947312004458.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['association', 'value', 'case', 'membership', 'dataset', 'b', 'classes', 'class', 'degree', 'instance', 'multiple classes', 'range', 'indeterminacy']	When the output belongs to case a, then it is 100% sure that it belongs to a specific class, as for example 31st instance of iris dataset generates de-fuzzified value of 0.13, that indicates its 100% association with iris-setosa. But when output belongs to case b, then it lies in the indeterminacy range where the output membership value belongs to multiple classes with varying degree of membership.	applied_soft_computing/S1568494612003419.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['attributes', 'method', 'dataset', 'rate', 'number', 'growth rate', 'results', 'iris', 'growth', 'complexity', 'cut-points', 'experiment', 'precision', 'evolution']	The results of Experiment (1) on the Iris dataset can be seen from Table 1. High testing precision through evolution can be achieved. From Table 2 the testing precision increases 9.3% (99.4%–90.1%) comparing with the entropy-based method, whereas the total number of cut-points grows 7. It is worth noticing that the total growth rate is only 6.14% (7/114) and the average growth of the number of the four attributes in the Iris dataset is only 1.7, which can explain the little increase of the complexity.	computers_&_mathematics_with_applications/S0898122108005658.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['base', 'fig', 'training', 'component', 'dataset', 'training set', 'system', 'inference system', 'rule', 'iris', 'rule base', 'set', 'inference', 'indeterminacy', 'truth']	Rule base for neutrosophic truth component for Iris dataset is shown in Fig. 9.4.Using the training set available, designing of inference system for indeterminacy component is done as follows:	applied_soft_computing/S1568494612003419.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['blocks', 'training', 'classification', 'error', 'samples', 'classification problem', 'image', 'weka', 'features', 'mlp', 'problems', 'subset', 'digits', 'algorithm', 'attributes', 'process', 'dataset', 'ranking algorithm', 'software', 'gain', 'disorders', 'letter', 'classes', 'image segmentation', 'problem', 'worth', 'work', 'handwriting recognition', 'results', 'class', 'user', 'comparison', 'information gain', 'attribute', 'iris', 'page', 'patterns', 'handwriting', 'set', 'respect', 'liver', 'subsets', 'recognition', 'segmentation', 'parameters', 'hand', 'test', 'classification error', 'glass', 'svm', 'information', 'identification']	The algorithm was applied to the Iris dataset with 4 attributes, the Glass Identification, the Liver Disorders, the Page Blocks, the Image Segmentation and Letter Recognition datasets [1]. The Glass Identification dataset used has 6 classes, 9 attributes and 214 samples (class 6 was not considered as it has very few samples). The Liver Disorders dataset has 2 classes, 6 attributes and 345 samples. The Page Blocks dataset has 5 classes, 10 attributes and 5473 samples. Like the Iris dataset, all the previous datasets were divided into two subsets to get the training and test patterns. The Image Segmentation dataset has 7 classes, 19 attributes and 210 samples for training and 2100 samples for testing. On the other hand, the Digits Handwriting Recognition dataset has 10 classes, 16 attributes and 10,992 samples with a training dataset of 7494 samples and a testing set of 3498 samples. The Letter Recognition dataset has 27 classes, 16 attributes and 20,000 samples, generally 16,000 samples are for training and 4000 for testing, in this work only 30% of the training and testing samples were used. For the Image segmentation and Letter recognition datasets, 10 attributes were used and 5 features were utilized for the Letter recognition dataset. For selecting the more useful attributes, the ranking algorithm “InfoGainAttributeEval” of the software WEKA was used. This algorithm evaluates the worth of an attribute by measuring the information gain with respect to the class. Table 6 presents the classification results for the different problems and a comparison with a MLP, a SVM and a RBN, with the parameters specified in the same way as for the ETH80 subset classification problem. As can be seen, in all cases the proposed algorithm obtains the lowest classification error. Furthermore, the proposed algorithm does not manually set parameters so that the training process is very simple for the user.	neurocomputing/S0925231213010916.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['c2', 'c1', 'values', 'algorithm', 'dataset', 'runs', 'r', 'parameters', 'paper', 'aco', 'tables', 'needs', 'pso']	The original PSO, original ACO, PSO–ACO and FAPSO–ACO algorithms needs to determine the associated parameters such as γ1, γ2, ρ, a, r, D0, c1, c2, ωmin and ωmax. In this paper, the best values for the aforementioned parameters are γ1=γ2=1.0, ρ=.99, a=15, r=0.5, D0=10, c1=c2=2, NSwarm=10–15, ωmin=0.4 and ωmax=0.9 determined by 10 runs of the algorithm. For example, the mentioned parameters are determined as shown in Tables 4–7 for iris dataset.	applied_soft_computing/S1568494609000854.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['candidates', 'repository', 'table', 'effectiveness', 'discretization', 'attributes', 'method', 'dataset', 'machine learning', 'decision', 'paper', 'condition', 'classes', 'number', 'results', 'data', 'discretization method', 'attribute', 'iris', 'experimental results', 'uci', 'uci repository', 'objects', 'learning', 'section', 'cut-points', 'machine', 'databases', 'analysis', 'information', 'datasets']	In this section we present experimental results and analysis on two datasets to demonstrate the effectiveness of the discretization method. The data comes from the Iris dataset of UCI Repository of Machine Learning Databases [5] and paper [3]. The Iris dataset includes 150 objects which fall into three decision classes, 4 continuous condition attributes and the number of candidates of cut-points is 114. The information table from paper [3] includes 7 objects, 2 condition attributes, 1 decision attribute and the number of candidates of cut-points is 7.	computers_&_mathematics_with_applications/S0898122108005658.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['class', 'attributes', 'instances', 'dataset', 'plants', 'distribution', 'type', 'iris', 'plant', 'classes', 'partitioning', 'length', 'width']	Iris: Partitioning of Iris dataset attempts to classify the instances into different type of Iris plants. There are three classes exist, where each of them represents a type of Iris plant, namely, Iris Setosa, Iris Versicolour and Iris Virginica. Each class contains 50 instances with 4 attributes refer to the sepal length, sepal width, petal length and petal width. The class Setosa is linearly separable from the others, whereas the Versicolour and Virginica are linearly non-separable with overlapping distribution between these two classes.	applied_soft_computing/S1568494615003701.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['class', 'instances', 'instance', 'dataset', 'iris', 'type', 'plant', 'classes', 'features', 'width', 'length']	Iris dataset contains 3 classes of 50 instances each, where each class refers to a type of Iris plant. Each instance is characterized by four features, i.e., sepal length, sepal width, petal length, and petal width.	applied_soft_computing/S1568494614004906.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'disease detection', 'quality', 'cylinder', 'ca', 'dataset', 'machine learning', 'types', 'detection', 'cancer', 'range', 'sonar', 'test', 'analysis', 'breast', 'datasets', 'values', 'repository', 'label', 'connectionist', 'cell', 'measure', 'voice', 'record', 'uci', 'learning', 'machine', 'rocks', 'information', 'rock', 'wisconsin', 'disease', 'table', 'conditions', 'tuples', 'mass', 'instances', 'school', 'constituents', 'malignant', 'chemical analysis', 'iris', 'patterns', 'breast cancer', 'wine', 'computer', 'signals', 'people', 'computer science', 'row', 'benign', 'diagnostic', 'angles', 'image', 'features', 'measurements', 'classes', 'california', 'data', 'metal', 'parkinson', 'characteristics', 'individuals', 'science', 'column', 'university']	Five of the datasets are collected from UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science [35]. The datsets are from different damain and vary greatly in their characteristics. The Iris dataset comprises of 150 tuples belonging to three classes with four features describing the sepal and petal lengths and widths. This is the most structured dataset consisting of real values. The Wine dataset is meant for chemical analysis of wine quality for three types of wines. It has 13 features which are the chemical constituents of all the three types of wine. The Diagnostic Wisconsin Breast Cancer (WDBC) dataset has two classes such as Malignant, Benign. The features are characteristics of the cell nuclei collected from a digitized image of a fine needle aspirate test of a breast mass. The dataset comprises of 32 features of 569 instances. The Parkinson's disease detection dataset is composed of a range of biomedical voice measurements from 31 people. Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals. The data discriminates healthy people from those with Parkinson's disease. The Connectionist Bench (Sonar, Mines vs. Rocks) dataset is meant for classification of sonar signals obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The label associated with each record is a rock or a mine (metal cylinder). The dataset comprises of 60 features of 208 patterns.	karbala_international_journal_of_modern_science/S2405609X15300701.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'function', 'mlps', 'mlp', 'digits', 'algorithm', 'on-line learning', 'dataset', 'machine learning', 'intersection', 'lvq', 'canada', 'gradient', 'management', 'snap', 'natural', 'datasets', 'natural computing', 'neurons', 'repository', 'methods', 'neural network', 'algorithms', 'method', 'language', 'neural networks', 'problem', 'results', 'piecewise', 'layer', 'uci', 'learning', 'gradient descent', 'activation function', 'recognition', 'domain', 'machine', 'networks', 'environments', 'neurocomputing', 'concepts', 'paper', 'http', 'neuron', 'iris', 'conference', 'patterns', 'international', 'active network', 'recognition problem', 'joint', 'descent', 'portugal', 'drift', 'phrase', 'generalisation', 'journal', 'network', 'features', 'problems', 'ability', 'natural language', 'input', 'combination', 'activation', 'feature', 'pp', 'feature learning', 'supervised learning', 'computing']	A novel combination of the adaptive function neural network (ADFUNN) and on-line snap–drift learning is presented in this paper and applied to optical and pen-based recognition of handwritten digits [E. Alpaydin, F. Alimoglu for Optical Recognition of Handwritten Digits and E. Alpaydin, C. Kaynak for Pen-Based Recognition of Handwritten Digits http://www.ics.uci.edu/~mlearn/databases/optdigits/ http://www.ics.uci.edu/~mlearn/databases/pendigits/]. Snap–drift [S.W. Lee, D. Palmer-Brown, C.M. Roadknight, Performance-guided neural network for rapidly self-organising active network management (Invited Paper), Journal of Neurocomputing, 61C, 2004, pp. 5–20] employs the complementary concepts of common (intersection) feature learning (called snap) and LVQ (drift towards the input patterns) learning, and is a fast, unsupervised method suitable for on-line learning and non-stationary environments where new patterns are continually introduced. ADFUNN [M. Kang, D. Palmer-Brown, An adaptive function neural network (ADFUNN) for phrase recognition, in: The International Joint Conference on Neural Networks (IJCNN05), Montréal, Canada, 2005, D. Palmer-Brown, M. Kang, ADFUNN: An adaptive function neural network, in: The 7th International Conference on Adaptive and Natural Computing Algorithms (ICANNGA05), Coimbra, Portugal, 2005] is based on a linear piecewise neuron activation function that is modified by a novel gradient descent supervised learning algorithm. It has recently been applied to the Iris dataset, and a natural language phrase recognition problem, exhibiting impressive generalisation classification ability with no hidden neurons. The unsupervised single layer snap–drift is effective in extracting distinct features from the complex cursive-letter datasets, and the supervised single layer ADFUNN is capable of solving linearly inseparable problems rapidly. In combination within one network (SADFUNN), these two methods are more powerful and yet simpler than MLPs, at least on this problem domain. We experiment on SADFUNN with two handwritten digits datasets problems from the UCI Machine Learning repository. The problems are learned rapidly and higher generalisation results are achieved than with a MLP.	information_sciences/S0020025508001564.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'granularity', 'implementation', 'strategy', 'search tree', 'technique', 'effectiveness', 'extension', 'gene', 'algorithm', 'dataset', 'shapes', 'gene expression', 'levels', 'decision rules', 'model', 'basis', 'attribute', 'ratio', 'techniques', 'attribute reduction', 'storage', 'multi-criteria decision analysis', 'paradigm', 'analysis', 'datasets', 'values', 'bayesian model', 'rule', 'methods', 'correlation', 'algorithms', 'method', 'task', 'al', 'enumeration', 'square', 'parallelization', 'programming model', 'k-nearest neighbor', 'dna', 'class', 'random', 'sum', 'islam', 'classification algorithms', 'rough sets theory', 'balancing', 'distribution', 'random forest', 'domain', 'phase', 'cost', 'framework', 'genes', 'theory', 'expression', 'information', 'forest', 'neighbor', 'clusters', 'performance', 'big data', 'rough', 'microarray', 'high throughput', 'decision', 'approach', 'tree structure', 'bayesian', 'explosion', 'decision tree', 'calculation', 'sets', 'efficient algorithm', 'parallel implementation', 'selection', 'variances', 'experiments', 'reduction', 'search', 'tree', 'dna microarray', 'gene selection', 'multi-criteria decision', 'microarray data', 'rough sets', 'features', 'mining', 'knn', 'rules', 'structure', 'data', 'pruning', 'bicluster', 'concept', 'throughput', 'hadoop', 'mapreduce']	A parallel hierarchical attribute reduction method can be applied on big data to analyze the intended data more efficiently [14]. This model is able to mine decision rules under different levels of granularity. The proposed algorithms are implemented on Hadoop framework using MapReduce paradigm that can parallelize the data and task and efficiently deal with big data. Li et al. [15] have developed the Dominance-based Rough sets Approach which is an extension of classical rough sets theory, for selecting relevant features efficiently. It processes information with preference-ordered attribute domain and then can be used for multi-criteria decision analysis. Ayadi et al. [16] have used the concept of biclusters using DNA gene expression for microarray data. Initially, they have considered a new tree structure, called Modified Bicluster Enumeration Tree (MBET), on which biclusters are represented by the profile shapes of genes. In the next phase, they have proposed an algorithm called BiMine+ which uses a pruning rule to avoid both trivial biclusters and the combinatorial explosion of the search tree. The performance of BiMine+ is assessed on both synthetic and real DNA microarray datasets. An algorithm called MROSEFW-RF, based on MapReduce parallelization strategy, proposed by Triguero et al. [17]. This algorithm ensembles several highly scalable, re-processing and mining methods, that performs the balancing of class distribution, detects the cost relevant features and builds an appropriate Random Forest model. Islam et al. [18] have proposed a MapReduce based parallel gene selection method, that utilizes sampling techniques to reduce irrelevant genes by using Between-groups to Within-groups sum of square (BW) ratio. The BW ratio indicates the variances among gene expression values. After gene selection, it applies MRkNN technique to execute multiple kNN in parallel using MapReduce programming model. Finally, the effectiveness of the method is verified through extensive experiments using several real and synthetic datasets. Wang et al. [19] have proposed a new method for calculating correlation and introduced an efficient algorithm based on MapReduce to optimize storage and correlation calculation. This algorithm is used as a basis for optimizing high throughput molecular data (microarray data) correlation calculation. He et al. [20] have proposed the parallel implementation of several classification algorithms like k-nearest neighbor, Naive Bayesian model, and decision tree which are executed concurrently on various clusters using iris dataset.	knowledge-based_systems/S0950705115003366.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'hausdorff', 'strategy', 'human brain', 'belongingness', 'algorithm', 'dataset', 'machine learning', 'dbscan', 'domains', 'hyperboxes', 'discovery', 'terms', 'addition', 'brain', 'datasets', 'methods', 'method', 'al', 'task', 'problem', 'logic', 'result', 'system', 'learning', 'information granules', 'recognition', 'machine', 'image processing', 'framework', 'compare', 'information', 'degrees', 'noise', 'error', 'fuzzy sets', 'clustering algorithm', 'clustering', 'computer vision', 'hausdorff distance', 'approach', 'fuzzy logic', 'granules', 'processing', 'calculus', 'sets', 'set', 'classifiers', 'hierarchical structure', 'computer', 'vision', 'development', 'image', 'improvement', 'problems', 'order', 'topic', 'deep learning', 'point', 'structure', 'version', 'allocation', 'pattern recognition', 'science', 'concept', 'collaboration', 'hu', 'information science', 'distance']	Pedrycz et al. [7] considered the hyperboxes discovery and noise point problems in classification. The fundamental development strategy was orchestrated by set (interval) calculus and fuzzy sets. They formed the hyperboxes based on set calculus. Information granules were built in the unsupervised and supervised version of the DBSCAN clustering algorithm. Furthermore, Hausdorff distance [33,34] has been applied to solve the problem of allocation of degrees of belongingness in Pedrycz et al. [7]. The proposed method is applied in the different datasets in various domains [19]. In particular for classification on iris dataset, their method mostly showed superior in terms of error to other traditional classifiers [47–49] and hyperbox set-based methods [47–51]. Hong Hu et al. [22] proposed the granular system in order to consider the computer vision topic. The proposed method constructed by hybridizing the fuzzy logic and machine learning in order to create the granular system for deep learning. Hong Hu et al. [21] proposed method simulated the hierarchical structure of human brain and applied on the haze-free as an example. The result of proposed method represented some improvement for the task of haze-free compare to the He et al. [27] approach. Hong Hu et al. [21] method considered the deep learning concept in a new way by collaboration of machine learning and fuzzy logic. The proposed method can be extended for pattern recognition and image processing in addition to the brain science and information science under the framework of deep learning.	knowledge-based_systems/S0950705114004559.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'immune system', 'classification performance', 'technique', 'algorithm', 'dataset', 'tool', 'detection', 'artificial immune algorithm', 'virus', 'data classification', 'svm', 'analysis', 'datasets', 'artificial immune system', 'exploratory data analysis', 'neural network', 'algorithms', 'fields', 'classifier', 'research', 'al', 'results', 'system', 'maps', 'anomaly detection', 'security', 'web mining', 'recognition', 'use', 'numerical results', 'evolution', 'immune', 'application', 'performance', 'artificial intelligence', 'clustering', 'scheduling', 'credit', 'fisher', 'unsupervised clustering', 'instance', 'fault diagnosis', 'iris', 'classifiers', 'computer', 'immune algorithm', 'intelligence', 'body', 'antigen', 'network', 'data analysis', 'anns', 'problems', 'fault', 'exploratory', 'ability', 'mining', 'characteristic', 'visualization', 'immune algorithms', 'data', 'pattern recognition', 'diagnosis', 'computer security', 'antibody', 'human body', 'web', 'clustering analysis']	The common characteristic of the above various immunity-based algorithms is a naturally strong ability of antigen recognition and antibody evolution derived from the human body. The numerical results of former research showed that the artificial immune algorithms have been applied and developed in various fields, for instance, anomaly detection, computer security and virus detection, data classification and clustering, fault diagnosis, pattern recognition, scheduling and web mining [3,9,12]. For data classification and clustering, an artificial immune algorithm, clustering analysis and self-origination maps neural network are used to classify Fisher Iris dataset. The numerical results confirmed that an artificial immune algorithm is the most effective technique [33]. Subsequently, Timmis et al. [27] first proposed the artificial immune system as an unsupervised clustering tool, but also stressed its use as an exploratory data analysis and visualization technique for a dataset with four-dimension. They suggested that the artificial immune system would involve the application of the algorithm to more complex problems of higher dimensional datasets. Leung et al. [20] compared the classification performance of some classifiers (e.g., ANNs, SVM, etc.) against an artificial intelligence technique based on the natural immune system, named the simple artificial immune system (SAIS), through three credit datasets. They showed that the simple artificial immune system is a competitive classifier.	applied_soft_computing/S1568494611004261.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'performance', 'ionosphere', 'classification performance', 'classifier', 'attributes', 'rates', 'fisher', 'characteristic', 'al', 'rate', 'classes', 'number', 'study', 'iris', 'classifiers', 'reason', 'difference', 'hand', 'university', 'accuracy']	According to the study of Leung et al. [20], for the Johns Hopkins University Ionosphere dataset the primary reason to the lower accuracy rate of the SAIS classifier is that the number of attributes and classes of the dataset affects the classification performance. However, we find that both the AINE-based and SAIS classifiers have this common characteristic, in which their accuracy rates tend to decrease when the number of attributes and classes of the dataset increase, however, our classifier is still more accurate than the SAIS one. On the other hand, the accuracy rate of the AINE-based classifier for the Johns Hopkins University Ionosphere dataset is less accurate than that of the AIRS-based classifiers, but it outperforms them for the Pima Indians Diabetes dataset. For the Fisher Iris dataset the AINE-based and AIRS-based classifiers have no significant difference in the accuracy rate. Therefore, our study shows that the AINE-based classifier can rival the AIRS-based classifiers.	applied_soft_computing/S1568494611004261.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'type', 'classification problem', 'features', 'length', 'instances', 'dataset', 'purpose', 'plant', 'classes', 'problem', 'experimental study', 'class', 'study', 'iris', 'patterns', 'width', 'cm']	The iris dataset is a three-class classification problem. The original dataset contained 3 classes of 50 instances each, where each class refers to a type of iris plant: Setosa, Versicolor, and Virginica. One class is linearly separable from the other two; the latter are not linearly separable from each other. The four features are (in cm): sepal length, sepal width, petal length, and petal width. For the purpose of the experimental study here, datasets 1, 2 and 3 each consisted of 50 patterns.	applied_soft_computing/S1568494608000380.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['classification', 'values', 'length', 'attributes', 'instances', 'dataset', 'plant', 'goal', 'classes', 'class', 'species', 'iris', 'characteristics', 'attribute', 'thickness']	IRIS dataset: This is the most popular and simple classification dataset based on multivariate characteristics of a plant species (length and thickness of its petal and sepal) divided into three distinct classes (Iris Setosa, Iris Versicolorand Iris Virginica) of 50 instances each. One class is linearly separable from other two; the later are not linearly separable from each other. In a nutshell, it has 150 instances and 5 attributes. Out of 5 attributes four attributes are predicting attributes and one is goal attribute. All the predicting attributes are real values.	computer_science_review/S1574013708000361.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['cluster analysis', 'classification', 'authors', 'methods', 'measurements', 'cluster', 'dataset', 'plants', 'literature', 'clustering methods', 'iris', 'examples', 'supervised classification', 'analysis', 'comparisons', 'datasets']	In comparisons of cluster analysis methods in the literature, authors often use datasets for which there is a given “true classification”. Often these are standard examples for supervised classification such as Fisher’s famous Iris dataset in which there are measurements on 150 Iris plants from three different subspecies. Clustering methods can generate clusterings ignoring the true classification to which they then can be compared.	pattern_recognition_letters/S0167865515001269.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['cluster analysis', 'clusters', 'body', 'repository', 'census', 'block', 'population density', 'cover', 'instances', 'dataset', 'cluster', 'machine learning', 'uc', 'wise', 'classes', 'distributed data', 'us', 'population', 'data', 'bureau', 'metropolitan', 'statistical', 'attribute', 'iris', 'area', 'set', 'water', 'points', 'deviation', 'learning', 'machine', 'areas', 'msa', 'density', 'analysis', 'column', 'standard deviation', 'datasets']	The analysis was run for five different datasets. The first data is the popular Iris dataset from UC Irvine Machine Learning Repository (UCIMLR) [4]. Attribute one of the data was used for clustering. The data having 150 points was classed into 5 clusters. The second data is US census block wise population data for the Metropolitan Statistical Area (MSA) of St. George Utah. The population, land area and water body area data was downloaded from the US Census Bureau website [11]. The area was calculated by summing and water and land areas for the census block. The population density for each census block was estimated by dividing population for the block with the area for the block. The data having 1450 points was clustered along population density into 10 clusters. The third data was Abalone dataset from UCIMLR [8]. Attribute 5 was used for clustering. The data has 4177 instances and was clustered into 25 classes. The fourth set of data was cloud cover data downloaded from Phillipe Collard [3]. Data in column 3 was used for cluster analysis. The data having 1024 points was clustered in 50 clusters. The fifth data set was randomly generated normally distributed data with mean 10 and standard deviation of 1. The data having 10,000 points was clustered into 100 clusters.	applied_soft_computing/S1568494612003377.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['cluster analysis', 'multiple experts', 'error', 'samples', 'measurement error', 'algorithms', 'case', 'problems', 'clustering', 'attributes', 'cluster', 'dataset', 'heterogeneity', 'fisher', 'results', 'sample', 'data', 'instance', 'flower', 'experts', 'species', 'iris', 'k-means', 'characteristics', 'set', 'dissimilarity', 'matrices', 'use', 'matrix', 'context', 'differences', 'analysis', 'clustering algorithms', 'datasets']	For many problems, only one dissimilarity matrix is available. For instance the Iris dataset (Fisher, 1936), one of the most popular datasets used in cluster analysis, consists of 150 samples from each of three species of Iris flowers where each flower is measured on four characteristics. Using the attributes measured, the flowers are typically cluster in the three (expected) species. The use of classical clustering algorithms (e.g. k-means, single-linkage, complete-linkage) on this dataset can provide excellent results. It is however possible that more than one dissimilarity matrix is available. In the context of the Iris data set, one could envision asking a sample of multiple experts to measure the same flowers, in case there has been significant measurement error. If there is heterogeneity in the data reported, we argue that aggregating the dissimilarity matrices might mask differences truly present in the data.	european_journal_of_operational_research/S0377221716301618.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clustering algorithm', 'cluster validity', 'image', 'tests', 'rgb', 'case', 'clustering', 'algorithm', 'method', 'cluster', 'dataset', 'types', 'fcm', 'results', 'data', 'grayscale', 'fischer', 'iris', 'concept', 'images', 'validity']	We realized different tests to different types of data. We started with the famous Fischer's Iris dataset then we tested the method to simple case of synthetic grayscale image and finally to digital RGB images. All results are compared with results of the FCM clustering algorithm using the same concept of cluster validity.	applied_soft_computing/S1568494615000563.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clustering analysis', 'classification', 'function', 'immune system', 'quality', 'fuzzy classifiers', 'classification techniques', 'technique', 'onset', 'pancreas', 'process', 'support', 'algorithm', 'rates', 'dataset', 'tool', 'aco', 'levels', 'base', 'person', 'resistance', 'ga', 'model', 'ant colony optimization', 'fuzzy model', 'discovery', 'range', 'particle', 'abc', 'techniques', 'particle swarm', 'particle swarm optimization', 'automatic diagnosis', 'experiences', 'gaussian', 'analysis', 'diabetes', 'computation', 'applications', 'input–output', 'abc algorithm', 'health', 'type', 'rule', 'methods', 'stage', 'classification rules', 'neural network', 'algorithms', 'wireless sensor network', 'classifier', 'load', 'regression', 'method', 'incidence', 'eas', 'literature', 'al', 'clustering method', 'swarm', 'neural networks', 'logic', 'pso', 'insulin', 'result', 'class', 'results', 'pso algorithm', 'numerical data', 'decision support systems', 'system', 'ants', 'reason', 'data mining', 'use', 'optimization algorithm', 'machine', 'definitions', 'hand', 'framework', 'networks', 'hormone', 'exploration', 'genetic algorithm', 'application', 'performance', 'numbers', 'space', 'evolutionary algorithms', 'disease', 'fuzzy sets', 'knowledge', 'insulin resistance', 'bee', 'function optimization', 'decision', 'paper', 'cognition', 'approach', 'rule base', 'fuzzy logic', 'wireless sensor', 'genetic', 'fuzzy classification', 'sets', 'iris', 'experiments', 'ant colony', 'classification system', 'classifiers', 'hybrid genetic algorithm', 'search space', 'blood glucose', 'decision support', 'local search', 'classification rule', 'symbolic', 'parameters', 'symbolic regression', 'beta', 'search', 'exploitation', 'candidate', 'body', 'authors', 'glucose', 'network', 'encoding', 'optimization', 'efforts', 'disorder', 'mutation', 'ant colony optimization algorithm', 'problems', 'metabolism', 'alternative', 'mining', 'rules', 'classes', 'order', 'fuzzy classifier', 'structure', 'data', 'interpretability', 'sensor', 'cells', 'machine learning techniques', 'colony', 'efficiency', 'diagnosis', 'systems', 'crossover', 'balance', 'fuzzy rules', 'decision making process', 'blood', 'wireless']	Diabetes is a complex and complicated disease characterized by either lack of insulin or a resistance to insulin, a hormone which is crucial for metabolism of blood sugar. In a healthy person, the pancreas produces insulin to help metabolize sugar in the blood and maintain blood glucose (sugar) levels within their normal range. Diabetics are unable to produce insulin or are resistant to insulin, and consequently cannot remove glucose from the bloodstream. Whether there is inadequate insulin or insulin resistance, glucose levels in the blood increase and cause severe health problems. The classification of diabetes includes two main clinical classes: Type 1 diabetes (previously called “juvenile diabetes”) is an autoimmune disorder in which the insulin producing beta cells are destroyed by the body's immune system. As a result the body is unable to produce insulin, Type 2 diabetes (previously called “adult onset diabetes”); in this type insulin is produced in insufficient amounts and/or cannot be used by the body to control blood sugar levels [1]. Therefore the need to detect and treat diabetes becomes obvious to reduce its incidence and costly associated metabolic disease. For this reason, in recent times, many machine learning techniques have been considered to design automatic diagnosis system for diabetes. This paper specifically focuses on the use of fuzzy modeling method to detect medical problems which relies on discovering human comprehensible knowledge. Fuzzy logic is originally proposed by Zadeh [2], who aimed to improve a classification and decision support systems by using fuzzy sets to define the overlapping class definitions. The application of fuzzy “if-then” rules also improves the interpretability of the results and provides more insight cognition into the classifier structure and the decision making process [3]. The performance of fuzzy classifier system depends on the “if-then” rules and their numbers that are generated from numerical data or human experiences. More rules might enhance the classification result but increase the computation load. Many intelligent methods, such as heuristic approaches [4], neural networks [5–7] and clustering method [8–10] have been proposed for building optimal fuzzy classifiers but in recent years Evolutionary Algorithms (EAs) have been widely used to optimize fuzzy classifiers. In the literature several EAs like Genetic Algorithm (GA), particle swarm optimization (PSO) and ant colony optimization (ACO) have been proposed to produce fuzzy classification system. Ganji et al. [11] used ACO algorithm to generate fuzzy classification rules called FSC-ANTMINER. They have used artificial ants in order to explore the search space and gradually make candidate fuzzy rules. Other works have adopted GA to optimize fuzzy classifiers [12–14]. In [15] Sanie et al. used a hybrid genetic algorithm to produce fuzzy rules and boosted it with an Ant Colony Optimization (ACO) heuristic based on local search to improve the quality of their final classification system. In [16] the authors proposed a fuzzy modeling framework able to generate automatically a rule base through a two stage genetic based search. On the other hand, PSO based approaches have proved their efficiency when optimizing fuzzy classification system. In [17] Sousa et al. proposed a first application of PSO as a new tool for classification rule discovery. Holden and Freitas [18] used a hybrid Particle Swarm Optimization and Ant Colony Optimization algorithm for discovering classification rules in data mining. Rani and Deepa [19] proposed a particle swarm optimization approach for optimal design of fuzzy classifier called PSOFLC. Experiments are performed on IRIS dataset, where the proposed technique is compared to two well-known classification techniques, including Genetic Fuzzy Classifier and Gaussian Fuzzy Classifier. The framework used for designing the fuzzy model from the available input–output data through PSO algorithm was proposed in [20] and the detailed encoding method was also provided. Khosla et al. [21] compared the computational efforts of PSO and GA using the similar method in [20]. Recently, a new Artificial Bee Colony algorithm (ABC) [22] was proposed as an alternative of the traditional Evolutionary Algorithms. The ABC algorithm is very simple and flexible when compared to other swarm based algorithms such as Particle swarm optimization. It does not require external parameters like mutation and crossover rates, which are hard to be defined in prior. The algorithm combines local search method with global one and tries to reach a certain balance between exploration and exploitation [22]. The ABC algorithm has been successfully applied to a wide range of applications such as clustering analysis [23], neural network training [24], function optimization [25,26], Wireless Sensor Network [36,37] and Symbolic Regression [38].	computer_methods_and_programs_in_biomedicine/S0169260713002447.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clustering', 'clusters', 'recognition', 'instances', 'dataset', 'cluster', 'iris', 'pattern recognition']	The Iris dataset is a famous dataset widely used in pattern recognition and clustering. It is a 4-attributes dataset containing 150 instances; it has three clusters each has 50 instances. One cluster is linearly separable from the other two and the latter two are not exactly linearly separable from each other [10].	knowledge-based_systems/S0950705113003535.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'classification', 'aspect', 'type', 'clustering algorithm', 'algorithm', 'instances', 'dataset', 'real-world', 'plant', 'order', 'classes', 'class', 'data', 'attribute', 'iris', 'set', 'scenario', 'reason', 'forward']	In order to test the algorithm in a real-world scenario, the well-known Iris dataset was considered. The data set contains 3 classes of 50 instances, each class referring to a type of iris plant. This dataset is appropriate for rather testing classification, but it was preferred for clustering too because the class attribute is given and hence there is a straight forward way to evaluate the algorithm. So apparently it would be ideal for the algorithm to produce 3 clusters of 50 instances each, the 3 clusters corresponding to the given 3 classes. But another reason for choosing this dataset is the fact that two of the classes are not linearly separable and a good clustering algorithm should sense this aspect.	procedia_computer_science/S187705091100620X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'classification', 'k-means algorithm', 'dissimilarity measure', 'gaussian mixture models', 'c-means', 'fuzzy c-means algorithm', 'technique', 'advance', 'algorithms', 'problems', 'clustering', 'algorithm', 'method', 'gac', 'dataset', 'dbscan', 'gaussian mixture', 'fcm', 'models', 'number', 'em', 'results', 'expectation-maximization', 'measure', 'classification problems', 'fuzzy c-means', 'k-means', 'uci', 'dissimilarity', 'mixture', 'experiment', 'density', 'analysis', 'clustering technique', 'datasets']	In the second experiment, we solve seven sub-datasets of the UCI datasets [16] classification problems using our method. The results are compared with those of the fuzzy C-means algorithm (FCM), a modified K-means algorithm using the manifold-distance-based dissimilarity measure (DSKM) [17], the genetic-algorithm-based clustering technique (GAC) proposed by Maulik and Bandyopadhyay [18], DBSCAN algorithm based on density proposed by Martin Ester [12], and expectation-maximization clustering based on Gaussian mixture models (EM) [19]. Then we make IRIS dataset classification problems by our method as the typical analysis. In all the algorithms, the desired number of clusters was set in advance.	applied_soft_computing/S1568494613004201.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'cluster validity index', 'index', 'values', 'cluster validity', 'db', 'clustering', 'structures', 'cluster', 'dataset', 'runs', 'finding', 'liu', 'number', 'compactness', 'chen', 'results', 'result', 'data', 'iris', 'behavior', 'researchers', 'indexes', 'reason', 'c', 'data structures', 'validity', 'difference', 'separation', 'indices', 'visual clustering']	The obtained results were tested and analyzed for the Iris dataset using the six indexes mentioned before. The average results of 10 runs are reported in Figs. 4.6 and 4.7. Finally, the results obtained are compared with the corresponding results reported by the other researchers [10,12]. According to [10], the optimal number of clusters found for the Iris data is 3, which ranks second for all the indexes except S-Dbw and C index (see Figs. 4.6 and 4.7). This finding is consistent with the result of the DB cluster validity index published by Cole [12]. The reason that these clusters are not the best is that the good values of the six indices indicate “good” clustering, which includes properly combined compactness and separation. Clusters are more compact but less separate from each other for the number of clusters taken as 3, while clusters with number of clusters taken as 2 are better separated. The visual clustering results given by Chen and Liu [10] show this difference clearly. The C index is likely to be data dependent and the behavior of the index may change when different data structures are used as reported in [18].	knowledge-based_systems/S0950705113003535.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'data', 'recognition', 'instances', 'dataset', 'machine', 'iris', 'pattern', 'literature', 'set', 'pattern recognition', 'uci', 'features']	The Iris dataset is a famous data set in the pattern recognition literature and can be obtained from the UCI machine learning website11http://www.ics.uci.edu/∼mlearn/Machine-Learning.html.. Iris has 150 instances, 4 features and 3 clusters.	applied_soft_computing/S1568494614004797.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'metrics', 'k-means algorithm', 'multi-center', 'global consistency', 'problems', 'clustering', 'convex', 'structures', 'gac', 'cluster', 'dataset', 'dbscan', 'algorithm', 'ability', 'fcm', 'distance metrics', 'representatives', 'em', 'structure', 'class', 'assignment', 'iris', 'page', 'ratio', 'center', 'k-means', 'consistency', 'wine', 'distribution', 'hand', 'balance', 'comparisons', 'breast', 'distance', 'datasets']	From Table 3, we can see clearly that the MFCM-TCSC did best on six out of the seven problems, while FCM did best only on the IRIS dataset. DBSCAN, EM and GAC only obtained the desired clustering for the two spheroid datasets, IRIS and Page. This is because the structure of the other five datasets does not satisfy convex distribution. On the other hand, the MFCM-TCSC can successfully recognize these complex clusters, and DSKM also obtained the true clustering on four problems, iris, zoo, balance and page, which indicates that the manifold distance metrics and the multi-center clustering are very suitable to measure complicated clustering structures. In comparisons between the MFCM-TCSC and DSKM, the MFCM-TCSC obtained the true clustering on wine and breast. Furthermore, for the Pimaindians problems, the MFCM-TCSC did a little better than DSKM in the clustering correct ratio. The main drawback of DSKM is that it has to recalculate the geometrical center of each cluster with the K-means algorithm after cluster assignment, which reduces its ability to reflect global consistency. The MFCM-TCSC avoids this drawback by the multi-centers of each class representatives.	applied_soft_computing/S1568494613004201.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'performance', 'space', 'improvement', 'length', 'solution', 'dataset', 'models', 'number', 'pso', 'execution', 'iteration', 'results', 'model', 'study', 'iris', 'performances', 'experimental results', 'search space', 'hybrid', 'iterations', 'search']	The length of a solution is proportional to the number of clusters. A large number of clusters complicates the search space, and more execution iteration is required to obtain a satisfying solution. The experimental results in this study are based on the performances at iteration 500 for the ACOR, the PSO, or the ACOR–PSO hybridized models. However, the clustering performance can be improved by increasing iterations. For example, for the Iris dataset, the performance using 10 clusters is 0.0941 at iteration 500 for the Hybrid III model. When the iteration increases to 1000, its performance is 0.0797, which is an improvement of approximately 15.3%.	applied_soft_computing/S1568494613001580.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'space', 'repository', 'methods', 'value', 'group', 'method', 'cluster', 'dataset', 'purpose', 'paper', 'effect', 'parameter', 'outliers', 'california', 'time', 'visualization', 'results', 'result', 'k', 'matter', 'component', 'components', 'principal component', 'uci', 'wine', 'objects', 'ins', 'acc', 'lof', 'machine', 'principal component analysis', 'analysis', 'natural', 'principal components', 'university', 'datasets']	We also applied the proposed method to two real-world-datasets. The wine dataset contains 113 objects that was group into 3 clusters. The iris dataset contains 106 objects that were grouped into 3 clusters too. Both datasets are obtained from the University of California, Irvine (UCI) machine learning repository. In this paper, we select two clusters among the three clusters as normal objects and select 6 objects from the remaining cluster as outliers. Since both of the datasets are multi-dimensional, we employed principal component analysis for the purpose of visualization. Each dataset was plotted on the space of two principal components. After computing the outlierness scores using the three methods, we find out 6 suspicious objects and compute the ACC and RP. The results are shown in Table 2. From the results of Table 2, we can see that the Natural Value is 20 and 15, and the Acc = 1 and RP = 1 of NOF is the highest for Wine and Iris datasets. For Wine dataset, LOF can get the best result too, Acc = 1 and RP = 1, when k = 50. But it must use more time than k = 20 (Natural Value). For Iris dataset, when the value of k is small the effect of LOF is bad, but the effect of INS is bad when the value of k is large. No matter LOF or INS get the best results that Acc = 1 and RP = 1 when the value of parameter(k) is Natural Value.	knowledge-based_systems/S0950705115004013.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'values', 'index', 'cluster validity', 'sd', 'db', 'subclasses', 'solutions', 'solution', 'cluster', 'method', 'dataset', 'approach', 'literature', 'number', 'work', 'result', 'results', 'data', 'iris', 'cancer', 'leukemia', 'silhouette', 'researchers', 'indexes', 'c', 'validity', 'indices', 'analysis']	The Ruspini dataset clustering result shows that four is the optimal number of clusters reported by all the cluster validity analysis indexes. This is consistent with earlier results, e.g., [56]. The Iris dataset gives similar result with the solutions of having the number of clusters two as the best solution and 3 clusters as the second best solution; both values are acceptable and have been reported by other researchers separately. According to the work described in [27], Fig2data has 10 clusters. The proposed approach gave the same result using the C index clustering validity method. The utilized cancer data has 15 clusters according to the result reported in [58]. MOKGA produces the same result using the DB index. The optimal number of clusters of the Leukemia dataset as agreed upon in the literature is 2 or 3 (with subclasses). Fortunately, MOKGA reported the same results using Dunn, DB, SD, and Silhouette indices.	knowledge-based_systems/S0950705113003535.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['clusters', 'values', 'repository', 'c-means', 'methods', 'algorithm', 'attributes', 'method', 'instances', 'dataset', 'benchmark', 'classes', 'number', 'covariance matrices', 'heart', 'iris', 'characteristics', 'uci', 'procedure', 'uci repository', 'coefficients', 'section', 'matrices', 'experiment', 'databases', 'covariance', 'datasets']	As a final comparative experiment in this section, we considered five classical benchmark datasets from the UCI repository of databases, whose characteristics are summarized in Table 4. As in [26], we only considered in the dataset Ecoli the classes ‘im’, ‘pp’ and ‘imU’, which are close and hard to separate. As before, the EK-NNclus algorithm was compared to the c-means algorithm (with the correct number of clusters), the pdfCluster algorithm, and the model-based method with and without contraints on the covariance matrices. For the model-based methods, the number of clusters was searched between 2 and 10. The pdfCluster algorithm was not applied to the Heart dataset, because it contains discrete attributes that cannot be handled by this procedure. Our algorithm was run 100 times with the following values of the coefficients: K=40,q=0.9, except for the Iris dataset, which has fewer instances, and for which we set K=30.	knowledge-based_systems/S0950705115003111.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['cmc', 'function', 'wisconsin', 'evolutionary algorithms', 'evaluations', 'algorithms', 'algorithm', 'changes', 'dataset', 'aco', 'number', 'sa', 'pso', 'simulation results', 'results', 'vowel', 'ga', 'simulation', 'fitness function', 'iris', 'k-means', 'cancer', 'swarms', 'breast cancer', 'variables', 'wine', 'ts', 'algorithmic', 'terms', 'parameters', 'glass', 'words', 'breast']	In terms of the number of function evaluations, k-means needs the least number of function evaluations, but the results are less than satisfactory. For the iris dataset, the number of function evaluations of PSO–ACO–K, PSO–ACO, PSO, ACO, SA, TS, GA, HBMO, PSO–SA, ACO–SA and k-means are 2468, 2523, 4953, 4931, 5314, 20201, 38128, 11214, 2566, 3629 and 120, respectively. The number of function evaluations of FAPSO–ACO–K for Wine, CMC, Vowel, Wisconsin breast cancer and Ripley's glass are 6315, 6868, 3487, 3492 and 6503, respectively. These results show that the number of function evaluations of FAPSO–ACO–K and PSO–ACO are less than those of other evolutionary algorithms. Based on the obtained simulation results, we can conclude that the changes of the number of fitness function evaluations of the proposed algorithm are less than other evolutionary algorithms for all cases. In the other words, the number of swarms in the FAPSO–ACO–K algorithm does not depend on the number of variables greatly. In the proposed algorithm, NSwarm for iris, wine, CMC, vowel, Wisconsin breast cancer and Ripley's glass is 10, 12, 12, 10, 10 and 14, respectively. Algorithmic parameters for all algorithms are illustrated in Table 18.	applied_soft_computing/S1568494609000854.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['cmc', 'gsa', 'cs', 'algorithms', 'proposal', 'dataset', 'black', 'aco', 'er', 'pso', 'results', 'vowel', 'black hole', 'iris', 'k-means', 'hole', 'wine', 'glass', 'datasets']	Table 12 illustrates the results of our algorithms on different datasets. The ER results of our proposal is compared with Black hole, ACO, PSO, GSA, CS and K-means and have been discussed in Table 13. As is obvious from Table 13 on Iris dataset, our proposal ER is 9.81 and 11.53. Also, on Wine dataset, our results are 6.18 and 9.24 and are better than other algorithms results and in Vowel, Glass and CMC datasets are also better than others.	applied_soft_computing/S1568494615007875.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['cmc', 'values', 'function', 'wisconsin', 'clustering algorithm', 'tables', 'methods', 'value', 'algorithms', 'clustering', 'solutions', 'algorithm', 'solution', 'global solution', 'dataset', 'runs', 'aco', 'sa', 'pso', 'simulation results', 'results', 'vowel', 'instance', 'simulation', 'ga', 'data', 'fitness function', 'word', 'iris', 'k-means', 'set', 'average', 'cancer', 'wine', 'partition', 'points', 'deviation', 'ts', 'standard', 'fitness', 'compare', 'breast cancer', 'glass', 'breast', 'standard deviation']	The simulation results given in Tables 8–17 show that FAPSO–ACO–K is very precise. In other word, it provides the optimum value and small standard deviation in compare to those of obtained by the other methods. For instance, the results obtained on the iris dataset show that FAPSO–ACO–K converges to the global optimum of 96.6500 in all of runs and PSO–ACO reaches to 96.6542 at almost times while the best solutions of PSO, ACO, SA, TS, GA, HBMO, PSO–SA, ACO–SA and K-means are 96.8942, 96.853, 97.4573, 97.365977, 113.986503, 96.752047, 96.66, 96.6602 and 97.333, respectively. The standard deviation of the fitness function for this algorithm, PSO–ACO and PSO–SA are 0, 0.009764 and 0.008, respectively, which they significantly are smaller than other methods. Table 9 shows the results of algorithms on the wine dataset. The optimum value is 16295.31, which is obtained in all the runs of FAPSO–ACO–K algorithm. Noticeably other algorithms fail to attain this value even once within 100 runs. Table 10 provides the results of algorithms on the CMC dataset. As seen from the results, the FAPSO–ACO–K algorithm are far superior those of obtained by the others. For the vowel data set, the best global solution, the worst global solution, the average and the standard deviation of the FAPSO–ACO–K are 148976.0005, 148976.0010, 148976.0100 and 0.001 respectively. For the PSO–ACO algorithm they are 148976.01, 149101.68, 148995.2032 and 24.5420931, respectively. The results of the FAPSO–ACO–K and the PSO–ACO algorithms are much better than those of other algorithms. On Wisconsin breast cancer dataset results given in Table 12, show that the FAPSO–ACO–K provides the optimum value of 2964.25 while the PSO–ACO, PSO, ACO and k-means algorithms obtain 2,964.38, 2,973.50, 2,970.49 and 2,987.19, respectively. The FAPSO–ACO–K was able to find the optimum in all of runs. Finally, Table 17 shows the best, average, worst and standard deviation values obtained by algorithms for Ripley's glass dataset. It is found that the FAPSO–ACO–K clustering algorithm is able to provide the same partition of the data points in all the runs.	applied_soft_computing/S1568494609000854.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['colon cancer', 'clusters', 'prototypes', 'error', 'function', 'possibilistic c-means', 'validation', 'c-means', 'methods', 'membership', 'effectiveness', 'gene', 'initialization', 'method', 'dataset', 'cluster', 'lung cancer', 'gene expression', 'paper', 'lung', 'order', 'checkerboard', 'time', 'number', 'superiority', 'iteration', 'functions', 'kernel functions', 'fuzzy c-means', 'cancer', 'iris', 'techniques', 'kernel', 'wine', 'running time', 'cluster validation', 'term', 'lymphoma', 'databases', 'matrix', 'genes', 'addition', 'membership function', 'subtypes', 'iterations', 'colon', 'expression']	This paper is introduced robust fuzzy possibilistic c-means techniques based on the membership function of fuzzy c-means, possibilistic c-means, kernel functions, and neighborhood term for finding subtypes of cancers in high dimensional gene expression cancer databases. In addition this paper is introduced prototypes initialization method to avoid more number of iteration. In order to establish the effectiveness of the proposed methods, this paper demonstrated experimental works on Synthetic dataset, Checkerboard dataset, IRIS dataset, and Wine Dataset. This paper has reported the superiority of the proposed methods through cluster validation, error matrix, running time, number of iterations and well separated clusters in clustering similar expression of genes in Lymphoma Cancer, Colon Cancer, and Lung Cancer databases.	applied_soft_computing/S1568494615003427.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['colon cancer', 'dual', 'performance', 'methods', 'core', 'algorithms', 'kfcm', 'dataset', 'lung cancer', 'processor', 'lung', 'checkerboard', 'results', 'iris', 'cancer', 'description', 'database', 'wine', 'intel', 'section', 'lymphoma', 'databases', 'colon', 'workstation', 'hex']	To evaluate the performance of proposed methods, this section implements the proposed methods with 2-dimensional dataset, Wine Dataset, IRIS Dataset, Checkerboard Dataset. Finally, the proposed methods have been successfully implemented with Lymphoma Cancer Database [2], Colon Cancer Database [1], Lung Cancer Database [16]. The description of the databases is listed in Table 1. The algorithms executed with workstation [HP Z800 INTEL Xeon HEX (6) Dual Core Processor]. The results of proposed methods are compared with the FPCM [43], KFCM [8], and KPFCM [35], TFPCM, NTFPCM.	applied_soft_computing/S1568494615003427.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['colon cancer', 'methods', 'speed', 'image', 'convergence speed', 'dataset', 'lung cancer', 'paper', 'lung', 'checkerboard', 'number', 'superiority', 'results', 'convergence', 'iris', 'cancer', 'wine', 'lymphoma', 'centers', 'iterations', 'colon', 'accuracy']	From the results on synthetic image, Checkerboard dataset, IRIS dataset, Wine Dataset, Lymphoma Cancer, Colon Cancer, and Lung Cancer databases, this paper proves the superiority of the proposed methods via convergence speed of optimal centers, number of iterations, and clustering accuracy.	applied_soft_computing/S1568494615003427.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['dataset', 'fitness', 'repository', 'iris', 'rules', 'uci', 'uci repository']	To demonstrate how the fitness metric works, a simple example of analysing extracted rules is included here. The dataset used is Fisher’s Iris dataset from the UCI repository [1].	artificial_intelligence_in_medicine/S0933365703000563.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['dataset', 'model', 'iris', 'mathematical model', 'x2']	The mathematical model obtained by our model for Iris dataset presented below.PD11=[12.265,−27.379,2.7124,−5.0129,16.636,2.6568]*poly(x1,x2),PD51=[3.4633,−0.61437,−4.7075,3.5712,0.32404,−0.37042]*poly(x2,x4),y= [0.02099x4+0.62292PD11+0.33345PD51+0.10778];	applied_soft_computing/S1568494610003200.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['datasets', 'fig', 'performance', 'network', 'variation', 'value', 'dataset', 'models', 'measure', 'model', 'iris', 'performances', 'version', 'non-determinism']	Unsurprisingly, Table 2 clearly shows that the overall performances of the network over real-world datasets are worse than artificial datasets for all three models. Besides, the real-world datasets produce more variation in clustering performance and are particularly worse for the gamma measure. Additionally, when comparing amongst the models, the deterministic version performed well and non-determinism appeared to be of limited value. A good performance by the deterministic model is illustrated for the IRIS dataset in Fig. 11.	applied_soft_computing/S1568494601000187.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['datasets', 'fuzzy system', 'evaluation', 'algorithms', 'comparative analysis', 'dataset', 'data', 'ga', 'system', 'dimension', 'means', 'wine', 'genetic fuzzy system', 'systems', 'pid', 'analysis', 'glass', 'accuracy', 'fuzzy']	The comparative analysis of the adaptive genetic fuzzy system is specified in Table 11. By means of four different datasets, evaluation has been made with the present four fuzzy genetic systems. Compared with other four algorithms in glass and PID datasets from the comparative analysis, the anticipated system attained improved accuracy. In glass dataset, AGFS obtained 86.05% accurateness but, the existing fuzzy GA [4] attained only 79.39%. Likewise, AGFS generated accuracy of 89.80% while the existing fuzzy-GA attained only 89.74%. In Wine and iris dataset, the existing seems good, since the AGFS is more suitable for high dimensional data when compared with less dimension data.	applied_soft_computing/S1568494614004852.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['datasets', 'studies', 'ionosphere', 'classifier', 'rates', 'attributes', 'instances', 'dataset', 'fisher', 'benchmark', 'al', 'classes', 'iris', 'classifiers', 'diabetes', 'information', 'university', 'accuracy']	The AINE-based classifier is compared with the accuracy rates of other three AIS-based classifiers for three well-known benchmark datasets, namely “Fisher Iris”, “Johns Hopkins University Ionosphere” and “Pima Indians Diabetes” [1], which are obtained from the studies of Leung et al. [20] and Watkins et al. [32]. Each information of datasets is as follows: The Fisher Iris dataset has 150 instances with 4 numerical attributes and 3 classes (Setosa, Versicolour and Virginica). The Johns Hopkins University Ionosphere dataset has 351 instances with 34 numerical attributes and 2 classes (good and bad). The Pima Indians Diabetes dataset has 768 instances with 8 numerical attributes and 2 classes (positive and negative).	applied_soft_computing/S1568494611004261.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['deviations', 'algorithm', 'dataset', 'ranks', 'model', 'sdp', 'iris', 'average', 'performances', 'ls', 'terms', 'ch', 'glass', 'cspa', 'rank', 'datasets']	As shown in Table 2, CE-GMDH(LS) outperforms CE-GMDH(CSPA) and CE-GMDH(SDP) for most datasets, while CE-GMDH(CSPA) and CE-GMDH(SDP) outperform CE-GMDH(LS) for some datasets, such as the Glass dataset and the Iris dataset. Performances of CE-GMDH(LS) corresponding to different datasets are better than or comparable with those of CE-GMDH(CSPA) and CE-GMDH(SDP) in terms of the average and the corresponding standard deviations of CH, RI and NMI. To determine which model is the best, follow [60], we computed the average rank of each algorithm. For example, the ranks of CE-GMDH(LS) over Glass dataset on CH, RI and NMI are 2, 1 and 1, respectively. The average rank of CE-GMDH(LS) is 1.33. Similarly, the average ranks of CE-GMDH(CSPA) and CE-GMDH(SDP) are 2.67 and 2, respectively. CE-GMDH(LS) also outperforms CE-GMDH(CSPA) and CE-GMDH(SDP) over Glass dataset. Finally, the average ranks of CE-GMDH(LS), CE-GMDH(CSPA) and CE-GMDH(SDP) over the 10 datasets are 1.27, 2.20 and 2.19.	applied_soft_computing/S156849461630031X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['elements', 'index', 'magnesium', 'implementation', 'repository', 'samples', 'type', 'total', 'iron', 'refractive index', 'containers', 'features', 'tests', 'length', 'alcohol', 'intensity', 'sodium', 'silicon', 'dataset', 'input', 'aluminum', 'order', 'color', 'proline', 'classes', 'potassium', 'constituents', 'class', 'acid', 'phenols', 'experimental tests', 'flower', 'windows', 'iris', 'uci', 'wine', 'width', 'variables', 'float', 'calcium', 'ash', 'curve', 'locations', 'vehicle', 'glass', 'hue', 'identification', 'datasets']	For experimental tests on the first implementation, four datasets were used: iris, wine, glass, available from the UCI dataset repository [8], and a 5th order polynomial curve. Where the iris dataset, has 4 input features (petal length, petal width, sepal length, and sepal width), and 3 outputs (iris setosa, iris virginica, and iris versicolor). With 50 samples of each flower type, with a total of 150 elements in the dataset. The wine dataset, with 13 input features of different constituents (Alcohol, malic acid, ash, alcalinity of ash, magnesium, total phenols, flavanoids, nonflavanoid phenols, proanthocyanins, color intensity, hue, OD280/OD315 of diluted wines, and proline) identifying 3 distinct Italian locations where the wine came from. With 59, 71, and 48 elements respectively in each class, for a total of 178 elements in the whole dataset. The glass identification dataset, has 9 input variables (refractive index, sodium, magnesium, aluminum, silicon, potassium, calcium, barium, and iron), and 7 classes (building windows float processed, building windows non float processed, vehicle windows float processed, containers, tableware, and headlamps). With 70, 76, 17, 13, 9, and 29 elements respectively in each class, for a total of 214 elements in the whole dataset.	applied_soft_computing/S1568494614002646.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['end', 'training', 'feature selection', 'k-nearest neighbours', 'repository', 'validation', 'samples', 'quality', 'cross-validation', 'value', 'features', 'labels', 'prediction', 'presence', 'greedy', 'subset', 'criterion', 'algorithm', 'classifier', 'mi', 'dataset', 'interest', 'backward', 'knn', 'neighbours', 'classes', 'problem', 'results', 'class', 'data', 'k', 'feature', 'sets', 'selection', 'iris', 'performances', 'uci', 'uci repository', 'knn classifier', 'section', 'experiment', 'test', 'noisy data', 'details', 'test sets']	To this end, a greedy backward feature selection algorithm based on the MI criterion, estimated as detailed in Section  2.3, was run on the well-known Iris dataset from the UCI repository (Frank and Asuncion, 2010). The same experiment was also carried out, but after 20% of the class labels have randomly been switched to another class, chosen equiprobable among the other classes. To measure the quality of feature selection, the performances of a k-nearest neighbours (kNN) classifier are observed for each obtained feature subset, since the kNN classifier is known to be very sensitive to the presence or irrelevant features. It is important to notice that only the feature selection is made with noisy data, while the training, the validation and the prediction steps of the kNN are made using the noise-free data. This allows us to compare the results on the actual problem of interest, feature selection. The experiment was repeated 100 times and samples are split into training and test sets (70%–30%). The optimal value of the meta-parameter k was chosen using ten-fold cross-validation. All the technical details are discussed in Section  6.	computational_statistics_&_data_analysis/S016794731300159X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['error', 'performance', 'methods', 'reference', 'algorithms', 'kfcm', 'dataset', 'order', 'classes', 'time', 'work', 'number', 'experimental study', 'results', 'data', 'study', 'iris', 'wine', 'running time', 'matrix', 'iterations', 'accuracy']	In order to investigate the performance of the proposed algorithms, this subsection uses 150×4 IRIS dataset and 178×13 Wine Dataset [42]. Table 10 lists the clustering results on IRIS & Wine data into three classes during the experimental work using the algorithms FPCM, KFCM, KPFCM, TFPCM, and NTFPCM. As shown in Table 10, the best clustering accuracy, running time, and number of iterations was obtained by using the proposed algorithms. The error matrix in Table 11 provides the accuracy between reference classes and the obtained classes of IRIS & Wine data during the experimental study. Table 11 indicates the proposed methods have better accuracy than the existed methods during the experimental study.	applied_soft_computing/S1568494615003427.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['estimation', 'region', 'implementation', 'eer', 'method', 'dataset', 'paper', 'generation', 'card', 'user', 'keys', 'codes', 'selection', 'template', 'experiments', 'stores', 'techniques', 'templates', 'recovery', 'error-correcting codes', 'systems', 'smart card', 'verification', 'masking', 'information', 'university']	In this paper, we present a method for secret key generation and recovery based on iris verification that, unlike typical systems which store iris templates persistently, stores recovery information on a smart card carried by the user. The scheme uses error-correcting codes to overcome the noisiness inherent in biometric readings. Our iris template incorporates reliable region selection, reliable bit estimation and one-sided masking techniques that provide for both robust verification and large key sizes. An implementation and experiments with the University of Bath iris dataset indicate that our scheme provides for generation and recovery of 260-bit keys with an EER of 0%.	pattern_recognition_letters/S0167865509003705.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['extraction', 'lens', 'error', 'alignment', 'samples', 'access', 'conditions', 'tables', 'methods', 'robustness', 'rates', 'false', 'error rates', 'attacks', 'dataset', 'multimodal', 'gar', 'feature extraction', 'contact', 'rate', 'clips', 'bsa', 'order', 'video', 'number', 'biometrics', 'pso', 'results', 'fusion', 'replay', 'experiments', 'database', 'print', 'experimental results', 'individuals', 'minimum', 'far', 'schemes', 'images', 'attack', 'face', 'face database', 'datasets']	The proposed multimodal schemes are tested on several face and iris spoofing datasets using real and spoof face and iris images. These experiments are performed to demonstrate the robustness of the proposed schemes against spoofing attacks. Several spoofing conditions are studied and the experimental results are presented in Tables 11–16 under face spoofing only, iris spoofing only, both face and iris spoofing and no spoof conditions. The experiments are conducted on Print Attack face database, Replay Attack face database, IIIT-Delhi Contact Lens iris database with transparent attack and colored attack images. The results are presented on the tables for both proposed schemes (using PSO and BSA) with and without alignment of the irises on the images. Minimum Total Error Rates (TER) and Genuine Accept Rates (GAR) at 0.01% False Accept Rate (FAR) are shown in the tables. In order to test the robustness of proposed schemes under spoofing attacks, 50 individuals with 10 real access samples and 10 attack samples are extracted from 200 available video clips in Print Attack face database. Consequently, the same number of individuals, real access and attack samples are extracted from Replay Attack face database. Table 11 presents feature level fusion results of five feature extraction methods on unimodal face biometrics. Additionally, we prepared an iris dataset using IIIT-Delhi Contact Lens Iris database with 50 individuals, 10 real access samples for left and right irises and 10 attack samples dealing with Transparent lens for left and right irises and 10 attack samples for colored contact lens for left and right irises. Table 12 presents score level fusion results of left and right irises on unimodal iris biometrics.	computer_vision_and_image_understanding/S1077314215000454.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['feature selection', 'application', 'performance', 'classification', 'foreground', 'methods', 'classification accuracy', 'rough', 'fields', 'classifier', 'algorithm', 'method', 'spect', 'dataset', 'lung cancer', 'http', 'lung', 'letter', 'order', 'tcmsp', 'results', 'feature', 'heart', 'iris', 'cancer', 'selection', 'database', 'cfs', 'uci', 'databases', 'naïvebayes', 'accuracy', 'datasets']	In order to verify the application foreground of TCMSP, the performance of TCMSP algorithm is compared with CFS [24] and Rough set [17] respectively using SPECT Heart Dataset, Lung Cancer Dataset and Iris Dataset obtained from UCI database (these datasets can be downloaded at http://www.ics.uci.edu). Table 8 lists the classification accuracies based on NaïveBayes classifier with three feature selection methods (i.e. CFS, Rough set and TCMSP); the best results are in bold letter. From Table 8, it shows that the classification accuracy with TCMSP outperforms those of Rough set and CFS method when applied to analyzing the databases “SPECT Heart”. Meanwhile, as for the other dataset “Lung Cancer” and “Iris”, performance of TCMSP also surpass that of Rough set and is as well as that of CFS method. Therefore, the proposed TCMSP method can be effectively applied into many different fields.	computer_methods_and_programs_in_biomedicine/S0169260709000947.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['feature selection', 'application', 'som', 'off-line', 'dataset', 'hfs', 'indications', 'approach', 'baseline', 'guidance', 'construction', 'feature', 'iris', 'selection', 'procedure', 'bip', 'applications']	The detailed application procedure of HFS, and SOM-based NLLP and BIP indications on the iris dataset is following, which is used as a guidance for using the proposed approach in real-world applications:Off-line: Feature selection using HFS and construction of the baseline SOM	applied_soft_computing/S1568494611001220.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'bootstrap method', 'repository', 'observations', 'measures', 'cross-validation', 'measurements', 'length', 'effectiveness', 'method', 'dataset', 'fisher', 'california', 'alcohols', 'data', 'r', 'species', 'scatterplots', 'width', 'examples', 'bootstrap', 'machine', 'university', 'datasets']	We examine the effectiveness of the bootstrap method in two real data examples, comparing it with the cross-validation method in Wang (2010). The first one is iris data (Fisher, 1936), which contains 150 observations from three different species of iris, each with four measures on the length and width of the sepal and petal. The second one is wine data (Aeberhard et al., 1992), which consists of 12 measurements for each of 178 alcohols from three different kinds. The iris dataset is available in R, and the wine dataset is downloaded from University of California Irvine machine learning repository. Fig. 4 displays the scatterplots of these two datasets.	computational_statistics_&_data_analysis/S0167947311003215.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'capabilities', 'numbers', 'samples', 'features', 'group', 'dataset', 'changes', 'outliers', 'groups', 'data', 'iris', 'σ', 'coordinate', 'experiment', 'centers', 'iterations']	The dataset used here is the famous IRIS dataset. It contains three groups (setosa, versicolor, virginica) with four-dimensional features and 50 data samples respectively for each group. In our experiment, we only use the dataset with the third and fourth dimensional features and cluster the dataset into two groups. One group is setosa and the other is versicolor and virginica. The dataset is illustrated in Fig. 5. In this experiment, σ is set to 1.5, and the same initial cluster centers (3, 0.4), (5, −0.3) are respectively used in FCNN and RFCNN, and the maximal numbers of iterations are respectively set as 20 and 300 in RFCNN and FCNN. By adding up different numbers of outliers in the coordinate (6, −1), we can examine the robust capabilities of FCNN and RFCNN for outliers. The changes ΔV of FCNN and RFCNN are listed in Table 2.	applied_soft_computing/S1568494606000469.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'classification', 'index', 'performance', 'yields', 'projection pursuit', 'observations', 'clustering algorithm', 'methods', 'measurements', 'pursuit', 'length', 'algorithm', 'method', 'cluster', 'dataset', 'fisher', 'partitioning', 'groups', 'clustering methods', 'el', 'data', 'multivariate', 'rand index', 'rand', 'species', 'iris', 'blue', 'average', 'single', 'distributions', 'linkages', 'width', 'procedure', 'projection', 'mixture', 'overlap', 'life', 'plot', 'k-medoids']	To illustrate the performance of the proposed method on real life data, we selected the celebrated and, perhaps, the most popular classification dataset: Iris (Anderson, 1935; Fisher, 1936). Three different species of Iris (Setosa, Virginica, and Versicolor) are represented by 50 4-dimensional observations each. The measurements are taken on sepal length and width as well as on petal length and width. As can be seen from the projection pursuit plot provided in Fig. 4, Setosa represents a well-separated green cluster while Virginica (blue) and Versicolor (red) overlap considerably. The Iris dataset is well-known for difficulties associated with clustering the two latter species. In particular, k-means and k-medoids algorithms both produce the same partitioning with 16 misclassifications and corresponding adjusted Rand index R=0.730. Average and Ward’s linkages in hierarchical clustering both produce R=0.759 with 14 misclassified observations. Single and complete linkages perform substantially worse with R=0.564 and R=0.642 respectively. Model-based clustering based on a mixture of skew-normal distributions yields R=0.837 with 24 misclassifications, while model-based clustering relying on a mixture of multivariate Gaussian distributions produces the highest R=0.904 with only 5 misclassifications. However, the EL clustering algorithm proposes the partitioning that yields R=0.941 and contains just three misclassifications. As can be seen from Fig. 4, the obtained classification is excellent. All three misclassified observations (78, 84, 134) are, indeed, very challenging since they are located between the Versicolor and Virginica groups. Hence, the EL procedure proved to be a viable competitor to popular clustering methods for this challenging real-life dataset.	computational_statistics_&_data_analysis/S0167947312004458.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'clusters', 'classification', 'x3', 'samples', 'problems', 'plane', 'attributes', 'method', 'cluster', 'dataset', 'b', 'order', 'classes', 'class', 'data', 'red', 'classification problems', 'flower', 'species', 'blue', 'analysis', 'x2', 'information']	In order to further illustrate our method, we make IRIS dataset classification problems by our method as the typical analysis. IRIS dataset consists of information on 150 iris flowers, 50 each from one of three iris species: Setosa (class 1), Versicolour (class 2), and Virginica (class 3). Each flower is characterized by four numeric attributes, denoted as X1, X2, X3 and X4. By projecting the data onto two dimensional plane, showed in Fig. 5(a) and (b) (the red, blue, and black dots represent three different classes), we can see that there are two large clusters: one consisting of samples of class 1 and the other consisting of samples from class 2 and class 3. The second cluster can further be divided into two subclusters, one composed of samples from class 2 and the other from class 3.	applied_soft_computing/S1568494613004201.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'clusters', 'colors', 'values', 'gap', 'presence', 'dataset', 'multidimensional scaling', 'types', 'squares', 'notice', 'outliers', 'dimensions', 'characters', 'data', 'triangles', 'scaling', 'species', 'iris', 'k-means', 'center', 'procedure', 'plot', 'c', 'suggestion', 'slope']	In the iris dataset, it has been found that iris setosa could be well separated from the other two species, whereas iris versicolor and iris virginica are somewhat overlapping. Fig. 4(a) is the plot after multidimensional scaling procedure, i.e., re-scaling the four dimensional data to two dimensions. The colors represent the two different clusters identified by k-means. The characters represent the three types of iris (the triangles, crosses and squares represent the setosa, versicolor and virginica iris, respectively). Notice that it is reasonable to conclude that the data contain either two or three clusters since iris versicolor and iris virginica are overlapping. In this example, the slope statistic correctly identified two clusters (followed by the suggestions of five or three) as can be observed by the slope statistic values in Fig. 4(c). The suggestion of five clusters is probably due to the outliers in the virginica iris (represented by squares) and the presence of a gap at the center of the virginica dataset, dividing the set into two sub-groups.	computational_statistics_&_data_analysis/S0167947313004507.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'clusters', 'factor', 'repository', 'vista', 'y', 'online', 'length', 'formula', 'following', 'cluster', 'changes', 'dataset', 'records', 'algorithm', 'http', 'b', 'visualization', 'user', 'data', 'functions', 'feature', 'k-means clustering algorithm', 'contribution', 'scaling factor', 'transformations', 'iris', 'dimension', 'range', 'k-means', 'uci', 'distribution', 'c', 'parameters', 'machine', 'indices', 'data distribution', 'users', 'column', 'overlaps']	Scaling transformations allow users to change the length of an axis, thus increasing or decreasing the contribution of a particular data column (particular dimension or feature) on the resultant visualization [25]. Using axis scaling interactively, a user can observe that the data distribution changes dynamically. This is done by adding α to Formula (1) in iVIBRATE [7] as following:(2)Pj(x,y)=(c/k)∑i=1kαiu→xi(dji−mini),(c/k)∑i=1kαiu→yi(dji−mini)where αi(i=1⋯k,αi∈[−1,1]) provides the visually adjustable parameters. As mentioned in [7], αi∈[−1,1] covers a considerable range of mapping functions and this range combined with the scaling factor c, is effective enough for finding a satisfactory visualization. It is known that linear mapping does not break clusters, but may cause cluster overlaps [26]. In Fig. 2 initial data distribution of the Iris dataset from the UCI machine learning repository (available online from http://www.ics.uci.edu/∼mlearn/databases/) is shown. Iris has a four-dimensional dataset with 150 records and 3 clusters. Fig. 3(A) depicts the original data distribution of Iris dataset together with the cluster indices achieved by applying the K-means clustering algorithm in VISTA, in which clusters overlap. Fig. 3(B) shows a better separated cluster distribution of Iris using α-adjustment performed interactively by an expert user.	applied_soft_computing/S1568494614004797.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'clusters', 'mapping', 'vista', 'samples', 'inference', 'effectiveness', 'pca', 'cluster', 'dataset', 'method', 'dimensionality reduction', 'effect', 'approach', 'tools', 'classes', 'visualization', 'number', 'result', 'data', 'multi-class', 'lle', 'dimensionality', 'comparison', 'results', 'iris', 'boundaries', 'area', 'reduction', 'section', 'coordinate', 'distribution', 'experiment', 'adjustments', 'star', 'adjustment']	Fig. 5A depicts the initial mapping of the Iris dataset using Star Coordinate without any user adjustment. Fig. 5B illustrates the result of this mapping after several adjustments by an expert user [6]. As shown in this figure, three clusters are detected after manual adjustment. The expert used the α-transformation and θ-rotation tools of VISTA [6] to detect these 3 clusters. Despite the fact that the pink and green clusters are partially overlapping, the total distribution of data clusters are very clear and well-separated. Fig. 5C shows the result of PCA mapping for visualization of the Iris dataset. Three classes are distinguishable but the cluster boundaries are somehow vague. Fig. 5D shows the result of LLE dimensionality reduction method applied to the Iris dataset. As shown in this figure, although the clusters are concentrated, two of them (the green and black clusters) are not easily distinguishable. In Fig. 5E, the visualization of Iris using our multi-class proposed method is shown. In comparison to Fig. 5B, it can be seen that our approach has achieved almost the same amount of overlapping. The minimized cluster overlapping area demonstrates the effectiveness of our proposed method and the results are very close to those of human inference. In this experiment, a small fraction of the data samples are labeled. The effect of the number of labeled samples over the visualization results is studied in the next section.	applied_soft_computing/S1568494614004797.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'clusters', 'paths', 'specialization', 'rule', 'relations', 'attributes', 'dataset', 'decision', 'domain theory', 'notice', 'rules', 'number', 'work', 'decision tree', 'results', 'class', 'data', 'future', 'lazy', 'degree', 'id3', 'iris', 'set', 'explanations', 'examples', 'domain', 'tree', 'theory']	Similar results have been obtained using LazyCL on the iris data set, although in this data set the attributes involved in the descriptions of clusters are not exactly the same than the ones of the decision tree. Fig. 14 shows both the theory from ID3 and the theory from LazyCL for iris. Notice that the ID3 theory contains several paths that do not classify any example (those which leaf is null). The theory from LazyCL has two rules for classifying iris-setosa. One of these rules (number 2) is the same one built by ID3. Concerning the rule 1 of LazyCL, by checking the available examples of the data set, we confirmed that this rule is correct since there are 50 examples satisying it. So, LazyCL discovered a new useful rule for iris-setosa. Concerning the rules for the class iris-versicolor, rules 1 and 2 from ID3 correspond to rules 1 and 2 from LazyCL, respectively. Notice that rule 3 of LazyCL is an specialization of rule 1, so a possible future work could be to analyze relations between the explanations forming the lazy domain theory. Similarly, for classifying iris-viginica, rules 1 and 2 from LazyCL correspond to rules 1 and 2 from ID3 and rule 3 does not correspond to any rule of LazyCL. Sumarizing, for the iris dataset, both ID3 and LazyCL theories are formed by almost the same rules and these rules have similar specialization degree.	artificial_intelligence_in_medicine/S0933365710001156.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'clusters', 'values', 'mapping', 'space', 'vista', 'cluster structure', 'cluster validity', 'knowledge', 'gap', 'mutation', 'tests', 'algorithms', 'experience', 'clustering', 'algorithm', 'cluster', 'method', 'dataset', 'b', 'liu', 'number', 'chen', 'results', 'structure', 'population', 'genetic algorithms', 'model', 'system', 'iris', 'range', 'domain knowledge', 'rendering', 'implements', 'domain', 'validity', 'parameters', 'crossover', 'analysis', 'users', 'genetic algorithm']	Chen and Liu [10] applied visual rendering to the Iris dataset. Fig. 4.4 shows their clustering results for the Iris dataset. The VISTA system that they used implements a linear and reliable mapping model to visualize the k-dimensional dataset in a 2D star-coordinate space. It allows users to validate and interactively refine the cluster structure based on their visual experience as well as on their domain knowledge. They found that one cluster had been separated from the other two. The gap between clusters A and B can be visually perceived but is not very clear. Fig. 4.4 explains why two is the number of clusters in our cluster validity analysis results. Cole also conducted tests on the Iris dataset using general genetic algorithms [12]. The values of the main parameters he used in the genetic algorithm are: number of iterations=1000, range of exponential mutation rate=from 10.0 to 0.000001, population size=50, crossover probability=1.00. For the cluster validity, the optimal number of clusters obtained are 3 for the Davies Bouldin method and 2 for the Calinski and Harabase method.	knowledge-based_systems/S0950705113003535.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'feature selection', 'application', 'type', 'features', 'length', 'instances', 'dataset', 'benchmark', 'approach', 'literature', 'pattern', 'plant', 'order', 'classes', 'scheme', 'class', 'data', 'f2', 'feature', 'study', 'selection', 'database', 'pattern recognition', 'procedure', 'uci', 'width', 'section', 'distribution', 'recognition', 't', 'data distribution']	In order to offer the detailed application procedure of the proposed approach, a benchmark dataset (i.e., iris dataset) from UCI [29] is used in this section. The iris dataset is perhaps the best known database to be found in the pattern recognition literature. The dataset has 4 features (i.e., sepal length, sepal width, petal length and petal width, that is represented by F=[f1, f2, f3, f4]T), and contains 3 classes (i.e., iris setosa, iris versicolour and iris virginica that are represented by Class 1, 2 and 3 in this study) of 50 instances each, respectively, where each class refers to a type of iris plant. Fig. 3 shows the data distribution for each feature, respectively. It is evident in Fig. 3 that a suitable feature selection scheme should be able to rank f3, f4, f1 and f2, in the same order.	applied_soft_computing/S1568494611001220.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'noise', 'classification', 'error', 'feature selection', 'features', 'presence', 'case', 'classifier', 'rates', 'dataset', 'knn', 'rate', 'number', 'class', 'confidence', 'feature', 'iris', 'selection', 'average', 'performances', 'intervals', 'knn classifier', 'label noise', 'confidence intervals', 'stability', 'classification error']	Fig. 1 shows that even for the simple Iris dataset, the performances of the kNN classifier are considerably affected by the presence of label noise. Indeed, the balanced classification error rate (the average of the classification error rates obtained for each class) is largely and significantly higher in the noisy case, whatever the number of selected features is. Moreover, the stability of the feature selection is also degraded by the presence of label noise, as shown by the much larger confidence intervals.	computational_statistics_&_data_analysis/S016794731300159X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'performance', 'algorithm', 'cluster', 'dataset', 'average performance', 'aco', 'models', 'number', 'local optimum', 'pso', 'hybrid models', 'optimum', 'convergence', 'iris', 'chat', 'section', 'centers', 'generations', 'iterations']	This section discusses the convergence of the proposed algorithm. The Iris dataset with ten cluster centers is regarded as an example. Fig. 5 shows the convergence chat for the PSO, the ACO, and the four ACOR–PSO hybrid models, where the X-axis represents the number of iterations, and the Y-axis is the average performance. In this example, the ACOR is more likely to trap into local optimum than the other models. Generally, the PSO-ACOR hybrid models are superior compared to the PSO and ACOR models. For the four hybrid PSO-ACOR models, Hybrid-III performs well because after 480 generations, its performance is superior to the other models.	applied_soft_computing/S1568494613001580.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'representation', 'instances', 'dataset', 'parent', 'specification', 'operations', 'class', 'data', 'instance', 'component', 'record', 'r', 'datatypes', 'attribute', 'iris', 'radix', 'examples', 'section', 'identifier', 'parameters']	In Section 6.1, we presented an example of datatype representation of the class attribute of the Iris dataset, which is an instance of a primitive datatype. Here, we present an example of a record datatype instance, describing data examples from the Iris dataset (see Fig. 5a). The Iris-record instance inherits all the characterizing operations, and datatype qualities from the parent class. Additionally, the Iris-tuple datatype has a specification of the component datatypes. For example, the Iris field-list contains Iris-field-component instances for each component datatype. Each component specification includes an identifier (e.g., ‘sepal length’) and denotes the datatype of the component (e.g., real(f:def,r:def), where f:def and r:def represent the fraction and radix parameters needed to define an instance of a real datatype class). Finally, the class component is described by the Iris-class datatype instance discussed in Section 6.1.	information_sciences/S0020025515005800.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'results', 'comparison', 'dataset', 'segmentation', 'sdp', 'iris', 'wine', 'datasets']	Fig. 4 displays the comparison of CE-GMDH(SDP) with SDP. Based on the results obtained on six datasets, CE-GMDH(SDP) gets better results than SDP on the Iris dataset, the Segmentation dataset, the Wine dataset and the Zoo dataset.	applied_soft_computing/S156849461630031X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'space', 'observations', 'table', 'features', 'algorithms', 'effectiveness', 'dataset', 'lung', 'classes', 'results', 'class', 'data', 'ft', 'iris', 'distributions', 'wine', 'distribution', 'words', 'accuracy', 'datasets']	To further investigate the effectiveness of the FT algorithms and explore the datasets, the 2-D data distributions obtained by the six algorithms for the Iris, Wine, Leukemia1, and Lung datasets are plotted in Figs. 3–6, respectively. In other words, two transformed features were used to project the original high dimensional data onto a 2-D space. From these results, we give the following observations:(1)As shown in Fig. 3, the Iris dataset is characterized by one well-separated class and two slightly overlapped classes. According to Table 4, KMPFT has obtained the highest average accuracy (=0.9838). Pretty compact distribution can be seen from Fig. 4.	applied_soft_computing/S1568494612003444.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'spanning tree', 'error', 'capabilities', 'estimator', 'bars', 'statistical software', 'convex hull', 'voronoi', 'delaunay triangulation', 'packages', 'convex', 'drawing', 'dataset', 'software', 'triangulation', 'choice', 'minimum spanning tree', 'window', 'dimensions', 'spanning', 'kernel density', 'path', 'bivariate', 'hull', 'statistical', 'iris', 'area', 'version', 'kernel', 'kernel density estimator', 'borders', 'plot', 'procedures', 'chart', 'probability', 'graph', 'delaunay', 'tessellation', 'density', 'substitute', 'voronoi tessellation', 'geometry', 'tree', 'computational geometry']	Among major commercial statistical software packages, SYSTAT® [16] is the only one that provides automated convex hull drawing. Convex hull is one of the optional additions to scatter-plots, together with related computational geometry procedures — Voronoi tessellation, Delaunay triangulation, minimum spanning tree and travelling salesman path. Instead of the convex hull, one can also use a nonparametric kernel density estimator with user-specified probability. In the Version 10, which was available to us for testing purposes, multi-group convex hull plots can only be produced in two dimensions. There is a huge choice of univariate density plots that can be displayed on chart's borders, but it is not possible to superimpose error bars or boxplots on the main plot area. As a substitute, “Gaussian bivariate ellipses” can be placed on the plot. An example of such a plot for the famous Iris dataset [17] is provided in Fig. 1. However, possibilities for subsequently enhancing the plot are very limited due to the modest capabilities of the Graph window.	computer_methods_and_programs_in_biomedicine/S0169260705000179.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'step', 'trace', 'som', 'value', 'features', 'subset', 'changes', 'dataset', 'input', 'hfs', 'square', 'feature', 'selection', 'iris', 'procedure']	Step 3: Based on the ranked features, the algorithm II of HFS is then implemented to generate the feature subset. Fig. 4 presents the value trace(Sw−1Sb) changes in the selection procedure for the iris dataset, respectively. It can be observed that the maximum trace(Sw−1Sb) value is 0.0961. It should be noted that, the square on the figure means that the corresponding feature is chosen. Finally the feature subset (i.e., f3 and f4) for the iris dataset is selected as the input feature set of SOM.	applied_soft_computing/S1568494611001220.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'tradeoffs', 'sensitivity', 'dataset', 'd', 'result', 'structure', 'data', 'results', 'diversity', 'iris', 'accuracy', 'datasets']	As a result, the more the data are complex, the more the diversity affects the accuracy. The sensitivity curves in Figs. 11 and 12 assist to partially describe the structure of data accordingly. Therefore, how the tradeoffs hold, partially depends on the structure of datasets. The results obtained by Iris dataset reveals the fact that an increase in accuracy may result not necessarily due to the increase in diversity as indicated by Fig. 11c and d.	applied_soft_computing/S1568494611002249.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'training', 'classification', 'training set', 'step', 'rule', 'inference', 'membership', 'dataset', 'rule base', 'base', 'functions', 'component', 'system', 'height', 'iris', 'set', 'inference system', 'membership functions', 'indeterminacy']	Rule base designed for neutrosophic indeterminacy component for Iris dataset is shown in Fig. 10.5.Using the training set available, designing of inference system for neutrosophic falsity component is done in the same way as for indeterminacy component discussed in step 4; except that here for this classification example, height of all the membership functions is 0.5.	applied_soft_computing/S1568494612003419.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'type', 'existence', 'clouds', 'instances', 'dataset', 'types', 'plant', 'classes', 'data', 'components', 'iris', 'correlations', 'points', 'principal components', 'information']	The Iris dataset consists of 150 instances belonging to one of three classes each referring to a particular type of Iris plant: setosa, versicolor or virginica. The data, being multidimensional, cannot be viewed directly. By projecting it to the first two principal components (Fig. 9) we notice the existence of two overlapped clouds of points. These two types of clouds belonging to iris versicolor and virginica are harder to distinguish. More information can be obtained by looking at the variable correlations in Fig. 10.	expert_systems_with_applications/S0957417415003346.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['fig', 'values', 'pareto-optimal front', 'value', 'algorithm', 'changes', 'dataset', 'curves', 'system', 'iris', 'change', 'generations']	Average changes in the Pareto-optimal front by running the proposed algorithm for the Iris dataset are displayed in Fig. 4.5 for different generations. It demonstrates how the system converges to an optimal Pareto-optimal front. As the actual change in the value of TWVC is not reflected in the curves plotted in Fig. 4.5 (the curves almost overlap), some key TWVC values are reported in Table 4.2.	knowledge-based_systems/S0950705113003535.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['flr', 'classification', 'performance', 'ann', 'classification performance', 'matlab', 'dataset', 'software', 'memory', 'flc', 'comparison', 'model', 'personal computer', 'computational cost', 'experiments', 'version', 'classifiers', 'cpu', 'computer', 'section', 'cost', 'svm', 'datasets']	In this section, we test the classification performance and the computational cost of the presented FLC model on five well-known datasets, mean the spiral dataset, Ripley's synthetic dataset, iris dataset, wine dataset and handwritten digit dataset. The conventional ANN, SVM, DT and FLR classifiers are also employed for a comparison. All the experiments are conducted on a personal computer with 2.93GHz CPU and 512M memory. The computing software employed is Matlab with version 7.4.1.	applied_soft_computing/S1568494612000555.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['forest', 'neighbor', 'repository', 'bayes net', 'nearest neighbor', 'bpa', 'belief functions', 'classifier', 'method', 'nearest', 'dataset', 'machine learning', 'decision', 'net', 'uc', 'decision tree', 'random', 'data', 'functions', 'sets', 'iris', 'experiments', 'naive bayes', 'classifiers', 'learning', 'random forest', 'experiment', 'bayes', 'machine', 'tree']	As a new way to determine BPA in a data classifier using belief functions, the proposed method will compare with other well-known classifiers in the following experiment. Other classifiers are considered including Naive Bayes [44], Bayes Net [45], Decision Tree Learner (REPTree) [46], Random Forest [47] and One Nearest Neighbor (IB1) [48]. Besides the Iris dataset, we also consider other real data sets in the experiments, which are from the UC Irvine Machine Learning Repository [49].	knowledge-based_systems/S0950705114002366.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['function', 'mamdani', 'type', 'length', 'membership', 'classifier', 'attributes', 'dataset', 'fuzzy logic', 'classes', 'fuzzy classifier', 'logic', 'functions', 'iris', 'width', 'overlapping', 'membership function', 'membership functions']	As overlapping is inherent of fuzzy logic so appropriate overlapping membership functions have been designed for all the Iris dataset attributes and output classes. Figs. 4a, 5a, 6a, 7a and 8a gives the membership function designed for Iris sepal length, sepal width, petal length, petal width and Iris output classes designed for Mamdani type fuzzy classifier.	applied_soft_computing/S1568494612003419.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['function', 'mapping', 'step', 'c-means', 'spread', 'membership', 'fuzzy vector quantization', 'dataset', 'relation', 'interpretation', 'maximum likelihood', 'lvq', 'em', 'sample', 'norm', 'account', 'gaussian function', 'variance', 'gm', 'approximations', 'fuzzy objective function', 'term', 'membership function', 'alternating optimization', 'covariances', 'training', 'type', 'rule', 'vector', 'vector quantization', 'mixtures', 'priors', 'method', 'al', 'scheme', 'likelihood', 'sum', 'optimum', 'fuzzy c-means', 'procedure', 'tuning', 'centers', 'framework', 'i', 'quantization', 'information', 'error', 'pal', 'covariance information', 'clustering', 'memberships', 'fuzzy clustering', 'cluster', 'approach', 'fcm', 'variant', 'supervision', 'iris', 'parameters', 'power', 'covariance', 'optimization', 'objective function', 'scs', 'fuzzy vector', 'm', 'learning rules', 'rules', 'models', 'local optimum', 'version', 'expectation–maximization', 'competition', 'errors', 'gaussian mixtures', 'distance']	The relation between hard c-means (HCM), fuzzy c-means (FCM), fuzzy learning vector quantization (FLVQ), soft competition scheme (SCS) of Yair et al. (1992) and probabilistic Gaussian mixtures (GM) have been pointed out recently by Bezdek and Pal (1995). We extend this relation to their training, showing that learning rules by these models to estimate the cluster centers can be seen as approximations to the expectation–maximization (EM) method as applied to Gaussian mixtures. HCM and unsupervised, LVQ use 1-of-c type competition. In FCM and FLVQ, membership is the −2/(m−1)th power of the distance. In SCS and GM, Gaussian function is used. If the Gaussian membership function is used, the weighted within-groups sum of squared errors used as the fuzzy objective function corresponds to the maximum likelihood estimate in Gaussian mixtures with equal priors and covariances. The fuzzy clustering method named fuzzy c-means alternating optimization procedure (FCM-AO) proposed to optimize the former is then equivalent to batch EM and SCS's update rule is a variant of the online version of EM. The advantages of the probabilistic framework are: (i) we no longer have spurious spread parameters that needs fine tuning as m in fuzzy vector quantization or β in SCS; instead we have a variance term that has a sound interpretation and that can be estimated from the sample; (ii) EM guarantees that the likelihood does not decrease, thus it converges to the nearest local optimum; (iii) EM also allows us to estimate the underlying distance norm and the cluster priors which we could not with the other approaches. We compare Gaussian mixtures trained with EM with LVQ (HCM), SCS and FLVQ on the IRIS dataset and see that it is more accurate due to its being able to take into account the covariance information. We finally note that vector quantization is generally an intermediate step before finding a final output for which supervision may be possible. Thus, instead of an uncoupled approach where an unsupervised method is used first to find the cluster parameters followed by supervised training of the mapping based on the memberships, we advocate a coupled approach where the cluster parameters and mapping are trained supervised in a coupled way. The uncoupled approach ignores the error at the outputs which may not be ideal.	neural_networks/S0893608097001470.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['functions', 'dataset', 'components', 'attribute', 'iris', 'fis', 'rules', 'ranges', 'rule', 'membership functions', 'indeterminacy', 'length', 'width', 'editor', 'membership']	Indeterminacy and falsity neutrosophic components have been designed for Iris dataset for the ranges that were shown overlapping in FIS. Figs. 4c, 5c, 6c, 7c and 8c show membership functions of indeterminacy component Iris-i for attribute sepal length, sepal width, petal length, petal width and 3 iris classes.b.Appropriate rules are developed using rule editor.	applied_soft_computing/S1568494612003419.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['genetic algorithm', 'studies', 'cp', 'credit', 'algorithm', 'dataset', 'parameter', 'results', 'ps', 'difference', 'parameters', 'pc', 'comparisons', 'datasets']	The t-test results indicate that there is a statistically significant difference between the 3 comparisons. The parameter set I outperformed set III on the credit approval and iris datasets; the parameter set II outperformed set III on the iris dataset. Consequently, parameter set I {30,20,10,8,4,250} is selected for later computational studies. Additionally the parameters of the genetic algorithm are determined as {ps, cp, mp, pc, maxin} = {40, 0.8, 0.2, 0.2, 100}.	knowledge-based_systems/S095070511500492X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['index', 'performance', 'values', 'function', 'trace', 'measures', 'cluster validity', 'db', 'algorithms', 'clustering', 'cluster', 'dataset', 'results', 'sw', 'measure', 'entropy measure', 'iris', 'entropy', 'ratio', 'eq', 'scatter matrix', 'partition', 'sb', 'validity', 'matrix', 'scatter']	Performance of the three algorithms on IRIS dataset is examined and compared with two cluster validity measures, Davies–Bouldin (DB) [17] and partition entropy. DB index is the ratio of the trace of inter-cluster scatter matrix tr(Sb) to the trace of intra-cluster scatter matrix tr(Sw). The larger the values of tr(Sb)/tr(Sw) is, the better the clustering performance is. Partition entropy measure is based on entropy function which is defined in Eq. (13), the clustering performance is better if the values of the function J(·) is smaller. The evaluated results of three algorithms are shown in Table 5, the performance of SCAPC is better.(13)J(⋅)=−1N∑k=1C∑i=1Nuik log2(uik)	applied_soft_computing/S156849461100192X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['index', 'yeast', 'tables', 'features', 'algorithms', 'dataset', 'weights', 'result', 'measure', 'rand index', 'sse', 'rand', 'entropy measure', 'iris', 'entropy', 'performances', 'weight', 'terms', 'replications', 'note', 'addition', 'clustering algorithms', 'datasets']	Based on 100 experiment replications, the clustering performances of the three clustering algorithms using the three datasets are shown in Tables 5–7, respectively. Note that only the features with the top five highest weights are displayed in the last rows of Tables 6 and 7. Based on these tables, we found that the proposed FWAS-K-means is slightly inferior to the w-K-means algorithm using the iris dataset in terms of the Entropy measure, and in the yeast dataset according to the Adjusted Rand index measure. Nevertheless, the performances of the proposed FWAS-K-means are better than the other two algorithms in general, especially when considering the SSE measure. In addition, when comparing to the w-K-means algorithm, the FWAS-K-means algorithm enhances representative features by assigning them more weight, which causes the FWAS-K-means algorithm to generate a better clustering result.	computational_statistics_&_data_analysis/S0167947308001552.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['ionosphere', 'network', 'value', 'mutation', 'ram', 'fisher', 'scalar', 'time', 'number', 'termination', 'system', 'iris', 'amd', 'cpu', 'computation time', 'xp', 'connectivity', 'evolution', 'addition', 'pc', 'clone', 'computation', 'university', 'datasets']	For the Fisher Iris and the Johns Hopkins University Ionosphere datasets, their most suitable initial constant scalar used to control connectivity and the mutation value are set to 0.5 and 0.1, respectively, while the initial number of the B-cells is set to 100. For the Fisher Iris dataset the scalar constant of clone and the evolution termination value of the network system are set to 5 and 0.7, respectively, and for the Johns Hopkins University Ionosphere dataset they are set to 4 and 0.8, respectively. Hence, the computation time and the average-RMSE for the Fisher Iris dataset are 50.2930s and 0.0698, respectively, and those for the Johns Hopkins University Ionosphere dataset are 905.4300s and 0.1695, respectively. In addition, for the Pima Indians Diabetes dataset the most suitable initial constant scalar used to control connectivity and the mutation value are set to 0.7 and 0.1, respectively, while the initial number of the B-cells is set to 300. The scalar constant of clone and the evolution termination value of the network system are set to 3 and 0.8, respectively. Hence, the computation time and the average-RMSE for Fisher Iris dataset are 4018.7000s and 0.3015, respectively. Here the computation time is based on a PC with an AMD Athlon XP CPU running at 1.15GHz with 384MB RAM.	applied_soft_computing/S1568494611004261.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['italy', 'yeast', 'repository', 'type', 'chemical', 'sites', 'prediction', 'dataset', 'types', 'literature', 'properties', 'plant', 'classes', 'problem', 'constituents', 'result', 'class', 'data', 'chemical analysis', 'iris', 'protein', 'localization', 'uci', 'wine', 'pattern recognition', 'section', 'recognition', 'machine', 'setup', 'analysis', 'datasets']	In this section, three well-known datasets in pattern recognition literature (i.e. iris, wine, and yeast) are used as benchmark datasets. The three datasets were retrieved from the UCI machine learning repository (Newman et al., 1998) and their properties are listed in Table 4. The iris dataset contains 3 classes and each class refers to an iris plant type. One class is linearly separable from the other two classes, and the other two are hard to linearly distinguish. The wine dataset is the chemical analysis result of three different wine cultivars grown in Italy. Thirteen constituents are used to identify wine types. The yeast dataset is used to predict 10 yeast protein cellular localization sites. This dataset is considered to be a difficult prediction problem, since most data belong to three classes. The experimental setup used in this section is the same as the one described in Section 4.2.	computational_statistics_&_data_analysis/S0167947308001552.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['learning', 'case', 'results', 'class', 'classification', 'dataset', 'iris', 'stage', 'classification accuracy', 'complexity', 'rank', 'accuracy']	Table 5 shows that the results obtained from MOPPSO by considering classification accuracy is higher rank than the architectural complexity for IRIS dataset. We can see that in the learning stage class 2 is misclassified in one case.	computer_science_review/S1574013708000361.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['level', 'fig', 'clusters', 'overlapping clusters', 'performance', 'cluster', 'dataset', 'stochastic model', 'b', 'classes', 'sa', 'mode', 'simulated annealing', 'data', 'model', 'temperature', 'iris', 'node', 'situation', 'annealing', 'parameters', 'nodes', 'addition', 'tree']	Interestingly, the stochastic model performed well in this situation (non-optimal parameters) especially the mode with the addition of simulated annealing. In particular, it improved the performance over the real-world data. SCENT with a fixed temperature in Fig. 12(a) was worse with IRIS dataset because it produced too many nodes to represent three main classes whereas SCENT (SA) produced two nodes on the first level of the tree with one node representing the first separated cluster and another one representing the overlapping clusters in Fig. 12(b).	applied_soft_computing/S1568494601000187.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['level', 'fig', 'training', 'application', 'elements', 'clusters', 'space', 'som', 'network', 'length', 'membership', 'process', 'dataset', 'input', 'pattern', 'dimensions', 'part', 'root', 'iris', 'patterns', 'width', 'topology', 'parts', 'areas', 'threshold', 'right', 'networks', 'map', 'element']	Another graphic example of this application for the Iris dataset can be seen in Fig. 11: each training pattern is shown with its two first dimensions (sepal length and sepal width) and the disposition and topology of the SOM process elements in the same space. Fig. 11 was split into four parts, the upper left part showing the network of the root element that tries to cover the whole input space. The upper right and lower left parts show networks of the next level, which specializes in different map areas. Finally, the lower right part shows the network that is specialized in those input patterns for which the membership level to clusters 1 and 2 has not reached the threshold.	applied_soft_computing/S1568494611003358.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['level', 'methodologies', 'feature selection', 'classification', 'humanoid robot', 'performance', 'laboratory', 'strategy', 'complex system', 'classification accuracy', 'process', 'classifier', 'algorithm', 'dataset', 'attribute selection', 'paper', 'run', 'form', 'evolutionary algorithm', 'time', 'robot', 'feature', 'system', 'behaviour', 'selection', 'philosophy', 'iris', 'role', 'classifiers', 'efficiency', 'wine', 'recognition', 'mimicry', 'precision', 'mechanism', 'field', 'blood', 'accuracy']	No one can fool mother nature but we can learn from her, device many new methodologies through bio mimicry, since nature is the single and most complex system that has been field tested the longest. Being inspired by the mechanism through which our mother nature handling our blood sugar level, in this paper we proposed a new evolutionary algorithm for classification based on it. In this process we have identified that feature selection plays a vital role in deciding the performance behaviour of classifiers and an efficient feature or attribute selection can considerably augment the classification accuracy as well as reduces the run time of the algorithm. The paper describes the philosophy of optimum blood sugar controlling strategy being implemented in optimizing the feature selection and precision process of the classifier in the form of an algorithm. The efficiency of proposed algorithm is demonstrated experimentally on classifying the Iris dataset and Wine recognition dataset together with our laboratory generated humanoid robot dataset.	applied_soft_computing/S1568494612001457.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['level', 'training', 'fig', 'classification', 'performance', 'space', 'validation', 'samples', 'fold', 'classification performance', 'improvement', 'case', 'problems', 'algorithm', 'representation', 'method', 'dataset', 'decision', 'approach', 'diagram', 'rate', 'time', 'number', 'superiority', 'diagrams', 'result', 'results', 'data', 'classification problems', 'iris', 'set', 'experiments', 'gcnn', 'efficiency', 'cross validation', 'computation time', 'phase', 'experiment', 'rest', 'computation', 'datasets']	Next experiment compares the efficiency of KNDPR and DT, on an adequate number of training samples. Result achieved using 5-fold cross validation is presented in Fig. 6. According to the results our proposed method improves the classification performance comparing to the classic DT. It can be observed that the KNDPR and the DT both performed equally on Iris dataset, while KNDPR could reach to a higher level of classification improvement than DT on the rest of datasets. These set of experiments imply that our approach not only can significantly enhance the classification rate on small-sized data, but also it shows its superiority even when an adequate number of training samples are presented in case of dealing with complex classification problems. Fig. 7 illustrates the number of DPRs selected in the first fold of 5-fold cross validation for each dataset. As it can be seen in this diagram, the GCNN algorithm chose a small set of DPRs among all the DPs, helping to reduce the computation time in testing phase. By considering these two diagrams, it can be inferred that KNDPR could propose a finer representation of the decision space by choosing a proper set of DPRs.	applied_soft_computing/S1568494613002834.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['level', 'training', 'retrieval', 'classification', 'activities', 'network', 'neurons', 'rule', 'neural coding', 'classification accuracy', 'lif', 'spiking neural network', 'neural network', 'integrate-and-fire', 'vector', 'temporal coding', 'dataset', 'tasks', 'real-world', 'decision', 'paper', 'approach', 'memory', 'coding', 'spiking neurons', 'stimuli', 'functions', 'learning rule', 'components', 'system', 'iris', 'patterns', 'means', 'pattern recognition', 'storage', 'learning', 'recognition', 'relevance', 'networks', 'readout', 'generalization', 'addition', 'brain', 'information', 'accuracy']	Neural coding and learning are important components in cognitive memory system, by processing the sensory inputs and distinguishing different patterns to allow for higher level brain functions such as memory storage and retrieval. Benefitting from biological relevance, this paper presents a spiking neural network of leaky integrate-and-fire (LIF) neurons for pattern recognition. A biologically plausible supervised synaptic learning rule is used so that neurons can efficiently make a decision. The whole system contains encoding, learning and readout. Utilizing the temporal coding and learning, networks of spiking neurons can effectively and efficiently perform various classification tasks. It can classify complex patterns of activities stored in a vector, as well as the real-world stimuli. Our approach is also benchmarked on the nonlinearly separable Iris dataset. The proposed approach achieves a good generalization, with a classification accuracy of 99.63% for training and 92.55% for testing. In addition, the trained networks demonstrate that the temporal coding is a viable means for fast neural information processing.	neurocomputing/S0925231214003452.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['majority', 'classification', 'distinctiveness', 'error rate', 'majority voting', 'chromosome', 'classification performance', 'three dimensions', 'evaluation', 'technique', 'process', 'algorithm', 'solutions', 'solution', 'dataset', 'population', 'ga', 'variations', 'discovery', 'liver', 'section', 'parts', 'datasets', 'training', 'fig', 'rule', 'selection algorithm', 'equivalent', 'cross-validation', 'incidence', 'rate', 'abilities', 'system', 'chromosomes', 'complexity', 'generations', 'minima', 'cubes', 'error', 'performance', 'capacity', 'local minima', 'misclassification', 'observation', 'dimensions', 'sets', 'convergence', 'selection', 'iris', 'combiner', 'classifiers', 'min', 'layers', 'generalisation', 'features', 'mutation', 'voting', 'simplicity', 'average performance', 'number', 'structure', 'characteristics', 'eq', 'misclassification rate', 'systems', 'selection process', 'computational complexity', 'dynamics']	To limit the computational complexity for each dataset the selection algorithm used a population of only 10 incidence cubes. The mutation rate was set to p=0.1 while specific selection technique described in previous section ensured non-decreasing convergence of the GA in the average classification performance. The chromosomes were built along three dimensions capturing features, classifiers and combiners incidence. They have been evaluated by the average misclassification rate obtained for all layers (combiners) separately. To preserve generalisation abilities of the system, the classifiers and hence the combiners were built on the separate training sets and tested on parts of the dataset which have not been used during training. Then the training and testing sets were swapped such that an equivalent of two-fold cross-validation rule has been used for chromosome evaluation. For simplicity the GA was stopped after 100 generations for all datasets, despite the fact that for some cases convergence was achieved earlier. Fig. 4 illustrates the dynamics of the testing performance characteristics during selection process carried out by the GA algorithm. The typical observation is that the algorithm relatively quickly finds the best performing system and then in subsequent generations it keeps improving other solutions in the population. The algorithm showed the capacity to get out of local minima which effectively means discovery of significantly better solution spreading swiftly in many variations during subsequent generations. Fig. 5 depicts the evaluation of a final population of chromosomes for both datasets. For Iris dataset the Min combiner showed the best average performance including the absolute best performing system with only 1.33% misclassification rate. Majority voting showed on average best performance for Liver dataset, including the absolute best performing system with 27.8% error rate. The best systems for both datasets were then further uncovered by illustrating the structure of the classifiers and features selected as shown in Fig. 6. Interestingly, for each selected classifier the algorithm selected at least two features. One classifier for both datasets was excluded. Other than that there is nothing significant about the selection structures shown in Fig. 6. This could only prove that it is very difficult to find the best performing systems as they do not exhibit any visible distinctiveness but are simply lost among large number of system designs embodying huge selection complexity as shown in Eq. (4).	applied_soft_computing/S1568494605000840.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['methods', 'high', 'algorithms', 'initialization', 'method', 'dataset', 'paper', 'terminology', 'checkerboard', 'clustering methods', 'results', 'clustering techniques', 'iris', 'cancer', 'experimental results', 'techniques', 'wine', 'workflow', 'section', 'databases', 'rest', 'accuracy']	The rest of the paper is organized as follows. In Section 2, this paper gives terminology of clustering techniques and workflow of the paper. Section 3 contains the proposed algorithms. Section 4 presents prototypes initialization method. Section 5 gives the method for clustering accuracy. The experimental results on Synthetic Dataset, Checkerboard Dataset, Wine dataset, IRIS Dataset, and High Dimensional Cancer Databases of the proposed clustering methods are reported in Section 6. Section 7 provides conclusion of this paper.	applied_soft_computing/S1568494615003427.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['mobile', 'dataset', 'iris', 'challenge', 'protocols', 'evaluation', 'miche']	Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris dataset and protocols	pattern_recognition_letters/S0167865515000574.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['names', 's', 'dataset', 'p', 'fisher', 'space', 'w', 'iris', 'column', 'length', 'width', 'l']	The original column names of the famous iris dataset (Fisher, 1936) are shortened in the example (S = sepal, P = petal, L = length, W = width) to save space.	astronomy_and_computing/S2213133715000669.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['nb', 'loss', 'function', 'poisson', 'poisson model', 'risk', 'zero–one', 'dataset', 'approach', 'advantage', 'models', 'picture', 'results', 'result', 'loss function', 'model', 'iris', 'past', 'experiments', 'magnitude', 'difference', 'reformulation', 'svm']	The overall picture of the results tells us one thing: when the NB models are trained with a zero–one loss function, there is a clear advantage in using the multinomial approach. This result has been confirmed by many experiments in the past (Eyheramendy, Lewis, & Madigan, 2003; Sebastiani, 2002; McCallum & Nigam, 1998). But when we use the reformulation of the conditional risk, the difference among the three NB models almost disappears, and it turns out that the Poisson model has a slight advantage over the others. Results with SVM are comparable and the magnitude of the difference between NB and SVM it is very similar to the one found for the Iris dataset.	information_processing_&_management/S0306457314000363.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['nb', 'loss', 'values', 'function', 'simple', 'type', 'zero–one', 'dataset', 'approach', 'results', 'class', 'loss function', 'model', 'flower', 'iris', 'experiments', 'learning', 'gaussian', 'sim', 'i', 'gaussian model']	The overall results of the experiments on the Iris dataset are shown in Table 5. For each class, a type of Iris flower, we show the F1 values of:•the NB Gaussian model for (i) the approach with a zero–one (0–1) loss function, (ii) the simple (sim) learning approach, (ii) the advanced (adv) learning approach;	information_processing_&_management/S0306457314000363.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['nb', 'type', 'variation', 'value', 'classifier', 'instances', 'dataset', 'fisher', 'benchmark', 'literature', 'pattern', 'classes', 'class', 'data', 'flower', 'r', 'species', 'iris', 'pattern recognition', 'recognition']	This is one the best known dataset in the pattern recognition literature for studying non-linearly separable data, and it contains real value data, which make this dataset a good benchmark for a Gaussian NB classifier. Introduced by Fisher (1936), the dataset contains the morphologic variation of three species of the Iris flower. There are 3 classes with 50 instances each, where each class refers to a type of Iris flower, see Table 1. One class, ‘Iris setosa’, is linearly separable from the other two, ‘Iris virginica’ and ‘Iris versicolor’; the latter are not linearly separable from each other. We used the Iris dataset available with the standard R installation.7http://archive.ics.uci.edu/ml/datasets/Iris.7	information_processing_&_management/S0306457314000363.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['nb', 'values', 'performance', 'space', 'region', 'poisson', 'collection', 'tables', 'intercept', 'coefficient', 'case', 'bernoulli', 'algorithm', 'solution', 'maximum margin', 'dataset', 'categories', 'approach', 'classes', 'newsgroup', 'margin', 'results', 'optimisation', 'model', 'iris', 'hyperplane', 'costs', 'performances', 'default', 'examples', 'brute-force', 'newsgroups', 'situations', 'experiment', 'hand', 'combinations', 'differences', 'svm', 'reuters-21578']	What if instead of the adapted angular region algorithm which is a very simple brute-force algorithm that keeps the first available solution among the many possible ones,17There are many situations in which different combinations of angular coefficient and intercept produce the same F1-score. In these cases, the angular region algorithm retains the first solution it encounters.17 we use an SVM to find the maximum margin hyperplane on the two dimensional space? The results of this experiment are reported in Tables 10–13. We trained a linear SVM on the three two-dimensional space generated by each NB model (Gaussian in the case of the Iris dataset, Bernoulli, multinomial and Poisson in the case of Reuters-21578, 20 Newsgroups, and RCV1), first with default values, and then by optimising the costs of the two classes using the same brute-force approach of the first experiment. Like our approach, SVM with default values performs well on bigger categories and on categories that are well separated from the negative examples (for example, the case of the Reuters-21578 on the two dimensional space generated by the multinomial model), while it requires some optimisation for smaller categories. Differences among the two approaches on the Iris dataset and the optimised Reuters-21578 collection are very subtle and not statistically significant. On the other hand, on the 20 Newsgroup dataset the performance of the SVM is still lower compared to the angular region algorithm, and it would require more optimisation to get the best performances possible. The same thing happens for the RCV1 dataset. The performance of the SVM on the two-dimensional space is significantly lower compared to the angular region algorithm.	information_processing_&_management/S0306457314000363.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['outcome', 'lapse', 'samples', 'one-way', 'report', 'eyes', 'regression', 'dataset', 'observation', 'match', 'linear regression', 'scores', 'models', 'time', 'data', 'sensor', 'images', 'analysis', 'details']	Czajka12 analysed the BioBase ageing iris dataset, which contains 571 iris images for 58 different eyes. The short-term and long-term matches for this data are considered to be two years in time lapse and five to nine years in time lapse respectively. All images were acquired with the IrisCUBE sensor (see the report for more details of this sensor). Match scores were generated with the VeriEye matcher, the Mirlin matcher13 and a custom Zak-Gabor matcher. The VeriEye matcher was found to be the most accurate, as the report says: “The genuine scores are approximately 14% lower when the time lapse between samples starts from five years and reaches more than eight years, and this observation is supported by the outcome of one-way unbalanced analysis of variance.” Trokielewicz has recently published a more exhaustive consideration of linear regression models using this dataset.14	biometric_technology_today/S0969476515301569.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['overview', 'focusing', 'dataset', 'decision', 'factors', 'interpretation', 'results', 'iris', 'description', 'performances', 'experimental results', 'classifiers', 'section', 'newsgroups', 'analysis', 'line', 'datasets']	In this Section, we present the results on the three chosen datasets. We first give a detailed description of the Iris dataset experimental results (Section 6.1) focusing on the interpretation of the two dimensional decision line. Then, we present an overview of the performances of the classifiers on the Reuters and the 20 Newsgroups datasets (Sections 6.2 and 6.3). Finally, a more specific analysis of the factors that can affect the performances (Section 6.5).	information_processing_&_management/S0306457314000363.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['performance', 'candidates', 'repository', 'samples', 'dataset', 'machine learning', 'uc', 'pattern', 'classes', 'boundary', 'iteration', 'class', 'data', 'iris', 'set', 'database', 'experiments', 'constraints', 'pattern recognition', 'learning', 'pairwise', 'recognition', 'machine', 'pairwise constraints']	The performance of SCAPC is first tested on IRIS dataset, which can be obtained from UC Irvine Machine Learning Repository [16]. Iris data set contains three classes of fifty samples each, one class is linearly separable from the other two, and the latter two are nonlinearly separable from each other. This dataset is perhaps the best known database to be found in the pattern recognition literatures. In the experiments three candidates of pairwise constraints are actively selected along ambiguous boundary at each iteration.	applied_soft_computing/S156849461100192X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['performance', 'exact solution', 'optimal solution', 'knowledge', 'value', 'bound', 'exact algorithm', 'criterion', 'solution', 'algorithm', 'dataset', 'http', 'approach', 'problem', 'branch-and-bound algorithm', 'program', 'model', 'euclidean', 'euclidean distance', 'iris', 'constraints', 'dissimilarity', 'objects', 'website', 's', 'lower bound', 'branch-and-bound', 'search', 'solver', 'distance']	Finding an exact solution for minimizing WCSD is difficult and we compare the performance of our new model in Gecode solver with the Repetitive Branch-and-Bound Algorithm (RBBA) [4]. The program RBBA has been obtained from the author's website (http://mailer.fsu.edu/~mbrusco/). To our knowledge, it is the best exact algorithm for the WCSD criterion. The dissimilarity between objects is measured by the squared Euclidean distance. Without user constraints, both our model and the RBBA approach can only find the optimal solution with the Iris dataset. Our model needs 4125 s to complete the search whereas RBBA takes 3249 s. RBBA solves the problem by repetitively solving sub-problems: finding the optimal solution with k+1,k+2,…,n objects. Relying on the optimal value of WCSD computed in the sub-problems, a better lower bound of WCSD can be computed, enabling RBBA to have better performance. However, extending this algorithm to integrate user constraints is difficult.	artificial_intelligence/S0004370215000806.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['performance', 'observations', 'tables', 'algorithms', 'cluster', 'dataset', 'results', 'yahoo', 'iris', 'appendix', 'experimental results', 'wine', 'ls', 'terms', 'ch', 'datasets']	The detailed experimental results are presented in Appendix A. Two observations can be made from the detailed experimental results. Firstly, CE-GMDH(LS) obtained better results than the other cluster ensemble algorithms over most datasets (the 2D2K dataset, the 8D5K dataset, the Iris dataset, the Libras dataset, the Yahoo! dataset and the Zoo dataset) in terms of CH, RI and NMI. Secondly, results in the Tables A.5, A.7 and A.8 also show that CE-GMDH(LS) has comparable performance to the other algorithms. Take the Wine dataset for example, CE-GMDH(LS) produces the best results over this dataset in terms of RI and NMI, while CE-GMDH(LS) has comparable performance with the other algorithms in CH.	applied_soft_computing/S156849461630031X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['performance', 'repository', 'length', 'algorithms', 'attributes', 'method', 'instances', 'dataset', 'machine learning', 'choice', 'classes', 'dimensions', 'pso', 'class', 'vowel', 'instance', 'species', 'iris', 'uci', 'wine', 'width', 'learning', 'experiment', 'machine', 'datasets']	Experiment 2 used four datasets to compare the performance of Starling PSO against other algorithms. These datasets are taken from UCI Machine Learning Repository [27]. The four datasets are Iris, Wine, Contraceptive Method Choice and Vowel datasets. The Iris dataset includes three species of Iris flowers (three classes). They are Iris Setosa, Iris Versicolour, and Iris Virginiga. There are 50 instances of each class, 150 instances in total. Each instance has four attributes (four dimensions) which are sepal length, sepal width, petal length, and petal width.	applied_soft_computing/S1568494615004299.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['pnn', 'x3', 'function', 'y', 'features', 'dataset', 'input', 'mathematical model', 'model', 'iris', 'description', 'layer', 't', 'x2', 'i']	The mathematical model obtained by our model for Iris dataset is presented as an example.PD21=[−0.96751,−0.25663,0.65004,−0.13994,0.044319,0.10067] * poly(x1,x3),PD31=[1.633,−1.0407,1.4825,−0.051333,0.09108,−0.067678]*poly(x1,x4);PD11=[−1.7572,1.4898,−2.4317,0.3308,−0.15092,−0.018686]*poly(x1,x2);PD162=[2.0965,1.4134,−0.84284,0.046201,−0.57289,0.079276]*poly(PD21,x3);PD282=[2.2683,1.7305,−1.2497,−0.22651,0.15639,0.15237]*poly(PD31,x2);PD382=[−1.8284,−0.58075,0.42213,0.081458,−0.25631,0.015491]*poly(PD11,x3);PD4963=[−0.054529,0.54087,0.45923,−1.1876,0.6309,0.60812]*poly(PD162,PD282);PD5573=[−0.043133,0.59432,0.41454,−0.75286,0.39684,0.3952]*poly(PD282,PD382);y= [0.0057053,1.158,−0.15925,5.3642,−2.208,−3.156]*poly(PD4963,PD5573);where function ploy(a1, a2) {return [1, a1, a2, a1*a2, a1^2, a2^2]T;}, PDji is the output of layer i and jth partial description, y is the output of the PNN model and x is the input features.	applied_soft_computing/S1568494610003200.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['position', 'training', 'size', 'samples', 'type', 'methods', 'algorithms', 'fault', 'dataset', 'types', 'effect', 'paper', 'bearing', 'detectors', 'quantity', 'time', 'order', 'equipment', 'results', 'detection', 'data', 'degree', 'selection', 'iris', 'anomaly detection', 'experiments', 'performances', 'ball bearing', 'detector', 'negative selection', 'datasets']	In the paper, two novel negative selection algorithms (NSAs) were proposed: FB-NSA and FFB-NSA. FB-NSA has two types of detectors: constant-sized detector (CFB-NSA) and variable-sized detector (VFB-NSA). The detectors of traditional NSA are generated randomly. Even for the same training samples, the position, size, and quantity of the detectors generated in each time are different. In order to eliminate the effect of training times on detectors, in the proposed approaches, detectors are generated in non-random ways. To determine the performances of the approaches, the experiments on 2-dimensional synthetic datasets, Iris dataset and ball bearing fault data were performed. Results show that FB-NSA and FFB-NSA outperforms the other anomaly detection methods in most cases. Besides, CFB-NSA can detect the abnormal degree of mechanical equipment. To determine the performances of CFB-NSA, the experiments on ball bearing fault data were performed. Results show that the abnormal degree based on the CFB-NSA can be used to diagnose the different fault types with the same fault degree, and the same fault type with the different fault degree.	applied_soft_computing/S1568494615005050.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['product', 'classification', 'space', 'collection', 'risk', 'transformation', 'sums', 'decision', 'task', 'interpretation', 'problem', 'newsgroup', 'likelihood', 'iris', 'text', 'logarithms', 'experimental analysis', 'section', 'probabilities', 'formulation', 'volume', 'log', 'anomalies', 'analysis', 'line', 'reuters-21578', 'binary classification', 'datasets']	For the experimental analysis of this new interpretation of conditional risk, we chose four widely dataset for the task of binary classification: the Iris dataset, the Reuters-21578 text collection, the 20 Newsgroup collection, the Reuters Collection Volume I (RCV1) dataset. The two Reuters datasets and the 20 Newsgroup dataset are so sparse that we could not work directly on the likelihood space. Instead, we had to use the log transformation to avoid arithmetical anomalies and transform the product of probabilities into sums of logarithms. This non-linear transformation into a log-likelihood space does not change anything in the formulation of the problem, but we need to be more careful when we interpret the decision line. See Section 6.6 for more thoughts on this issue.	information_processing_&_management/S0306457314000363.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['rank', 'extraction', 'opacity', 'performance', 'classification', 'function', 'artificial neural networks', 'rule', 'top', 'humans', 'overview', 'evaluation', 'credit risk', 'classification techniques', 'risk', 'vector', 'recommendation', 'support', 'classifier', 'credit', 'dataset', 'medical diagnosis', 'paper', 'rule extraction', 'rules', 'property', 'percentage', 'neural networks', 'support vector', 'machines', 'svms', 'comprehensibility', 'scoring', 'support vector machines', 'iris', 'range', 'experiments', 'techniques', 'diagnosis', 'requirement', 'credit scoring', 'limitation', 'networks', 'svm', 'applications', 'accuracy', 'datasets']	In recent years, support vector machines (SVMs) were successfully applied to a wide range of applications. However, since the classifier is described as a complex mathematical function, it is rather incomprehensible for humans. This opacity property prevents them from being used in many real-life applications where both accuracy and comprehensibility are required, such as medical diagnosis and credit risk evaluation. To overcome this limitation, rules can be extracted from the trained SVM that are interpretable by humans and keep as much of the accuracy of the SVM as possible. In this paper, we will provide an overview of the recently proposed rule extraction techniques for SVMs and introduce two others taken from the artificial neural networks domain, being Trepan and G-REX. The described techniques are compared using publicly available datasets, such as Ripley’s synthetic dataset and the multi-class iris dataset. We will also look at medical diagnosis and credit scoring where comprehensibility is a key requirement and even a regulatory recommendation. Our experiments show that the SVM rule extraction techniques lose only a small percentage in performance compared to SVMs and therefore rank at the top of comprehensible classification techniques.	european_journal_of_operational_research/S0377221706011878.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['regularization parameter', 'training', 'classification', 'values', 'error', 'numbers', 'regularization', 'training set', 'samples', 'tables', 'median', 'solutions', 'solution', 'dataset', 'parameter', 'percentage', 'results', 'instance', 'iris', 'set', 'average', 'combiner', 'classifiers', 'kernel', 'classification error', 'svm', 'accuracy', 'datasets']	Figs. 6 and 7 illustrate the values of classification error and the Q-statistic for the three solutions selected for all the 14 datasets on the training set. Due to these three solutions, we can obtain the following results. The classification average accuracy (i.e. the percentage of correctly classified samples) of these solutions over 14 datasets are 99.3%, 95.76% and 88.09% on the training set, respectively; their average Q-statistic values are 0.805, 0.739, and 0.595 on the training set, respectively. Also, the average numbers of the selected classifiers for these solutions are 19.57, 18.21, and 13.92, respectively. For instance, Table 6 presents the corresponding numbers of optimal classifiers and their combiner based on Tables 2 and 3 for the three non-dominated solutions of Iris dataset. SUBC and SVM classifiers, with polynomial kernel and regularization parameter C=5, as well as the VOTE and MEDIAN combiners are the most frequently selected classifiers and combiners by the Second solution.	applied_soft_computing/S1568494615005797.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['results', 'classification', 'method', 'performance', 'dataset', 'decision', 'iris', 'rules', 'methods', 'classification rules', 'experimental results', 'tree', 'decision tree']	The performance of the proposed method in deducing the classification rules for the Iris dataset is compared with that of the decision tree and hyper sphere methods. The experimental results are listed in Table 2.	computers_in_biology_and_medicine/S0010482513002229.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['results', 'data', 'clustering', 'model-based clustering', 'dataset', 'model', 'fisher', 'observations', 'iris', 'species', 'variables', 'width', 'length', 'model fitting', 'fitting']	Iris dataset (Anderson, 1935; Fisher, 1936) has 150 observations and 4 variables: sepal length, sepal width, petal length, and petal width. The collected data represent three Iris species called Setosa, Versicolor, and Virginica. The results of model fitting and model-based clustering are provided in Table 8.	computational_statistics_&_data_analysis/S0167947316300019.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['results', 'duplication', 'fitness', 'rule', 'iris', 'rules', 'top', 'examples']	In the results on the Iris dataset several examples of rule duplication arise. For example, the following rule was ranked as one of the five most predictive rules (the fitness for this rule is reported in square brackets at the top of the rule):	artificial_intelligence_in_medicine/S0933365703000563.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['results', 'method', 'dataset', 'experiment', 'fitness', 'iris', 'precision', 'column', 'generation']	Fivefold cross-validations are employed. Table 1 gives results of the experiment on the Iris dataset by our method, “Precision” gives the testing precision, “Best” gives the fitness of the best individual, “WIB” gives the worst fitness in the generation producing the best, The column entitled “AIB” gives the average fitness in the generation producing the best.	computers_&_mathematics_with_applications/S0898122108005658.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['samples', 'length', 'attributes', 'dataset', 'plants', 'types', 'classes', 'data', 'sets', 'iris', 'characteristics', 'width', 'cm', 'experiment', 'summary', 'complete sets']	For the first experiment, complete sets of famous 150 Iris samples are used. The dataset has four attributes, namely; sepal length in cm, sepal width in cm, petal length in cm, petal width in cm. Iris dataset has three classes representing the three types of Iris plants namely, Iris Setosa, Iris Versicolour and Iris Virginica. Table 2 presents a summary of IRIS data characteristics.	applied_soft_computing/S1568494610000931.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['separators', 'instances', 'dataset', 'classes', 'd2', 'disks', 'π', 'class', 'assignment', 'iris', 'boundaries', 'projections', 'probabilities', 'lines', 'difference']	In Figs. 1–3 we display different [π; Δ] projections for Fisher’s well known 4-dimensional Iris dataset. We confine ourselves to the two classes that are not perfectly separable: Versicolor, red disks, 50 instances, Virginica, blue disks, 50 instances. We also show the visual difference between the d1 vs. d2 and ln(d1) vs. ln(d2) projections. Dashed lines represent the optimal class separators (not necessarily the 45° lines that pass through the origins). Disks outside the dotted lines have class assignment probabilities, p⩾0.75. (The dotted lines representing the boundaries separating p⩾0.75 from p⩽0.75 are parallel for the ln(d1) vs. ln(d2) plots [19]).	journal_of_biomedical_informatics/S1532046411000682.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['similarity', 'gaussian mixture models', 'kullback–leibler', 'phases', 'image', 'evaluation', 'gmm', 'spectral clustering', 'effectiveness', 'clustering', 'algorithm', 'criterion', 'method', 'dataset', 'contributions', 'gaussian mixture', 'paper', 'approach', 'models', 'image segmentation', 'operations', 'problem', 'divergence', 'regions', 'em', 'result', 'first', 'similarity matrix', 'model', 'components', 'reconstruction', 'iris', 'expectation–maximization', 'mixture', 'experimental evaluation', 'distances', 'segmentation', 'image reconstruction', 'matrix', 'framework', 'gaussian mixture model']	A novel image segmentation method that combines spectral clustering and Gaussian mixture models is presented in this paper. The new method contains three phases. First, the image is partitioned into small regions modeled by a Gaussian Mixture Model (GMM), and the GMM is solved by an Expectation–Maximization (EM) algorithm with a newly proposed Image Reconstruction Criterion, named EM-IRC. Second, the distances among the GMM components are measured using Kullback–Leibler (KL) divergence, and a revised Floyd׳s algorithm developed from Zadeh׳s operations is used to build the similarity matrix based on those distances. Finally, spectral clustering is applied to this improved similarity matrix to merge the GMM components, i.e., the corresponding small image regions, to obtain the final segmentation result. Our contributions include the new EM-IRC algorithm, the revised Floyd׳s algorithm, and the novel overall framework. The experimental evaluation on the IRIS dataset and the real-world image segmentation problem demonstrates the effectiveness of our proposed approach.	neurocomputing/S0925231214006298.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['size', 'clusters', 'hybridization', 'table', 'cluster', 'dataset', 'types', 'hybrid model', 'paper', 'approach', 'models', 'brevity', 'number', 'pso', 'hybrid models', 'data', 'model', 'pheromone', 'iv', 'iris', 'k-means', 'experiments', 'sequence', 'performances', 'exchange', 'datasets']	This paper performed experiments in data clustering for the hybrid models, traditional K-means, the PSO, and the ACOR. For the sake of brevity, the hybridization of the sequence approach is abbreviated as Hybrid I, that of the parallel approach as Hybrid II, the sequence with the enlarged size of the pheromone table as Hybrid III, and the global best exchange as Hybrid IV. Table 4 illustrates the average 5-run performances with a cluster number from 2 to 10 by using the Iris dataset for K-means, the PSO, the ACOR and the four types of the hybrid model. The 5-run performances of the various number of clusters are further averaged for the nine experimental datasets, which are shown in Table 5.	applied_soft_computing/S1568494613001580.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['size', 'error', 'space', 'architecture', 'som', 'network', 'operators', 'measures', 'preservation', 'quality', 'high-dimensional data analysis', 'objective', 'vector', 'vector quantization', 'reaction mechanism', 'data visualization', 'dataset', 'paper', 'models', 'order', 'immune network', 'visualization', 'reaction', 'shape', 'structure', 'population', 'data', 'model', 'high-dimensional data', 'iris', 'self-organizing map', 'procedure', 'consistency', 'topology', 'capability', 'topological structure', 'difference', 'topology preservation', 'antibody', 'artificial immune network', 'map', 'analysis', 'shape space', 'mechanism', 'tree', 'quantization', 'immune']	The architecture and learning procedure of a novel artificial immune network, referred to as tree structured artificial immune network (TSAIN), are described in this paper. One major difference between this model and current models is that the topological structure can be strictly guaranteed as a tree, which allows to analyze the presented data (antigens) hierarchically. The other is that a novel antibody reaction mechanism inspired from the self-organizing map (SOM) is adopted in order to maintain consistency between the shape space metric and the topological metric, which is an important objective in high-dimensional data analysis. Moreover, several novel immune operators are also employed to improve the quality of the antibody population as well as to control its size. It is qualitatively demonstrated as well as quantitatively verified on a 3-D synthetic dataset and Iris dataset that TSAIN exhibits promising data visualization capability and low vector quantization error. We also use three well-known topology preservation measures to validate the topology preservation capability of our proposed model.	neurocomputing/S092523120900277X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['size', 'step', 'value', 'dataset', 'percentage', 'step size', 'number', 'results', 'comparison', 'hyperboxes', 'experiments', 'classifiers', 'minimum', 'terms', 'accuracy', 'datasets']	Total 121-MFMMN experiments are conducted on each of these datasets for θ=0–1 with the step size of 0.1 and for each θ value, thresh is varied from 0 to 1 with the step size of 0.1. Also for comparison, the results of FMMN and GFMMN are obtained for θ=0–1 with the step size of 0.1. These results are in terms of minimum, maximum and average percentage accuracies as recorded in Table 10 and the minimum, maximum and average number of hyperboxes as given in Table 11. The average percentage accuracy given by MFMMN is higher for all the datasets except iris dataset. This higher accuracy is obtained with very less number of efficient hyperboxes as compared to other classifiers.	applied_soft_computing/S1568494615006729.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['size', 'training', 'classification', 'performance', 'repository', 'samples', 'classification performance', 'features', 'length', 'dataset', 'machine learning', 'classes', 'problem', 'class', 'iris', 'testing', 'uci', 'width', 'learning', 'subsets', 'machine', 'hand', 'datasets']	On the other hand, the classification performance of the DMNN was evaluated on seven well-known datasets, which can be found in the UCI Machine Learning Repository [1]. The Iris dataset was the first considered problem. This dataset has 3 classes (Iris setosa, Iris virginica and Iris versicolor) and 4 features (the length and the width of the sepals and petals, in centimeters). The dataset has 50 samples per class. The original dataset was divided into two subsets of the same size, taking samples randomly for training and testing.	neurocomputing/S0925231213010916.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['space', 'case', 'dataset', 'input', 'absence', 'classes', 'number', 'regions', 'scheme', 'consequence', 'confidence', 'data', 'class', 'optimum', 'convergence', 'selection', 'iris', 'intervals', 'constraints', 'situation', 'points', 'active learning', 'confidence intervals', 'learning', 'distances', 'centers', 'note', 'compare', 'moves', 'constraint']	Active learning, being a way of introducing constraints on carefully selected points, seems a good way of avoiding such a situation. Figs. 7–10 compare the active learning scheme with random constraint selection. Note that active learning does not involve any random selection of constraints, hence the absence of confidence intervals. Overall, active learning allows faster convergence than does random selection. In the case of the Iris dataset, the optimum is obtained with 60 constraints when using active learning, whereas it is still not obtained with 200 constraints using random selection. Remark that active learning may be outperformed by random selection (Figs. 9 and 10), especially with a small number of constraints. In this case, active learning tends to introduce constraints on data that belong to specific regions of the input space. This may result in undesired moves of the class centers. As a consequence, other data points whose distances to the center increase may switch to other classes.	computational_statistics_&_data_analysis/S0167947310003646.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['symptoms', 'dataset', 'explanation', 'paper', 'rules', 'goal', 'time', 'warfarin', 'wall', 'member', 'data', 'measure', 'target', 'ensembles', 'basis', 'inr', 'iris', 'ratio', 'fit', 'international', 'patients', 'behavior', 'procedure', 'children', 'monitoring', 'use', 'dose', 'bronchiolitis', 'blood', 'datasets']	The paper by Wall et al. explores the use of KBN ensembles to provide comprehensible descriptions. They argue that comprehensible descriptions can be obtained on a case-by-case basis by capturing the local behavior of ensemble members. For each ensemble member rules are extracted, and from rules that are operative for a particular example, those rules for explanation are selected for which the example is a clear fit. The paper illustrates the procedure using the well-known iris dataset, and also applies it to two medical datasets, bronchiolitis and warfarin data. The goal with the bronchiolitis dataset is to predict those children displaying symptoms of bronchiolitis that should be admitted to hospital overnight for monitoring. The warfarin data was used to predict the subsequent dose of warfarin to be administered to patients with a high target International Normalized Ratio (INR), a measure of blood clotting time.	artificial_intelligence_in_medicine/S093336570300054X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['test set', 'training', 'training set', 'samples', 'method', 'dataset', 'detection', 'data', 'set', 'anomaly detection', 'items', 'parameters', 'test']	For anomaly detection, versicolor data was considered as normal data and the other two data are taken as anomaly data. For iris dataset, the first 100 samples are chosen for the training set and remaining 50 items for the test set. The obtained best parameters are given in Table 5 for the proposed method.	applied_soft_computing/S1568494609002166.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'classification', 'case', 'responses', 'algorithm', 'cluster', 'dataset', 'decision', 'response', 'rate', 'outliers', 'classifications', 'success', 'number', 'bp', 'class', 'capacities', 'iris', 'objects', 'object', 'taxonomy', 'spectra', 'defects']	As we could observe for the case of the Iris dataset, the algorithm can generate a variable number of responses according to its decision capacities for classifying a given object. When the object belongs to a cluster in which the represented objects clearly belong to one single class of the taxonomy, the algorithm provides one single response. This is what we called classification into one class. In the case of the photometric (BP and RP) spectra, we work with rare objects (outliers) and therefore do not expect, a priori, a very high rate of classifications into one class: the training objects will be known objects without formal defects. We pretend to maximize the success rate for the cases where the algorithm provides one single classification.	applied_soft_computing/S1568494611003358.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'classification', 'network', 'stage', 'value', 'process', 'clustering', 'effectiveness', 'ability', 'dataset', 'paper', 'unsupervised clustering', 'scheme', 'simulation results', 'adjusting', 'result', 'results', 'epochs', 'simulation', 'learning process', 'illustration', 'iris', 'centers', 'hybrid network', 'threshold', 'vigilance']	A hybrid GrayART network consisting of an unsupervised clustering stage and a supervised adjusting stage is proposed in this paper. In this hybrid learning scheme, the unsupervised clustering stage estimates cluster centers of a given dataset, and the supervised adjusting stage follows to tune the cluster centers for improving the training result. With a proper vigilance threshold value, the overall learning process needs only two epochs, one for each stage. The hybrid network is applied to solve the Iris dataset for illustration. Simulation results demonstrate the classification ability and effectiveness of the proposed network.	neurocomputing/S0925231208001458.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'classification', 'neurons', 'samples', 'features', 'length', 'algorithm', 'method', 'dataset', 'classes', 'sample', 'data', 'target', 'contribution', 'species', 'iris', 'normalization', 'width']	To illustrate the advantages of FCCA, we apply our algorithm to a widely used dataset, Iris dataset [8] for classification. It consists of three target classes: Iris Setosa, Iris Virginica and Iris Versicolor. Each species contains 50 data samples. Each sample has four real-valued features: sepal length, sepal width, petal length and petal width. Before training, data are normalized using (1). After normalization, all features of these samples are between zero and one. By doing this, all features have the same contribution to Iris classification. In our method, two output neurons are needed to represent Iris Setosa and Iris Virginica. Samples from Iris Setosa cause the first output neuron to fire. Samples from Iris Virginica cause the second output neuron to fire. And the samples causing neither output neuron to fire belong to Iris Versicolor.	applied_soft_computing/S1568494606001104.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'classification', 'values', 'training set', 'yeast', 'hepatitis', 'table', 'tables', 'classification accuracy', 'sensitivity', 'specificity', 'algorithm', 'dataset', 'lung cancer', 'lung', 'investigation', 'results', 'sets', 'heart', 'testing', 'cancer', 'set', 'average', 'breast cancer', 'term', 'terms', 'addition', 'diabetes', 'breast', 'accuracy', 'datasets']	Tables 5 and 6 show the statistical results for sensitivity, specificity and classification accuracy of MEPDEN and MEPGAN on the training set and the testing set for all datasets. In terms of sensitivity and accuracy, MEPDENf1f2 produced the best results on training set and testing set for all datasets and in average as well. In term of specificity, MEPDENf1f2 provided the best results on training set and testing set for iris dataset. However, MEPGANf1f2 and MEPGANf1-f3 have the best specificity results in the training and testing sets for diabetes, heart, hepatitis, liver, wine datasets and in average as well. The results reported in Table 5 and Figs. 3–5 show that MEPDENf1f2 and MEPGANf1f2 produced the same specificity results for breast cancer, lung cancer, yeast, and QAC datasets. Although MEPDENf1f2 has the best values for sensitivity in all datasets, it was observed that it comparable or competitive algorithm to MEPGANf1f2 regarding to specificity for diabetes, hepatitis and liver datasets on the training set and the testing set. In addition, it can be shown from Table 6 and Figs. 3–5 that MEPDENf1-f3 produced the best sensitivity results for all datasets while MEPGANf1-f3 produced the best specificity results for all datasets and in average as well except QAC dataset. Also MEPDENf1-f3 produced the best accuracy results for all datasets and in average as well except breast cancer dataset. Investigation on the results reported in Tables 5 and 6 reveals that on average, MEPDENf1f2 and MEPDENf1-f3 are superior the other algorithm in terms of sensitivity and accuracy.	applied_soft_computing/S156849461100161X.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'dom', 'table', 'y', 'features', 'case', 'dataset', 'types', 'literature', 'pattern', 'goal', 'order', 'classes', 'partitioning', 'partitions', 'segment', 'class', 'attribute', 'iris', 'pattern recognition', 'partition', 'idea', 'recognition', 'note', 'datasets']	In order to illustrate the idea of partitioning, consider the training set in Table 1 which contains a segment of the Iris dataset. This is one of the best known datasets in the pattern recognition literature. The goal in this case is to classify flowers into the Iris subgeni according to their characteristic features. The dataset contains three classes that correspond to three types of iris flowers: dom(y)={IrisSetosa,IrisV ersicolor,IrisV irginica}. Each pattern is characterized by four numeric features (measured in centimeters): A={sepallength,sepalwidth,petallength,petalwidth}. Tables 2 and 3 respectively illustrate mutually exclusive horizontal and vertical partitions of the Iris dataset. Note that despite the mutually exclusiveness, the class attribute must be included in each vertical partition.	computational_statistics_&_data_analysis/S0167947309002631.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'implementation', 'rule', 'simulations', 'matlab', 'length', 'case', 'classifier', 'attributes', 'instances', 'dataset', 'http', 'test case', 'classes', 'fuzzy classifier', 'class', 'experiments', 'width', 'test']	Sections 6 and 7 discuss implementation of fuzzy classifier and neutrosophic classifier, respectively. For simulations iris dataset (http://archive.ics.uci.edu/ml/datasets/Iris) is used. All experiments have been carried out on MATLAB 7.0 [18]. Iris dataset consists of 4 attributes; sepal length, sepal width, petal length and petal width and has 150 instances which are categorized into three classes: iris-setosa, iris-versicolor and iris-virginica. Thirty instances from each class have been used for training (for making rule set) and 20 from each class have been used as test case.	applied_soft_computing/S1568494612003419.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'network', 'samples', 'tables', 'methods', 'method', 'dataset', 'run', 'number', 'results', 'diversity', 'experts', 'cells', 'variance', 'testing', 'iris', 'performances', 'behavior', 'classifiers', 'face', 'columns']	As noted above, these tables present the results of the methods in the face of insufficient training samples. This insufficiency leads to a poor diversity among the basic classifiers. Regarding to this poor diversity, it has been seen that in some cases, for example the two last columns of Table 2, the performances of the methods are independent from the number of experts. Further, in some cells of Table 2, it can be seen that the variance of the performances of the methods is equal to zero. We have examined the outputs of our method and we have observed that the method is converged to a similar network and the same testing samples are wrongly classified in each run. This behavior is also due to the poor diversity among the basic classifiers, and the nature of the Iris dataset.	applied_soft_computing/S1568494613002834.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'performance', 'validation', 'tables', 'recognition rate', 'algorithm', 'dataset', 'trials', 'rate', 'order', 'time', 'data', 'behaviour', 'sets', 'iris', 'testing', 'set', 'wine', 'disjoint', 'cross validation', 'subsets', 'recognition', 'errors', 'datasets']	Further, in order to evaluate the performance behaviour of our proposed algorithm for unforeseen data, we did a 5-fold cross validation. In this 5-fold cross validation, the datasets are divided in to five disjoint subsets. Each time, we select one of the five subsets as the testing set and remaining four subsets are used as training sets. Then the average errors across all five trials are computed. This five-fold cross validation for Iris dataset is shown in Tables 5 and 6 represents the five-fold cross validation of Wine recognition dataset. It is obvious from Tables 5 and 6 that the recognition rate for previously unseen data of Iris dataset it is 96.62%. Similarly for Wine dataset, it is 97.31%.	applied_soft_computing/S1568494612001457.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'size', 'classification', 'space', 'training set', 'samples', 'table', 'tables', 'classification problem', 'improvement', 'case', 'problems', 'representation', 'method', 'dataset', 'decision', 'phoneme', 'problem', 'number', 'results', 'sample', 'small sample size', 'iris', 'set', 'performances', 'claim', 'small sample', 'accuracy', 'datasets']	According to Table 2, the performances of KNDPR and DT on Iris dataset are quite similar except in the case of 40 training samples. Further, Table 3 represents the results on Satimage dataset, which is a complex classification problem. Based on these results, KNDPR remarkably outperformed DT (about three percent improvement), implying that KNDPR can achieve higher accuracy in classifying complex problems when a small number of training samples are available. Considering the obtained results on the other two datasets, Phoneme and Pendigits, provided in Tables 4 and 5 demonstrates that higher classification improvement is achieved when a smaller training set is exploited. Examining and analyzing the above mentioned results confirm the claim that our proposed method conquers the small sample size problems through suggesting a proper representation for the decision space.	applied_soft_computing/S1568494613002834.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'size', 'pan', 'values', 'performance', 'space', 'samples', 'vowels', 'measures', 'feature space', 'image', 'length', 'problems', 'digits', 'algorithm', 'syllables', 'method', 'attributes', 'dataset', 'types', 'aim', 'phoneme', 'plant', 'ta', 'classes', 'nasal', 'sample', 'pixels', 'feature', 'vowel', 'satellite image', 'iris', 'database', 'uci', 'width', 'neighborhoods', 'hand', 'test', 'sample size', 'files', 'information', 'datasets']	To examine the performance of our method over problems with small and large sample sizes, high dimensional feature space, and numerous classes, we applied our algorithm on four different datasets of the UCI database [12]: Iris, Satimage, Phoneme and Pendigits. The famous Iris dataset contains numerous samples of three different types of iris plant namely Iris Setosa, Iris Versicolour and Iris Verginica based on 4 measures of sepal's width and length, and petal's width and length. Satimage, on the other hand, contains multi-spectral values of pixels in 3×3 neighborhoods in a satellite image. Next, the aim of Phoneme dataset is to distinguish between nasal and oral vowels. This dataset contains vowels coming from 1809 syllables (pa, ta, pan, …). Five different attributes were chosen to characterize each vowel. Finally, the last dataset we used is Pendigits whose aim is to recognize handwritten digits. This dataset is partitioned into two distinct files: pendigits.tra and pendigits.tes, consisting of 7494 training samples and 3498 test samples, respectively. Due to large sample size, 2000 of samples are extracted randomly from these datasets. Table 1 summarizes the information of the four aforementioned datasets.	applied_soft_computing/S1568494613002834.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'step', 'som', 'features', 'dataset', 'indications', 'baseline', 'detection', 'class', 'iris', 'testing', 'capability', 'bip']	Step 4: Based the selected features, the baseline SOM is constructed by using the normal training dataset. To implement the abnormal detection based on NLLP and BIP indications, the iris dataset is separated into two sub-datasets: one with Class 2 and 3 is used as the training dataset (i.e., it is used as the normal dataset), and the other with the Class 1 is used as the testing dataset (i.e., it is used as the abnormal dataset). The training dataset is used to train the baseline SOM, and then the testing dataset is used to verify the abnormal detection capability of SOM-based NLLP and BIP indications.	applied_soft_computing/S1568494611001220.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['training', 'values', 'repository', 'validation', 'dataset', 'records', 'machine learning', 'parameter', 'approach', 'd', 'number', 'generation', 'data', 'training data', 'iris', 'procedure', 'uci', 'learning', 'machine']	To demonstrate the procedure of the proposed AISCBR approach, one generation of the AISCBR was used to solve the iris dataset in the UCI Machine Learning Repository [57]. The parameter values for AISCBR were set as follows: Npop, n, β, d were 5, 3, 3, and 5. The number of records of iris dataset in the training data, the testing data and the validation data were 90, 30 and 30.	applied_soft_computing/S1568494611002146.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['transaction', 'cameras', 'tablets', 'market', 'resolution', 'conditions', 'authentication', 'high resolution', 'mobiles', 'method', 'dataset', 'insight', 'paper', 'approach', 'wave', 'task', 'fingerprint', 'invariance', 'topic', 'readiness', 'biometrics', 'diffusion', 'class', 'data', 'results', 'technology', 'mobile technology', 'sensors', 'iris', 'means', 'experimental results', 'smartphones', 'platform', 'histograms', 'identity', 'recognition', 'iris recognition', 'exchange', 'devices', 'requirement', 'generations', 'mobile devices', 'face', 'applications', 'subjects', 'identification', 'accuracy']	The worldwide diffusion of latest generations mobile devices, namely smartphones and tablets, represents the technological premise to a new wave of applications for which reliable owner identification is becoming a key requirement. This crucial task can be approached by means of biometrics (face, iris or fingerprint) by exploiting high resolution imaging sensors typically built-in on this class of devices, possibly resulting in a ubiquitous platform to verify owner identity during any kind of transaction involving the exchange of sensible data. Among the aforementioned biometrics, iris is known for its inherent invariance and accuracy, though only a few works have explored this topic on mobile devices. In this paper a comprehensive method for iris authentication on mobiles by means of spatial histograms is described. The proposed approach has been tested on the MICHE-I iris dataset, featuring subjects captured indoor and outdoor under controlled and uncontrolled conditions by means of built-in cameras aboard three among the most diffused smartphones/tablets on the market. The experimental results collected, provide an interesting insight about the readiness of mobile technology with regard to iris recognition.	pattern_recognition_letters/S0167865514003286.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['type', 'dataset', 'fisher', 'literature', 'plant', 'classes', 'work', 'references', 'class', 'iris', 'pattern recognition', 'examples', 'recognition', 'turn', 'field']	In literature, the Iris dataset is probably the best known dataset for pattern recognition. The article by Fisher [13] is a classic in the field and references to his work are frequent [9]. This dataset contains 3 classes of 50 examples each, each class referring to a type of Iris plant. The Setosa class can be separated linearly from the other two (Virginia, Versicolor), which, in turn, cannot.	applied_soft_computing/S1568494611003358.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['university', 'rating', 'clusters', 'performance', 'similarity', 'recommender', 'online', 'measures', 'quality', 'phases', 'products', 'algorithms', 'recommendation', 'effectiveness', 'clustering', 'algorithm', 'problems', 'dataset', 'overload', 'opinions', 'paper', 'ars', 'recommendations', 'form', 'california', 'number', 'recommender system', 'user', 'results', 'system', 'shopping', 'world', 'iris', 'k-means', 'ants', 'database', 'sparsity', 'internet', 'experimental results', 'similarity measures', 'website', 'information overload', 'phase', 'matrix', 'web', 'users', 'mechanism', 'personalized recommendation', 'information', 'marketplace']	In recent years, there is overload of products information on world wide web. A personalized recommendation is an enabling mechanism to overcome information overload occurred when shopping in an Internet marketplace. This paper proposes a novel centering-bunching based clustering (CBBC) algorithm which is used for hybrid personalized recommender system (CBBCHPRS). The proposed system works in two phases. In the first phase, opinions from the users are collected in the form of user-item rating matrix. They are clustered offline using CBBC into predetermined number clusters and stored in a database for future recommendation. In the second phase, the recommendations are generated online for active user using similarity measures by choosing the clusters with good quality rating. This helps to get further effectiveness and quality of recommendations for the active users. The experimental results using Iris dataset show that the proposed CBBC performs better than K-means and new K-medodis algorithms. The performance of CBBCHPRS is evaluated using Jester database available on website of California University, Berkeley and compared with ants recommender system (ARS). The results obtained empirically demonstrate that the proposed CBBCHPRS performs superiorly and alleviates problems such as cold-start, first-rater and sparsity.	expert_systems_with_applications/S0957417411011316.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['values', 'performance', 'algorithm', 'dataset', 'runs', 'statistical analysis', 'fcm', 'partitioning', 'results', 'score', 'iris', 'addition', 'differences', 'analysis']	It can be observed from Table 2 that, the partitioning on Iris dataset using PSDFCM algorithm gives the most unsatisfactory performance, since the achieved average MS score of 0.9629 is the highest among others. It can be inferred from Table 6 that both of FCM and MSSLFCM algorithms demonstrate comparable partitioning results. In addition, except the BFCM, statistical analysis reveals that the differences in the MS values from MSSLFCM algorithm over other algorithms in thirty runs are statistically significant.	applied_soft_computing/S1568494615003701.txt
created_by (Iris Dataset: dataset, ?:author)	['Fisher']	['visual interface', 'genetic algorithm', 'clusters', 'mapping', 'space', 'cluster structure', 'clustering algorithm', 'overview', 'solutions', 'cluster validity', 'knowledge', 'experience', 'process', 'algorithm', 'cluster', 'method', 'dataset', 'input', 'parameter', 'shapes', 'advantage', 'operations', 'boundary', 'chen', 'structure', 'results', 'data', 'result', 'comparison', 'model', 'system', 'sets', 'iris', 'sense', 'set', 'domain knowledge', 'plan', 'rendering', 'domain', 'validity', 'interface', 'map', 'adjustment', 'users', 'errors']	Keke Chen applied visual rendering clustering algorithm on the Iris dataset. The system implements a linear mapping model to visualize k-dimensional data sets in a 2D star-coordinate space; then it provides a set of interactive rendering operations to enable users to validate and interactively refine the cluster structure based on their visual experience as well as their domain knowledge. Using this method, Chen successfully divided the data set into three clusters. But, this system needs manual parameter adjustment to get a better separate map and manual boundary set. These are inefficient and may cause some errors. Without needing such manual process, MOKGA successfully grouped the data set into three clusters. Results clearly show that separating them into two clusters is also reasonable. This can be verified from the map delivered by the visual rendering method. In comparison to the visual rendering method, MOKGA has the following advantages: it is more efficient in the sense that no user’s input is required during the clustering process, and it also can give users a more clear cluster validity result so that users can get an overview about the dataset. But, the visual rendering method has the advantage that users can get a visual clustering result and it may work well in dealing with clusters of irregular shapes. We have a plan to extend MOKGA with a visual interface which will be capable of displaying the alternative clustering solutions and how they evolve during the genetic algorithm process.	knowledge-based_systems/S0950705113003535.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['candidate', 'large databases', 'training', 'retrieval', 'classification', 'query', 'training set', 'conjunction', 'samples', 'cluster structure', 'strategy', 'alternatives', 'digit', 'process', 'digits', 'cluster', 'method', 'dataset', 'literature', 'classes', 'groups', 'structure', 'class', 'first', 'recall', 'comparison', 'euclidean', 'euclidean distance', 'sense', 'set', 'database', 'raster', 'idea', 'object', 'item', 'pairwise', 'images', 'imagery', 'graph', 'mnist', 'hand', 'databases', 'information retrieval', 'information', 'rank', 'distance']	For purposes of classification, we note that we can use the posteriors given by αωi in conjunction with the cluster structure in the MNIST database so as to search for the most similar object in a manner akin to that in [60]. The idea is to find the digit-classes which are most similar to the testing imagery. This can be viewed as a query in which the graph within the database-classes that is most similar to the query is the one that is retrieved. The search process is as follows. First, we compute the set of posteriors for the testing digit for each class in the dataset. The classes with the most similar digits are expected to have larger posteriors in comparison with the others. Thus, we select the first 3 classes, i.e. digit groups, in the training set which correspond to those posteriors αωi that are the largest in rank amongst those computed for the testing raster scan. With the candidate classes at hand, we search the training samples in each class so as to find the one that is most similar to the testing digit. The testing raster scan then belongs to the class whose item, i.e. digit, corresponds to the minimum pairwise Euclidean distance. It must be stressed that this can also be viewed as a simple recall strategy which illustrates how the embedding method presented here can be used for purposes of retrieval in a broader sense. The information retrieval literature contains more principled and more efficient alternatives which should be used if the example given here is scaled to very large databases of thousands or tens of thousands of images.	computer_vision_and_image_understanding/S1077314211000737.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['exploration', 'training', 'prototypes', 'classification', 'mapping', 'equivalence', 'network', 'neurons', 'formation', 'neural network', 'ram', 'process', 'digits', 'extension', 'ability', 'input', 'categories', 'response', 'benchmark', 'device', 'frequency', 'filtering', 'rules', 'drasiw', 'neural network model', 'work', 'model', 'positions', 'patterns', 'addresses', 'database', 'examples', 'wisard', 'functionality', 'idea', 'recognition', 'phase', 'images', 'mnist', 'skills', 'fuzzy rules', 'i', 'field', 'database of handwritten digits']	The WiSARD (Wilkie, Stonham and Aleksander's Recognition Device) weightless neural network model has its functionality based on the collective response of RAM-based neurons. WiSARD's learning phase consists on writing at the RAM neurons’ positions addressed (typically through a pseudo-random mapping) by binary training patterns. By counting the frequency of writing accesses at RAM neuron positions during the learning phase, it is possible to associate the most accessed addresses with the corresponding input field contents that defined them. The idea of associating this process with the formation of “mental” images is explored in the DRASiW model, a WiSARD extension provided with the ability of producing pattern examples, or prototypes, derived from learnt categories. This work demonstrates the equivalence of two ways of generating such prototypes: (i) via frequency counting and filtering and (ii) via formulating fuzzy rules. Moreover, it is shown, through the exploration of the MNIST database of handwritten digits as benchmark, how the process of mental images formation can improve WiSARD's classification skills.	neurocomputing/S0925231210000159.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['filter', 'error', 'performance', 'state', 'function', 'error rate', 'online', 'image', 'logistic function', 'improvement', 'code', 'representation', 'algorithm', 'dataset', 'approach', 'rate', 'art', 'codes', 'database', 'sparse representation', 'learning', 'linear filter', 'encoder', 'online algorithm', 'mnist', 'note']	The state of the art approach uses a linear filter for encoding an image into a code and a sparsifying logistic function that the code goes through before being decoded by another linear filter [12]. The learning is achieved by training the encoder to produce codes whose sparse representation accurately reconstruct the original image after decoding. The error rate on the MNIST database reported is 0.39%. Note that we use this dataset to evaluate the performance of our online algorithm and we do not claim improvement over the state of the art performance of an offline algorithm on this specific dataset.	image_and_vision_computing/S0262885609002364.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['performance', 'repository', 'table', 'features', 'alphabet', 'digits', 'solutions', 'attributes', 'method', 'numerical experiments', 'dataset', 'dna sequences', 'letter', 'segment', 'outdoor', 'dna', 'satellite images', 'dimensionality', 'pixel', 'experiments', 'database', 'uci', 'uci repository', 'ref', 'use', 'sequences', 'images', 'mnist', 'neighborhoods', 'satellite', 'english', 'datasets']	We conduct numerical experiments with 5 different datasets from the UCI Repository [28] as well as the MNIST database [24] to compare the performance of the proposed method to those of the classical solutions presented in Ref. [29] (see Table 1). The Letter and Segment datasets (low dimensionality) use 16 and 19 primitive numerical attributes and represent, respectively, features extracted from images of capital letters in the English alphabet and hand-segmented outdoor images. The Satellite dataset is based on 36 attributes extracted from satellite images in 3×3 pixel neighborhoods in 4 spectral bands. With 114 numerical and 180 logical attributes, respectively, the MNIST and DNA datasets represent features extracted from images of handwritten digits and DNA sequences. These last two datasets are considered high dimensional [24].	applied_soft_computing/S1568494610002553.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['projection methods', 'end', 'training', 'fig', 'classification', 'size', 'variety', 'training set', 'ocr', 'methods', 'image', 'digits', 'attributes', 'algorithm', 'ability', 'method', 'dataset', 'aim', 'approach', 'paper', 'literature', 'list', 'results', 'sample', 'study', 'testing', 'set', 'database', 'techniques', 'projection', 'section', 'use', 'images', 'subspace', 'mnist', 'note', 'reader', 'scope']	Now we illustrate the ability of our method to embed graph attributes extracted from a real-world dataset. To this end, we show results on digit classification making use of the MNIST dataset. The MNIST database which contains a training set of 60,000 grey-scale images of handwritten digits from 0 to 9, and a testing set of 10,000 images. Sample handwritten digits in the dataset are shown in Fig. 7. The image sizes are 28×28. We have chosen the dataset due to its size and its widespread use in the literature. We stress in passing that the aim in this section is not to develop an OCR algorithm but rather to provide a quantitative study on a large, real-world dataset comparing our embedding approach with other subspace projection methods. Note that there are a wide variety of OCR techniques that have been applied to the dataset. As these are out of the scope of this paper, for a detailed list we refer the reader to the MNIST website.2http://yann.lecun.com/exdb/mnist/.2	computer_vision_and_image_understanding/S1077314211000737.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['size', 'aspect', 'shape analysis', 'methods', 'binary images', 'image', 'digits', 'dataset', 'box', 'descriptors', 'shape', 'technology', 'pixel', 'shape descriptors', 'national', 'nist', 'ratio', 'database', 'mixed', 'aspect ratio', 'standards', 'images', 'experiment', 'mnist', 'analysis', 'columns']	In the third experiment, we used images from the MNIST database (Mixed National Institute of Standards and Technology database) [28], which is a public image dataset that contains 10,000 images of handwritten digits from the NIST Special Database 1 (SD-1) and Special Database 3 (SD-3), to assess our shape descriptors. The original binary images from SD-1 and SD-3 were size normalized to fit in a pixel box with 20 rows per 20 columns (preserving their aspect ratio) and centered in grayscale images with 28 rows per 28 columns (which were binarized before applying the shape analysis methods tested).	computer_vision_and_image_understanding/S1077314214001374.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['tests', 'profile', 'digits', 'goal', 'order', 'groups', 'database', 'writers', 'pattern recognition', 'recognition', 'images', 'terms', 'databases', 'mnist', 'people', 'datasets']	Since the writers of the digits in the SD-1 and SD-3 databases belong to two very distinct groups of people (in social and cultural terms), the goal of the MNIST database is to mix images from both datasets in order to make the tests with pattern recognition algorithms more complex and realistic, i.e., more independent from the social and economic profile of the writers [28].	computer_vision_and_image_understanding/S1077314214001374.txt
description_of (MNIST Database: dataset, ?:description)	['Database of handwritten digits']	['toy', 'space', 'image', 'correspondence', 'digits', 'clockwise', 'number', 'domains', 'rotation', 'data', 'pixels', 'model', 'set', 'database', 'objects', 'section', 'domain', 'share', 'mnist', 'information', 'degrees']	First, we demonstrate the proposed model described in Section 3 using a toy data set with three domains, which is created using handwritten digits from the MNIST database (LeCun, Bottou, Bengio, & Haffner, 1998). The first domain contains original handwritten digits, where each image is downsampled to 16 × 16 pixels. We synthesize objects for the second and third domains by rotating handwritten digits by 90 and 180 degrees, clockwise, respectively. Thus, we obtain three-domain objects that share a latent space. The number of objects in each domain is 200 for all domains. We would like to match the rotated digits in different domains without information about rotation or correspondence.	information_processing_&_management/S0306457315001508.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['candidate', 'test sets', 'bisection', 'performance', 'function', 'ionosphere', 'bisection method', 'samples', 'methods', 'objective function', 'value', 'sensitivity', 'indication', 'method', 'g-means', 'dataset', 'decision', 'parameter', 'models', 'local optimum', 'results', 'sample', 'balances', 'optimum', 'sets', 'heart', 'boundaries', 'selection', 'set', 'eq', 'kernel', 'edge', 'cardiotocography', 'interval', 'terms', 'parameters', 'cost', 'hand', 'dtl', 'test', 'iterations', 'diabetes', 'errors', 'ocsvm', 'breast', 'datasets']	In terms of the g-mean metric, as shown in Table 4, IMIES is significantly superior to MIES on five datasets (Heart, Ionosphere, Breast, Cardiotocography and Spambase). Even for other three datasets, our method still gives comparable results to MIES. While accompanying low TNRs, interestingly, MIES achieves rather high TPRs on the Breast, the Cardiotocography and the Spambase datasets, from which we can know that the kernel parameters selected by MIES on these datasets produce overfitting decision boundaries. The high test accuracies on the positive samples (i.e., high TPRs) are obtained at the cost of the high test errors on the negative samples (i.e., low TNRs), which finally results in the low g-means. This may be caused by the improper selection of the edge or the interior sample set. For example, regarding the Spambase dataset, if we use the new objective function Eq. (20) but still select the edge samples using EISDTP, then MIES can obtain a kernel parameter value that yields a g-mean of 47.93%, larger than the original g-mean of 19.13% (see Table 4); furthermore, if we use the new objective function and use ESDCP to select the edge samples, namely, using the proposed IMIES, the g-mean further increases to 72.36 % (see Table 4). This is an indication that EISDTP employed in the original MIES has selected the improper edge or interior sample set. On the other hand, IMIES also outperforms DTL on five datasets (Biomed, Heart, Ionosphere, Breast and Cardiotocography) and receives comparable g-means on the Diabetes and the Svmguide1 datasets. DTL gives unstable performance. For example, the performance of DTL can be comparable to that of IMIES on the Diabetes and the Svmguide1 datasets and even better on the Spambase dataset. But for the Heart and the Ionosphere datasets, it results in overfitting models, with high TPRs but low TNRs. The unstable performance is due to the fact that DTL gets a candidate kernel parameter by the bisection method in its iterations [27]; however, the bisection method could easily get stuck in a bad local optimum because of its sensitivity to the initial candidate interval of the kernel parameter. Among these methods, only our method achieves high g-means on the test sets and gets good balances between TPRs and TNRs. In summary, compared with other two methods, the proposed IMIES is able to select more suitable kernel parameters for OCSVM.	knowledge-based_systems/S095070511500355X.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['classification', 'classification performance', 'mlp', 'rates', 'algorithm', 'dataset', 'frequency', 'characters', 'detection', 'recall', 'model', 'significance', 'synthesis', 'attribute', 'ratio', 'modeling', 'words', 'rankings', 'accuracy', 'messages', 'training', 'methods', 'stage', 'classification accuracy', 'value', 'classifier', 'method', 'insight', 'software', 'literature', 'data reduction', 'neural networks', 'measure', 'comparison', 'means', 'subsets', 'use', 'machine', 'precision', 'paradigms', 'networks', 'information', 'predictors', 'performance', 'approach', 'model development', 'time', 'relative importance', 'classifiers', 'reduction', 'abductive network', 'development', 'network', 'ranking', 'features', 'spam detection', 'attributes', 'input', 'learning algorithm', 'exclusion', 'probabilistic neural networks', 'models', 'groups', 'work', 'data', 'feature', 'ensembles', 'importance', 'item', 'improvements', 'difference']	We have demonstrated the use abductive machine learning for spam detection and the ranking of 57 spam detection attributes of the spambase dataset according to their predictive value. In general, the proposed approach offers the following advantages: improved classification accuracy, greater insight into the email feature set, data reduction through automatic exclusion of insignificant input features, simpler model development, and reduced training time. Classification accuracies of about 91.7% were achieved with a single model that automatically selects only 10 input attributes, thus achieving data reduction by a ratio of approximately 6:1. Accuracy was further improved up to 92.4% through the use of a 3-member abductive network ensembles with the members trained on different subsets of the training set using all 57 attributes. Abductive modeling was compared with probabilistic neural networks employing a genetic learning algorithm and developed using the NeuroShell Classifier software. Results indicate that the abductive approach gives better classification accuracy and requires much shorter training times. Three methods were adopted for ranking the 57 attributes to determine the most effective spam predictors: (1) The Z-statistic measure of the significance of the difference between the two means of each attribute for the spam and legitimate subsets of the data, (2) A GMDH-based approach of iteratively forcing the synthesis of a simple model and excluding the inputs selected at each stage, and (3) Information provided by the NeuroShell Classifier on the relative importance of inputs to the genetic learning algorithm. The top most effective 12 predictors in each of the above three rankings have 9 attributes in common. The same 9 attributes are automatically selected by the 10-input optimum monolithic abductive network model. These attributes highlight the greater frequency of certain words or characters in either spam or legitimate messages. Performance was compared with that of MLP neural networks and naïve Bayesian classifiers reported in the literature for the same email dataset. The comparison indicates that the abductive network models provide better classification accuracy, spam recall and spam precision with false-positive rates as low as 4.3%. Future work would consider methods for finer ranking within attribute groups in the GMDH-based method to achieve complete ranking of the item set, and the use of ensembles with members trained using different learning paradigms to achieve further improvements in classification performance.	applied_soft_computing/S1568494609002610.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['fig', 'roc curves', 'performance', 'imbalance', 'methods', 'roc analysis', 'proposal', 'dataset', 'curves', 'roc', 'result', 'k', 'instance', 'representative', 'ratio', 'experiments', 'section', 'claim', 'smote', 'volume', 'analysis']	To further justify our claim, we present a ROC analysis result with the spambase dataset. This dataset is considered here since it has a moderate imbalance ratio and instance volume. The original spambase has an imbalance ratio of 10; therefore, in this experiments, we test K from 1 to 9. Through experiments in Section 6.3.1, we notice that SMOTE achieves a better performance comparing to other state-of-the-art methods; we hence adopt it as a representative to compare with our proposal. Finally, we depict the ROC curves of the two approaches in Fig. 7. Clearly, EnSVM+ outperforms the other two methods in general.	information_processing_&_management/S030645731000097X.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['message', 'dataset', 'parameter', 'emails', 'characters', 'population', 'account', 'variations', 'magnitude', 'words', 'messages', 'values', 'value', 'correlation', 'statistic', 'z', 'results', 'class', 'character', 'means', 'database', 'subsets', 'predictors', 'performance', 'instances', 'paper', 'approach', 'name', 'list', 'instance', 'average', 'mean', 'labs', 'sequences', 'power', 'keyword', 'end', 'ranking', 'attributes', 'input', 'email', 'percentage', 'models', 'order', 'number', 'deviation', 'populations', 'difference', 'column', 'standard deviation']	To evaluate and compare the performance of the proposed approach, we used the spambase dataset [42]. This database has been created in June–July 1999 by M. Hopkins, E. Reeber, G. Forman, and J. Suermondt at Hewlett-Packard Labs. It consists of 4601 instances of legitimate and spam email messages with 39.4% being spam. Each instance is characterized by 57 input attributes and is labeled as spam (represented as 1) or legitimate (represented as 0). Table 2 lists the attribute number and name, as well as the average and standard deviation values for each attribute in both the legitimate and spam populations. Attributes 1–48 give the percentage of words in the email message for the respective keyword indicated in the attribute name. Attributes 49–54 give the percentage of characters in the email message for the respective character indicated in the attribute name. Attributes 55 and 56 give the average and maximum lengths, respectively, of uninterrupted sequences of capital letters in the message. Attribute 57 gives the total number of capital letters in the message. The attribute number will be used as the variable number for models described throughout this paper. Attribute number 58 in the dataset is the true email class. The dataset has no missing attribute values. Table 2 also lists the mean and standard deviation of each attribute in both the legitimate population (μl and σl respectively) and spam population (μs and σs respectively). The last column in Table 2 gives the absolute value of the Z statistic for each attribute. The z parameter measures the magnitude of the absolute difference between the two means of a given attribute in the legitimate and spam populations relative to variations about the two means. The larger the value of z, the more significant is the difference between the two means and therefore the larger the value discriminatory power of the attribute in classifying the dataset as spam/legitimate. Sorting the list of input attributes in a descending order based on the values of their Z statistic gives the following ranking: {21, 25, 26, 19, 23, 7, 53, 52, 16, 5, 27, 17, 57, 6, 11, 9, 30, 37, 3, 24, 18, 35, 28, 8, 46, 56, 45, 42, 43, 29, 15, 36, 31, 20, 39, 33, 32, 10, 34, 1, 13, 41, 44, 48, 55, 50, 22, 40, 51, 49, 14, 54, 47, 4, 38, 2, 12}. Attributes toward the beginning of the list should prove more effective than attributes toward the end. However, this simple ranking approach does not take into account possible mutual correlation between various attributes when grouped together to classify the dataset. Since using several attributes is more common in discriminating spam from legitimate emails and leads to better results, forming subsets of attributes based on the ranking of individual attributes may not produce compact subsets of predictors and may miss complementary attributes that have low individual ranking.	applied_soft_computing/S1568494609002610.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['messages', 'end', 'training', 'fig', 'position', 'estimate', 'ranking', 'bar', 'value', 'spam detection', 'statistic', 'presence', 'subset', 'classifier', 'attributes', 'dataset', 'input', 'software', 'keywords', 'z', 'list', 'name', 'number', 'characters', 'results', 'detection', 'feature', 'model', 'importance', 'spam', 'relative importance', 'set', 'agreement', 'researcher', 'schemes', 'affiliation', 'chart', 'lists', 'words', 'row']	The NeuroShell Classifier software provides an estimate of the relative importance of each input used to train the model, which is updated during training. It would be interesting to compare results using this feature with the ranking based on the Z statistic and the tentative GMDH-based ranking. Unfortunately, the NeuroShell ranking is reliable only for a small number of inputs, e.g. up to 20 inputs [45], and therefore could not be reliably used to give an overall ranking of the full set of 57 attributes of the spambase dataset. Fig. 4 is a bar chart depicting the importance of input attributes, which is produced by the software at the end of training for model 3 in Table 8 on the top 19 attributes. According to Fig. 4, the most important 12 of these attributes are {57, 27, 25, 53, 23, 52, 37, 56, 7, 5, 21, 16}. As shown in Fig. 5, this subset contains 9 out of the 12 attributes at the top of the Z statistic ranking list and 10 out of the 12 attributes at the top of the GMDH-based ranking list. This indicates reasonable agreement between the three ranking schemes considered. In all three ranking lists, attribute 23 occupies the 5th position from the top, while attribute 5 occupies the 10th position. The 9 attributes {5, 7, 16, 21, 23, 25, 27, 52, 53}, which are shaded in Fig. 5, are common to the top 12 attributes in all three ranking lists. It is interesting to note that all these 9 attributes are included in the subset of 10 input attributes selected automatically by the optimum monolithic abductive model with CPM=1 (middle row in Table 3). Rows for these most effective spam detection attributes are shaded in Table 2. All these 9 attributes are characterized by a large value of the Z statistic. Seven attributes indicate a larger presence of the keywords “our”, “remove”, “free”, “your”, “000” and the characters “!” and “$” in spam messages. The remaining two attributes indicate a larger presence in legitimate messages of the words “george” and “hp”, which refer to the first name and the affiliation of the researcher who developed the spambase dataset.	applied_soft_computing/S1568494609002610.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['model complexity', 'classification', 'training set', 'classification performance', 'evaluation', 'specificity', 'subset', 'effectiveness', 'process', 'dataset', 'parameter', 'levels', 'model', 'attribute', 'section', 'modeling', 'terms', 'matrix', 'accuracy', 'datasets', 'messages', 'training', 'classification accuracy', 'value', 'robustness', 'classifier', 'z', 'fn', 'results', 'positions', 'complexity', 'subsets', 'model evaluation', 'experiment', 'precision', 'binary classification', 'receiver', 'outcome', 'predictors', 'performance', 'confusion', 'paper', 'approach', 'cpm', 'sets', 'set', 'parameters', 'operation', 'abductive network', 'operator', 'aspect', 'network', 'confusion matrix', 'fp', 'sensitivity', 'attributes', 'structures', 'input', 'simpler', 'email', 'percentage', 'models', 'number', 'spam', 'ensembles', 'negative', 'characteristics', 'thresholds', 'threshold', 'errors']	Abductive network models were developed for classifying the spambase dataset into spam or legitimate messages. The dataset was randomly split into a training set of 2844 cases and an evaluation set of 1757 cases. The percentage of spam in the two datasets was 38.4% and 41.1%, respectively. In the first modeling experiment, the full training set was used to develop abductive network models that utilize all available 57 attributes for spam classification. We refer to such models as monolithic models to contrast them with those developed later using the modular approach of abductive network ensembles to be described in Section 5.2. Table 3 shows the abductive model structures synthesized at three levels of model complexity as indicated by the CPM parameter specified prior to training, together with their classification accuracy on both the training and evaluation sets. The variable number indicated at a model input, e.g. Var_i, corresponds to the number of the attribute selected as input to the model during training. Var_58 is the binary classification model output (i.e. spam or legitimate). As the CPM value increased, simpler models are synthesized. In all three models developed with CPM=0.3, 1, and 3; only a small subset of the 57 input attributes is selected automatically during training. The number of selected attributes is 10, 10, and 9, respectively. Eight attributes {7, 16, 21, 23, 25, 46, 52, and 53} are common to all three input subsets. This indicates the effectiveness of such attributes as spam predictors and the robustness of the modeling process. During model evaluation on the evaluation set, computed model output was rounded to 0 (legitimate) or 1 (spam) based on a simple threshold of 0.5. We will later consider operation at other thresholds by presenting the receiver operator characteristics in Section 5.4. Classification performance on the external evaluation set is approximately equal to that on the training set. Best classification performance on the evaluation set is obtained using the optimum model with CPM=1 which gives a classification accuracy of 91.7%. This model will be considered as the optimum monolithic model throughout the paper. The subset of 10 input attributes selected by this model is {5, 7, 16, 21, 23, 25, 27, 46, 52, and 53}. Eight of these 10 attributes are among the top 10 positions of the Z statistic ranking given in Section 4, which confirms their superior explanatory qualities. Table 4(a) shows the resulting confusion matrix and Table 5(a) lists the parameters characterizing the classification performance of this optimum model on the evaluation set, in terms of sensitivity, specificity, “spam” precision, and “legitimate” precision, as well as classification accuracy. The results indicate a minimum value of approximately 88.2% for all performance parameters. A good aspect of this classifier is that specificity is greater than sensitivity. This means fewer false-positive (FP) classification errors where a legitimate email is wrongly classified as spam email, which is a more serious outcome compared to a spam being considered as legitimate i.e. a false negative (FN). Table 4(a) gives the number of FP and FN errors as 61 and 85, respectively, out of the 1757 evaluation cases.	applied_soft_computing/S1568494609002610.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['nb', 'classification', 'values', 'f-measure', 'performance', 'authors', 'spam filtering', 'validation', 'measures', 'evaluation', 'classification accuracy', 'improvement', 'mlp', 'subset', 'effectiveness', 'classifier', 'dataset', 'contrast', 'paper', 'approach', 'literature', 'filtering', 'cutoff', 'rate', 'models', 'neural networks', 'naïve bayesian classifier', 'time', 'work', 'recall', 'results', 'performance measures', 'model', 'spam', 'sp', 'multilayer perceptron', 'architectures', 'experiments', 'classifiers', 'fpr', 'fm', 'cross validation', 'section', 'subsets', 'experiment', 'acc', 'definitions', 'threshold', 'fnr', 'precision', 'terms', 'networks', 'perceptron', 'comparisons', 'sr', 'accuracy']	The effectiveness of using multilayer perceptron neural networks and naïve Bayesian classifiers has been recently evaluated for spam filtering on the spambase dataset used in this paper [48]. In [48], the dataset was randomly shuffled and then partitioned into five independent subsets using 5-fold cross validation. Five experiments were conducted. In each experiment, four subsets were used for training the classifier and the remaining subset was used for evaluation. The authors of [48] examined various MLP architectures to determine the best classifier model which was then tested for various cutoff threshold values (see Table 6 in [48]). They also tested the naïve Bayesian classifier (see Table 7 in [48]). Our comparisons with their best models are based on classification accuracy (Acc), false-positive rate (FPR), false-negative rate (FNR), spam recall (SR), spam precision (SP) and F-measure (FM). Table 9 summarizes the performance of our four abductive modeling experiments of Table 5 of this paper, and that of the five MLP experiments and five NB experiments reported in [48]. In contrast to their work, we label “spam” as positive and “legitimate” as negative, which is more common in anti-spam literature, and we recalculated the performance measures for their work according to the definitions in Section 3. The results indicate that our proposed approaches yield much better performance compared to the naïve Bayesian classifier in all cases and offer a slight improvement on the MLP models in most of the cases in terms of classification accuracy, spam recall and spam precision. Moreover, unlike the MLP that requires long training times (see Section 7 in [48]), the GMDH-based approach requires much shorter training time as shown in Table 8.	applied_soft_computing/S1568494609002610.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['product', 'application', 'values', 'function', 'estimation', 'technique', 'distribution function', 'method', 'dataset', 'machine learning', 'imputation', 'empirical likelihood', 'research', 'paper', 'approach', 'topic', 'uncertainty', 'emails', 'likelihood', 'detection', 'confidence', 'spam', 'missing values', 'means', 'mean', 'uci', 'change detection', 'researchers', 'populations', 'learning', 'distribution', 'interval', 'confidence interval', 'change', 'machine', 'cis', 'differences', 'power', 'datasets']	Detecting differences between populations (or datasets) is an important research topic in machine learning, yet an common application means of evaluating, such as a new medical product by comparing with an old one. Previous researchers focus on change detection. In this paper, we measure the uncertainty of structural differences, such as mean and distribution function differences, between populations, using a confidence interval (CI), via an empirical likelihood approach. We present a statistically sound method for estimating CIs for differences between non-parametric populations with missing values, which are imputed by using simple random hot deck imputation method. We illustrate the power of CI estimation as a new machine learning technique for, such as, distinguishing spam from non-spam emails in spambase dataset downloaded from UCI.	pattern_recognition_letters/S0167865508000068.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['size', 'feature selection', 'classification', 'filter', 'step', 'methods', 'classification accuracy', 'features', 'case', 'subset', 'large datasets', 'method', 'dataset', 'gain', 'time', 'execution time', 'execution', 'results', 'feature', 'wrapper', 'information gain', 'selection', 'hand', 'information', 'accuracy', 'datasets']	Additionally, the execution time of the proposed method has been also compared to those of the wrapper-based feature selection methods on the different datasets and the obtained results are reported in Table 13. The reported results show that in most cases the execution time of the proposed method is lower than the wrapper based methods. For example it can be seen from the results that the GCACO selected the final subset of features for Spambase dataset after 6846ms. While in this case the HGAFS, ACOFS and HGAFS selected the final subset after 98,328, 87,193 and 531,439ms respectively. In this case the results show the proposed method is nearly 88 times faster than the PSOFS method. While the results of Table 10 show that the accuracy of PSOFS method is only 0.5 times higher than GCACO method. Moreover, the results show that for very large datasets such as Arcene the execution time of the HGAFS and ACOFS methods are lower than the proposed method. This is due to the fact that these methods are executed in two steps. In the first step they applied a filter based method (such as Information gain or Gini-index) to rank the features independently and then a small size subset (i.e. a subset with only 100 top ranked features) are selected to be incorporated a wrapper based method in the second step. While in this case the proposed method is applied over all of the original features. On the other hand Table 10 results show that in this case the classification accuracy of the proposed method is much higher than the HGAFS and ACOFS methods.	knowledge-based_systems/S0950705115001458.txt
tasks_annotated_for (Spambase Dataset: dataset, ?:task)	['Spam detection', 'classification']	['training', 'performance', 'learning methods', 'training set', 'network', 'measures', 'methods', 'effectiveness', 'dataset', 'machine learning', 'paper', 'models', 'work', 'relationships', 'performance measures', 'set', 'experiments', 'techniques', 'learning', 'section', 'machine', 'rest', 'network models']	The rest of this paper is organized as follows: Section 2 describes the GMDH-based learning methods used for building optimal network models that capture the input–output relationships in the training set. Section 3 defines the performance measures used for evaluating and comparing the effectiveness of the proposed methods. Section 4 describes the spambase dataset used. Section 5 describes the various experiments conducted using abductive machine learning. Section 6 compares the performance of the proposed methods with three other existing techniques. Finally, Section 7 concludes the paper and proposes directions for future work.	applied_soft_computing/S1568494609002610.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['accuracy', 'classification', 'support vector machine', 'step', 'clustering algorithm', 'methods', 'classification accuracy', 'image', 'features', 'image classification', 'vector', 'vector quantization', 'problems', 'process', 'representation', 'algorithm', 'cluster', 'support', 'bag-of-words', 'limitations', 'pascal', 'keypoints', 'representations', 'baseline', 'support vector', 'time', 'number', 'keypoint', 'iteration', 'results', 'first', 'feature', 'bow', 'selection', 'computational cost', 'experiments', 'image representations', 'points', 's', 'computational time', 'recognition', 'distances', 'images', 'cost', 'centers', 'machine', 'hand', 'framework', 'svm', 'feature representation', 'quantization', 'distance', 'datasets']	One of the most popular image representations for image classification is based on the bag-of-words (BoW) features. However, the number of keypoints that need to be detected from images to generate the BoW features is usually very large, which causes two problems. First, the computational cost during the vector quantization step is high. Second, some of the detected keypoints are not helpful for recognition. To resolve these limitations, we introduce a framework, called iterative keypoint selection (IKS), with which to select representative keypoints for accelerating the computational time to generate the BoW features, leading to more discriminative feature representation. Each iteration in IKS is comprised of two steps. In the first step some representative keypoint(s) are identified from each image. Then, the keypoints are filtered out if the distances between them and the identified representative keypoint(s) are less than a pre-defined distance. The iteration process continues until no unrepresentative keypoints can be found. Two specific approaches are proposed to perform the first step of IKS. IKS1 focuses on randomly selecting one representative keypoint and IKS2 is based on a clustering algorithm in which the representative keypoints are the closest points to their cluster centers. Experiments carried out based on the Caltech 101, Caltech 256, and PASCAL 2007 datasets demonstrate that performing keypoint selection using IKS1 and IKS2 to generate both the BoW and spatial-based BoW features allows the support vector machine (SVM) classifier to provide better classification accuracy than with the baseline features without keypoint selection. However, it is found that the computational cost of IKS1 is larger than the baseline methods. On the other hand, IKS2 is able to not only efficiently generate the BoW and spatial-based features that reduce the computational time for vector quantization over these datasets, but also provides better classification results than IKS1 over the PASCAL 2007 and Caltech 256 datasets.	information_sciences/S0020025515006040.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['candidate', 'filters', 'visual attention', 'performance', 'sliding window', 'origin', 'methods', 'saliency', 'image', 'object detection', 'eyes', 'attention', 'method', 'dataset', 'pascal', 'human visual system', 'approach', 'visual system', 'window', 'decline', 'processing', 'time', 'visual information', 'relative', 'part', 'detection', 'results', 'person', 'model', 'system', 'inria', 'area', 'role', 'objects', 'life', 'thresholds', 'detection accuracy', 'saliency map', 'map', 'mechanism', 'information', 'accuracy']	Selective visual attention plays an important role in human visual system. In real life, human visual system cannot handle all of the visual information captured by eyes on time. Selective visual attention filters the visual information and selects interesting one for further processing such as object detection. Inspired by this mechanism, we construct an object detection method which can speed up the object detection relative to the methods that search objects by using sliding window. This method firstly extracts saliency map from the origin image, and then gets the candidate detection area from the saliency map by adaptive thresholds. To detect object, we only need to search the candidate detection area with the deformable part model. Since the candidate detection area is much smaller than the whole image, we can speed up the object detection. We evaluate the detection performance of our approach on PASCAL 2008 dataset, INRIA person dataset and Caltech 101 dataset, and the results indicate that our method can speed up the detection without decline in detection accuracy.	neurocomputing/S0925231214006857.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['classification', 'evaluation', 'image', 'image classification', 'support', 'pascal', 'guidelines', 'imagenet', 'voc', 'conclusions', 'generalization', 'datasets']	Given the fact that large-scale image classification has become much more active in recent years [25–27], we consider two large-scale datasets, namely Caltech 256 [3] and ImageNet [5]. Furthermore, combined with three typical ones including 15 Scenes, Caltech 101 and PASCAL VOC 2007, the evaluation on these primary datasets can provide strong support and generalization for the conclusions and guidelines.	image_and_vision_computing/S0262885614001693.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['classification', 'vocabulary', 'application', 'conditions', 'methods', 'evaluation', 'image', 'viewpoints', 'features', 'image classification', 'image representation', 'representation', 'bag-of-words', 'pascal', 'response', 'relation', 'extensions', 'paper', 'problem', 'memory', 'coding', 'local features', 'pooling', 'model', 'bow', 'visual vocabulary', 'range', 'requirements', 'imagenet', 'efficiency', 'voc', 'conclusions', 'combinations', 'framework', 'precision', 'applications', 'datasets']	The Bag-of-Words (BoW) framework is well-known in image classification. In the framework, there are two essential steps: 1) coding, which encodes local features by a visual vocabulary, and 2) pooling, which pools over the response of all features into image representation. Many coding and pooling methods are proposed, and how to apply them better in different conditions has become a practical problem. In this paper, to better use BoW in different applications, we study the relation between many typical coding methods and two popular pooling methods. Specifically, complete combinations of coding and pooling are evaluated based on an extremely large range of vocabulary sizes (16 to 1M) on five primary and popular datasets. Three typical ones are 15 Scenes, Caltech 101 and PASCAL VOC 2007, while the other two large-scale ones are Caltech 256 and ImageNet. Based on the systematic evaluation, some interesting conclusions are drawn. Some conclusions are the extensions of previous viewpoints, while some are different but important to understand BoW model. Based on these conclusions, we provide detailed application criterions by evaluating coding and pooling based on precision, efficiency and memory requirements in different applications. We hope that this study can be helpful to evaluate different coding and pooling methods, the conclusions can be beneficial to better understand BoW, and the application criterions can be valuable to use BoW better in different applications.	image_and_vision_computing/S0262885614001693.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['configurations', 'position', 'fingers', 'classification', 'indoor', 'collection', 'classification accuracy', 'image', 'finger', 'view', 'dataset', 'rate', 'absence', 'palm', 'detection', 'care', 'detection rate', 'system', 'nonuniform', 'positions', 'background', 'well', 'examples', 'individuals', 'far', 'roi', 'camera', 'hand', 'right', 'skin detection', 'mask', 'motion', 'accuracy', 'environments']	For testing the system's classification accuracy with still images we used the well known Caltech 101 Dataset [11], taking 8994 images out of 9145 (we discarded only too large images). None of the selected images contains hands. Then we added 400 other images containing hands in various positions, different finger configurations, taken both in indoor and outdoor environments and with a nonuniform background. Each image contains just one hand. The hands belong to different individuals whom we asked to randomly open some fingers and move their hand in a random position but taking care to have the palm roughly parallel to the camera view. Both sides of the hand (and both the right and the left hand) are represented in the collection (see Figs. 5 and 11 for a few examples). Table 2 shows the detection rate and FAR achieved. In this still image experiment the ROI mask is realized using only skin detection (due to the absence of motion).	image_and_vision_computing/S026288561100120X.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['cortex', 'classification', 'performance', 'similarity', 'programming', 'angles', 'category', 'image', 'correlation', 'reference', 'advance', 'vector', 'attention', 'method', 'real-world', 'input', 'dataset', 'paper', 'computer vision', 'task', 'pattern', 'problem', 'results', 'pixels', 'model', 'pixel', 'patterns', 'orientation', 'set', 'visual cortex', 'techniques', 'procedure', 'experimental results', 'computer', 'object', 'systems', 'pixel-wise', 'recognition', 'dynamic programming', 'images', 'segmentation', 'direction', 'areas', 'test', 'vector field', 'vision', 'field', 'quantization']	Categorical classification for real-world images is a typical problem in the field of computer vision. This task is extremely easy for a human due to our visual cortex systems. However, developing a similarity recognition model for computer is still a difficult issue. Although numerous approaches have been proposed for solving the tough issue, little attention is given to the pixel-wise techniques for recognition and classification. In this paper, we present an innovative method for recognizing real-world images based on pixel matching between images. A method called two-dimensional continuous dynamic programming (2DCDP) is adopted to optimally capture the corresponding pixels within nonlinearly matched areas in an input image and a reference image representing an object without advance segmentation procedure. Direction pattern (a set of scalar patterns based on quantization of vector angles) is made by using a vector field constructed by the matching pixels between a reference image and an input image. Finally, the category of the test image is deemed to be that which has the strongest correlation with the orientation patterns of the input image and its reference image. Experimental results show that the proposed method achieves a competitive and robust performance on the Caltech 101 image dataset.	journal_of_computer_and_system_sciences/S0022000012001043.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'authors', 'rates', 'criterion', 'dataset', 'pascal', 'approach', 'al', 'rate', 'results', 'detection', 'result', 'measure', 'detection rate', 'correctness', 'background', 'experiments', 'set', 'sketches', 'horse', 'papers', 'criteria']	Some results on the Weizmann horse dataset are shown in Fig. 19. Table 6 shows the detection rates using the PASCAL criterion of 0.4 FPPI averaged over the five sketches, where the entire dataset was used for experiments. In this table, the results of previous papers are taken from Yang and Latecki [26], where the authors use a strict PASCAL 50%-IOU criteria as the correctness measure and the background set is taken from Caltech 101 [60]. As shown, we achieved a detection rate of 95.2% at 0.4 FPPI, which is as good as the best result achieved by the learning-based approach of Shotton et al. [23]. However, this detection rate was in fact first achieved at a lower FPPI of 0.33.	computer_vision_and_image_understanding/S1077314215001332.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'classification', 'photograph', 'category', 'jordan', 'classification problem', 'image', 'multiclass', 'attention', 'dataset', 'combination', 'categories', 'spatial information', 'lot', 'task', 'noble', 'models', 'color', 'problem', 'liu', 'sift', 'vectors', 'results', 'kernels', 'data', 'background', 'challenge', 'set', 'experiments', 'dance', 'kernel', 'images', 'management', 'fan', 'visual words', 'words', 'applications', 'quantization', 'information']	In another set of experiments, we have tackled the task of image semantic classification. This problem has proven to be a difficult challenge, has received a lot of attention over the lat years and has several applications such as digital photograph management (Liu, Zhang, Li, Zhang, & Wang, 2005). We have considered a multiclass image semantic classification problem using the Caltech 101 data set (Fei-Fei et al., 2004). Our classification has been based on a combination of our generated kernels used to model the color spatial information and a linear kernel (Bach, Lanckriet, & Jordan, 2004; Lewis, Jebara, & Noble, 2006) used to discriminate between 500-dimensional vectors of visual words, representing the different images, generated by SIFT vectors quantization (Csurka, Dance, Fan, Willamowski, & Bray, 2004). The Caltech 101 dataset contains 9197 images comprising 101 different object categories and a background category. We ran our models using 30 images per category. The classification results using different generated kernels are shown in Fig. 5. The obtained results are good as compared to previous state-of-the-art results (Mutch & Lowe, 2006) and show again that GDM-based kernels perform better than DM- and GM-based kernels.	information_processing_&_management/S030645731200074X.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'image', 'subset', 'clutter', 'dataset', 'categories', 'centre', 'class', 'butterfly', 'system', 'background', 'experiments', 'database', 'helicopter', 'objects', 'images', 'rotations', 'brain', 'details']	We use a subset of the Caltech 101 database [30]. This dataset contains 101 object categories with 40–800 images per class. Most of the images in the database have little or no clutter. Furthermore, the objects tend to lie in the centre of the images and appear in similar poses. Some images have a partially black background due to artificial image rotations. For our experiments we have randomly chosen the following 10 categories: ewer, sunflower, kangaroo, starfish, trilobite, menorah, helicopter, butterfly, brain and grand piano. In the experiments we randomly select 50% of the images for training the system. Some of the images are shown in Fig. 8. More details are summarised in Table 1.	computer_vision_and_image_understanding/S1077314210002407.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'neighbor', 'culling', 'values', 'error', 'nearest neighbor', 'dft', 'methods', 'correlation', 'image', 'case', 'traversal', 'benchmark', 'number', 'model', 'correlations', 'benchmarks', 'cost model', 'approximate nearest neighbor search', 'nearest neighbor search', 'cost', 'nodes', 'ratios', 'search', 'queries', 'distance']	We check the correlation of our cost model with approximate nearest neighbor search queries. In this case, we measure the correlation between values of our cost model and distance error ratios achieved when we limit the number of traversed nodes to 1K nodes. Fig. 4 shows the correlations of our cost model against the distance error ratios achieved by different traversal methods for the Caltech 101 and UKBench image benchmarks. Our cost model shows high correlations, 0.89, 0.89, and 0.85, over DFT w/o culling, DFT w/ culling, and BBFS respectively in the Caltech 101 benchmark. We observe that our cost model shows similar, high correlations, 0.73, 0.73, and 0.86, against DFT w/o culling, DFT w/ culling, and BBFS respectively in the UKBench image benchmark.	computer_vision_and_image_understanding/S1077314212000768.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'performance', 'conditions', 'evaluation', 'algorithm', 'observation', 'landmark', 'results', 'data', 'illumination', 'positions', 'set', 'database', 'landmarks', 'qualitative evaluation', 'images', 'annotation', 'face', 'help', 'face database']	For the second data set, we aim to automatically annotate the 33 landmarks on 255 images from Caltech 101 face database [34] with the help of 50 manually annotated images from the ND1 database. Since we do not have ground-truth landmark positions for this database, we can only perform qualitative evaluation by visual observation. Fig. 12b illustrates sample annotation results. Although the congealing on the Caltech 101 images is much more difficult than that of the first data set (ND1 images) due to cluttered backgrounds and challenging illumination conditions, our algorithm can still achieve satisfactory annotation performance.	computer_vision_and_image_understanding/S1077314212000616.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'pooling', 'size', 'relation', 'precision', 'methods', 'value', 'coding', 'datasets']	In this subsection, the relation between average pooling and coding is studied. Fig. 2 shows the precision of average pooling on the five datasets. It can be observed that most coding methods are comparable. With the increasing vocabulary size, the precision gradually achieves the highest, e.g., 8192 on 15 Scenes and 16,384 on Caltech 101, while it begins to decrease after the highest value. Besides, it can be observed in Fig. 2(f) and (g) that SV and IFK perform better than other coding methods, and IFK is superior to SV.	image_and_vision_computing/S0262885614001693.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['fig', 'values', 'space', 'query', 'kd-tree', 'image features', 'image', 'features', 'image statistics', 'natural images', 'image database', 'benchmark', 'natural image statistics', 'sift', 'dimensions', 'statistics', 'regions', 'sets', 'database', 'distributions', 'benchmarks', 'plotting', 'gradients', 'distribution', 'images', 'datasets']	Fig. 3 shows the distributions of SIFT image features in our tested image benchmark datasets, Caltech 101, UKBench, and Oxford [6]. We simply project values of all the dimensions of these image features into one dimensional space to draw the distributions. We draw distributions of these values of image features drawn from different image sets of the same benchmark and drawn from different images of two different benchmarks. Even though they are computed from different images across different image benchmarks, these distributions have a nearly similar tendency. This is because inside natural images, most regions are smooth regions or regions with small gradients [28] and our plotting on the image features agrees with such natural image statistics. Hence, we can expect the distribution of potential query images follows the same distribution as the distribution of image features that were used to construct the kd-tree from an image database.	computer_vision_and_image_understanding/S1077314212000768.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['level', 'mapping', 'step', 'capacity', 'association', 'quality', 'methods', 'image', 'histogram', 'knowledge', 'case', 'ham', 'host', 'process', 'degradation', 'method', 'carrier', 'input', 'dataset', 'paper', 'approach', 'image quality', 'classes', 'scalability', 'class', 'data', 'pixel', 'block', 'experiments', 'performances', 'average', 'scenario', 'images', 'terms', 'test', 'perceptual quality', 'information', 'blocks']	This paper proposes a novel reversible information hiding method aiming to achieve scalable carrier capacity while progressively distorting the image quality. Unlike the conventional methods, the proposed method HAM (Histogram Association Mapping) purposely degrades the perceptual quality of the input image through data embedding. To the best of our knowledge, there is no method that attempts to significantly increase the carrier capacity while introducing (tolerating) intentional perceptual degradation for avoiding unauthorized viewing. HAM eliminates the expensive pre-processing step(s) required by the conventional histogram shifting data embedding approach and improves its carrier capacity. In particular, the host image is divided into non-overlapping blocks and each block is classified into two classes. Each class undergoes different HAM process to embed the external data while distorting quality of the image to the desired level. Experiments were conducted to measure the performances of the proposed method by using standard test images and CalTech 101 dataset. In the best case scenario, an average of ~2.88 bits per pixel is achieved as the effective carrier capacity for the CalTech 101 dataset. The proposed method is also compared with the conventional methods in terms of carrier capacity and scalability in perceptual quality degradation.	signal_processing:_image_communication/S0923596513001501.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['neighbor', 'retrieval', 'query', 'nearest neighbor', 'dft', 'image features', 'image', 'features', 'observation', 'benchmark', 'number', 'k', 'image retrieval', 'construction', 'node', 'kd-trees', 'correlations', 'tree construction', 'section', 'images', 'terms', 'nodes', 'queries', 'details']	To verify this observation, we measure correlations between the DFT with and without culling in terms of the number of traversed nodes that have been performed to find the nearest neighbor node given a query. To measure correlations, we construct 500 different kd-trees with image features from the Caltech 101 image benchmark that consists of around 10 K images; The details about how these 500 different kd-trees are constructed will be given in Section 6. We perform 10K different queries used for SIFT-based image retrieval for each kd-tree. These queries are chosen from images that are in the same benchmark, but are not used for the tree construction.	computer_vision_and_image_understanding/S1077314212000768.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['neighbor', 'retrieval', 'values', 'normals', 'nearest neighbor', 'image', 'value', 'features', 'benchmark', 'sift', 'partitioning', 'number', 'image retrieval', 'construction', 'model', 'root', 'node', 'kd-trees', 'tree construction', 'cost model', 'images', 'cost', 'nodes', 'tree']	We randomly choose 500 images from the Caltech 101 image benchmark and extract around 100K SIFT features from those images for constructing the kd-trees. 500 different kd-trees are constructed by randomly choosing the partitioning values given the partitioning normals. For each kd-tree, we evaluate our cost model with the tree by measuring the cost value associated with the root node of the kd-tree. We have also measured the number of traversed nodes to find the nearest neighbor features given 10K query SIFT features. These 10K SIFT features are extracted from different 500 query images for our SIFT-based image retrieval. These query images are randomly chosen from the same image benchmark, but are not used for the tree construction.	computer_vision_and_image_understanding/S1077314212000768.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['noise', 'training', 'performance', 'ground truth', 'feret', 'methods', 'evaluation', 'image', 'truth', 'robustness', 'challenges', 'algorithm', 'initialization', 'method', 'aim', 'parameter', 'quantitative evaluation', 'aspects', 'population', 'ground', 'data', 'comparison', 'dimensionality', 'training data', 'sets', 'background', 'automatic annotation', 'experiments', 'set', 'database', 'high dimensionality', 'landmarks', 'images', 'experiment', 'annotation', 'test', 'deal', 'face', 'face database', 'subjects']	For the experiments, we employed three test data sets: the first data set contains 400 images from the Notre Dame (ND1) database [33] with more than 100 subjects; the second data set consists of 255 images from Caltech 101 face database [34] and 50 images from the ND1 database; and the third data set contains 1176 images from FERET database [35] and ND1 database. We manually annotated 33 landmarks for each image in the first and the third data sets to establish a ground truth and to enable a quantitative evaluation for the automatic annotation performance. The experiments on the first data set aim to quantitatively evaluate the proposed annotation algorithm on various aspects including the robustness to the noise in initialization and the parameter setting, and to enable a quantitative comparison with other aforementioned methods [10,8]. The experiments on the other two data sets intend to demonstrate that the proposed annotation algorithm can generalize to a large population and deal with real-world challenges such as cluttered background and various illuminations (the second data set), and high dimensionality of the image ensemble (the third data set). Furthermore, the experiment on the third data set also provides a quantitative comparison with the supervised method [9] with same amount of training data.	computer_vision_and_image_understanding/S1077314212000616.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['quality', 'correlation', 'image', 'benchmark', 'time', 'uk', 'model', 'kd-trees', 'correlations', 'cost model', 'cost', 'search', 'queries']	To further analyze the correlation of our cost model against the quality of kd-tree, we measure the time spent to traversing kd-trees. Our cost model shows a high correlations, 0.74, with the average time spent on performing exact search queries that run by using the BBFS in the UK Bench image benchmark and the Caltech 101 image benchmark.	computer_vision_and_image_understanding/S1077314212000768.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['retrieval', 'vertex', 'hyperedge', 'similarity', 'label', 'strategy', 'ranking', 'methods', 'image', 'correlation', 'inference', 'sampling strategy', 'feature descriptors', 'labels', 'vi', 'effectiveness', 'image search', 'hypergraph', 'method', 'dataset', 'incidence', 'descriptors', 'paper', 'approach', 'task', 'order', 'problem', 'retrieval system', 'scene', 'structure', 'image retrieval', 'similarity matrix', 'feature', 'vertices', 'system', 'higher order', 'experiments', 'constraints', 'k-nearest neighbors', 'learning', 'feedback', 'images', 'share', 'affinity', 'relationship', 'cost', 'matrix', 'framework', 'centroid', 'transductive learning', 'search', 'computation', 'information']	In this paper, we propose a new transductive learning framework for image retrieval, in which images are taken as vertices in a weighted hypergraph and the task of image search is formulated as the problem of hypergraph ranking. Based on the similarity matrix computed from various feature descriptors, we take each image as a ‘centroid’ vertex and form a hyperedge by a centroid and its k-nearest neighbors. To further exploit the correlation information among images, we propose a soft hypergraph, which assigns each vertex vi to a hyperedge ej in a soft way. In the incidence structure of a soft hypergraph, we describe both the higher order grouping information and the affinity relationship between vertices within each hyperedge. After feedback images are provided, our retrieval system ranks image labels by a transductive inference approach, which tends to assign the same label to vertices that share many incidental hyperedges, with the constraints that predicted labels of feedback images should be similar to their initial labels. We further reduce the computation cost with the sampling strategy. We compare the proposed method to several other methods and its effectiveness is demonstrated by extensive experiments on Corel5K, the Scene dataset and Caltech 101.	pattern_recognition/S0031320310003535.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['training', 'category', 'foreground', 'labels', 'dataset', 'part', 'object recognition', 'background', 'respect', 'recognition', 'images']	The second dataset is part of the Caltech 101 dataset [25] commonly used for object recognition. We use 10 training images and 11 testing images from the ‘Faces’ object category. We work with 2 semantic labels, the ‘face’ (foreground) and the ‘non-face’ (background). The ‘face’ exhibits a fairly low intravariance with respect to the highly variant background.	computer_vision_and_image_understanding/S1077314215002039.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['training', 'error', 'training set', 'validation', 'mse', 'resolution', 'optimization', 'binary images', 'optimization model', 'bernoulli', 'dataset', 'classes', 'rbms', 'model', 'sets', 'silhouettes', 'set', 'images', 'test', 'test sets']	CalTech 101 Silhouettes dataset1818https://people.cs.umass.edu/∼marlin/data.shtml.: it is based on the former Caltech 101 dataset, and it comprises silhouettes of images from 101 classes with resolution of 28×28. Since we have gray-scale images available in a training, validation and test sets, we converted them in to binary images before using Bernoulli RBMs. Additionally, we used only the training and test sets, since our optimization model aims at minimizing the MSE error over the training set.	applied_soft_computing/S1568494615005517.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['training', 'fig', 'performance', 'implementation', 'category', 'samples', 'correct', 'viewpoints', 'off-line', 'view', 'multiple view', 'algorithm', 'instances', 'dataset', 'match', 'approach', 'aspects', 'levels', 'baseline', 'sift', 'bag-of-features', 'user', 'pyramid', 'data', 'comparison', 'system', 'stock', 'multiple', 'experiments', 'kernel', 'complexity', 'object', 'images', 'multiple images', 'computational complexity', 'datasets']	The performance of our system, tested off-line, is qualitatively comparable with algorithms performing at baseline levels on standard datasets such as the Caltech 101 [44]. However, direct comparison is not straightforward because we do not use multiple (supervised) hand-labeled training samples for each category, but instead use multiple images of the same object, relying on the user to sample multiple aspects (viewpoints). Instead, we tested a baseline algorithm on the dataset [43] with multiple view images of object instances; we took a stock implementation of SIFT bag-of-features with spatial pyramid match kernel (SIFT-SPMK) from the VLFeat library [44] that performs at 65% on the Caltech 101 dataset, and tested it on Moreels data where it scored 92.8% correct. In Fig. 7, our TST-BTD approach performs better (TST-BTD 96.0%, TST-SIFT 91.4%), even with far less computational complexity. The experiments are explained below.	image_and_vision_computing/S0262885611000709.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['training', 'vocabulary', 'performance', 'step', 'clustering algorithm', 'stage', 'cross-validation', 'cut-off', 'correlation', 'codebooks', 'recognition rate', 'labels', 'subset', 'algorithm', 'criterion', 'method', 'cluster', 'refinement', 'nearest', 'dataset', 'combination', 'stages', 'aim', 'adaptive refinement', 'approach', 'cpm', 'rate', 'neighbours', 'time', 'vocabularies', 'scheme', 'adaptive threshold', 'compactness', 'object recognition', 'results', 'class', 'account', 'visual vocabulary', 'representativeness', 'experiments', 'nearest neighbours', 'learning', 'meta-clustering', 'recognition', 'images', 'threshold', 'precision', 'impact', 'visual words', 'words', 'rnn', 'next']	We present a novel method for constructing a visual vocabulary that takes into account the class labels of images, thus resulting in better recognition performance and more efficient learning. Our method consists of two stages: Cluster Precision Maximisation (CPM) and Adaptive Refinement. In the first stage, a Reciprocal Nearest Neighbours (RNN) clustering algorithm is guided towards class representative visual words by maximising a new cluster precision criterion. As we are able to optimise the vocabulary without the need for expensive cross-validation, the overall training time is significantly reduced without a negative impact on the results. Next, an adaptive threshold refinement scheme is proposed with the aim of increasing vocabulary compactness while at the same time improving the recognition rate and further increasing the representativeness of the visual words for category-level object recognition. This is a correlation clustering based approach, which works as a meta-clustering and optimises the cut-off threshold for each cluster separately. In the experiments we analyse the recognition rate of different vocabularies for a subset of the Caltech 101 dataset, showing how RNN in combination with CPM selects the optimal codebooks, and how the clustering refinement step succeeds in further increasing the recognition rate.	computer_vision_and_image_understanding/S1077314210002407.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['values', 'dbn', 'features', 'displays', 'algorithm', 'dataset', 'learning algorithm', 'shapes', 'classes', 'results', 'reconstruction', 'techniques', 'learning', 'section', 'mnist', 'errors', 'layers']	In this section, we present the reconstruction results considering Caltech 101 dataset, which is more challenging than MNIST dataset, since it has more classes and complex shapes. Once again, IHS obtained the best results, but now with a DBN using three layers and FPCD as the learning algorithm. As aforementioned, it is expected a better resulting using a deeper DBN, since this dataset present more complex shapes, thus requiring better learned features. Table 3 displays such results, being the bolded values the similar techniques with the lowest errors.	applied_soft_computing/S1568494615005517.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['visual surveillance', 'sparse coding', 'fig', 'classification', 'performance', 'competitions', 'methods', 'image', 'image classification', 'features', 'classifier', 'representation', 'image representation', 'computer vision', 'bag-of-words', 'pascal', 'response', 'surveillance', 'problem', 'coding', 'part', 'image analysis', 'feature', 'model', 'strategies', 'bow', 'role', 'imagenet', 'computer', 'distribution', 'voc', 'local features', 'vision', 'analysis', 'applications', 'datasets']	Image classification is a fundamental problem in computer vision. It plays a key role in many applications such as image analysis and visual surveillance. In recent years, the Bag-of-Words (BoW) model has been widely used on many popular datasets and competitions, e.g., 15 Scenes [1], Caltech 101 [2], Caltech 256 [3], PASCAL VOC [4] and ImageNet [5]. In BoW, local features are first extracted to construct image representation, which is then fed into a classifier, as shown in Fig. 1. Specifically, the representation is an essential part, which includes two steps:Coding:Coding means that local features are encoded by a vocabulary and the response of the feature on the vocabulary is generated. The probabilistic strategies [6–9] describe the distribution of local features, while sparse coding methods [10–15] better reconstruct the features. Recently, superior performance has been obtained by some high-dimensional coding methods [16–19].	image_and_vision_computing/S0262885614001693.txt
tasks_annotated_for (Caltech 101: dataset, ?:task)	['Classification', 'object recognition']	['voc', 'pascal', 'paper', 'imagenet', 'datasets']	Five popular datasets are used in this paper. Three of them are the typical small-scale datasets, including 15 Scenes [1], Caltech 101 [2] and PASCAL VOC 2007 [44]; while another two are the large-scales ones, which include Caltech 256 [3] and ImageNet [5].	image_and_vision_computing/S0262885614001693.txt