{
  "instances": [
    {
      "support": [
        {
          "text": {
            "text": "In the present paper we have proposed a novel unsupervised algorithm called EE-MC for the computational prediction of protein complexes on PPI graphs. EE-MC is theoretically able to overcome intrinsic limitations of existing methodologies such as their inability to handle weighted PPI networks, their constraint to assign every protein in exactly one cluster and the difficulties they face concerning the parameter tuning. To validate this theoretical superiority of EE-MC we have deployed it to predict PPIs in an experimentally verified PPI graph of yeast organism and an adequately studied part of the experimentally verified PPI network of Human organism. In the latter, weights were added to its edges using a computational technique which uses functional and structural information. The results of EE-MC were compared with corresponding results of the well established methodologies of MCL and RNSC when using various benchmark protein complex sets. As indicated in the experimental results, EE-MC clearly outperformed in the separation metric existing methodologies in the yeast dataset and has marginally improved the performance both geometric accuracy and separation metrics of MCL and RNSC in the human dataset.",
            "id": "artificial_intelligence_in_medicine/S093336571400150X.txt"
          }
        },
        {
          "text": {
            "text": "Yeast dataset contains the expression levels of 2879 yeast genes under 17 cell cycle conditions, covering approximately two full cell cycles. Data were selected and preprocessed according to [10].",
            "id": "computers_in_biology_and_medicine/S0010482507000182.txt"
          }
        },
        {
          "text": {
            "text": "In a similar study conducted by the authors in Ref. [8], the ECOC SVMs were better in the classification of E. coli and yeast protein localizations. The proteins, provided by Ref. [6], were represented by signals and amino acid information. The error rates achieved were relatively high in the yeast dataset, although they were similar to the ones presented in Ref. [6]. This may be due to the fact that the datasets employed were old, presenting few protein data. The inclusion of new proteins and also alternative features could improve the results obtained.",
            "id": "computers_in_biology_and_medicine/S0010482506000175.txt"
          }
        },
        {
          "text": {
            "text": "The object assignment extension discovered some interesting non-disjoint clusters from the yeast dataset, but in general some important clusters could be missed if their structures are not captured by some disjoint clusters before object assignment. We propose two future extensions of HARP for identifying these clusters: to allow each cluster to be merged with multiple clusters, and to produce disjoint clusters on different small data samples, and then reassign other objects to the clusters. Both approaches allow the discovery of more projected structures.",
            "id": "journal_of_biomedical_informatics/S1532046404000541.txt"
          }
        },
        {
          "text": {
            "text": "The first dataset is a PPI dataset from the well-studied yeast organism (Saccharomyces cerevisiae). It was published by Friedel et al. [24] and containsinformation about 5195 proteins and 62,876 interactions. All these interactions areassigned with a confidence score whose values range from 0 to 1. These confidence scores were calculated by combining the purification experimental results and reflect the possibility of each protein pair to be a protein interaction and the frequency of each protein pair to occur as a protein interaction. The majority of yeast PPIs have been discovered with experimental techniques, and for this reason the yeast dataset is very reliable for the comparison of protein complex prediction techniques. Moreover, a plethora of high confidence protein complex datasets have been published for the yeast organism and some of them are presented in Section 2.3.",
            "id": "artificial_intelligence_in_medicine/S093336571400150X.txt"
          }
        },
        {
          "text": {
            "text": "Table 1 lists the most enriched GO categories in each transcript module uncovered from the yeast dataset. In this table, false discovery rate (FDR) corrected P-value [15], which is based on hypergeometric distribution, was used to show the enrichment of the clusters. In addition, we used the list of all genes in the dataset as a reference array. One thing that should be noted is that, among the 6152 genes, only 6114 genes were annotated by GO and the Kyoto encyclopedia of genes and genomes (KEGG) database [26]. Table 2 lists a few functional categories in each significantly enriched modules (corrected P-value <10\u221210). Fig. 2 shows the enriched combinations of significant annotations of biological process (BP) of module C18, using a pie graph and a bar graph, respectively. Fig. 3 shows the heat-map of module C18, which is significantly enriched by \u201cRibosome\u201d (corrected P-value <6.0315\u00d710\u2212149). From Fig. 3 it can be seen that the expression patterns of the genes in this module are very similar. On the contrary, the heat-map of Module C2, which is significantly enriched by the \u201cribosome biogenesis\u201d (corrected P-value <3.19181\u00d710\u221237), reveals at least two distinctive expression patterns (denoted by A and B, Fig. 4). In summary, our method can not only cluster the genes, which show similar expression profiles with similar functions, but also cluster the genes, which show different expression profiles but similar functions. Moreover, during the experiment we also find that many genes are clustered in several clusters.",
            "id": "computers_in_biology_and_medicine/S0010482511001880.txt"
          }
        },
        {
          "text": {
            "text": "The meaning of R(Ci) is the ratio of elements in category Ci that belong to the area of overlap with any of the other categories. Most classification experiments show that the accuracy of classification for each category differs from that of others. We may expect that if R(Ci)>R(Cj), then the accuracy of classification for Ci is less than one of Cj. We selected the Yeast dataset [9], and calculated the classification accuracy of each category using the three classifiers. The Yeast dataset has nine categories and 1484 instances. Fig. 11 shows the results, and Fig. 12 results from correlation analysis between the classification accuracy of classifiers and R(Ci).",
            "id": "computers_in_biology_and_medicine/S0010482510001757.txt"
          }
        },
        {
          "text": {
            "text": "As mentioned earlier, it is highly important that estimation methods should be able to demonstrate lower estimation errors across the range of datasets. For instance, BPCA method had lower imputation error for ovarian cancer (Fig. 17) but showed largest disparity for yeast dataset (Fig. 19). Similarly, LLSImpute exhibited lower NRMS error for cancer datasets but could not maintain the same performance for both yeast datasets (Figs. 19 and 20) and the error rate was comparable to KNN. The CMVE (Fig. 17) exhibited lower NRMS error for both types of cancer data; but it did not maintain this performance when determining significant genes in the ovarian cancer data (Fig. 3) due to global correlation structure possessed by the dataset. In contrast, AMVI adapted to the correlation structure of the data and showed improved performance for all types of datasets, so endorsing the strategy to compute the optimal number of predictor genes based on the correlation structure of the data, rather than using a fixed value.",
            "id": "journal_of_biomedical_informatics/S1532046407001487.txt"
          }
        },
        {
          "text": {
            "text": "The overall relationship between proteins network distances and their functional similarities were assessed for different GO function categories and the result is shown in Fig. 2. In interaction network of S. cerevisiae, functional similarity decreases strongly by increasing distance between proteins. However, decreasing ratios vary among different function types. For example, decreasing rate of function similarity for Molecular-function is less than other types of functions in yeast dataset.",
            "id": "computational_biology_and_chemistry/S1476927112000941.txt"
          }
        },
        {
          "text": {
            "text": "The results show that NRC significantly improves the prediction results compared with all the other methods in the prediction of Cellular-component in both networks (Figs. 3A and 4-A). In predicting Molecular-function, NRC overcomes other methods in the Human network (Fig. 4B). But, there is no significant difference between the performance of NRC and FS-weight #1&2 method for the yeast dataset (Fig. 3B).",
            "id": "computational_biology_and_chemistry/S1476927112000941.txt"
          }
        },
        {
          "text": {
            "text": "We thank Dr. Haixu Tang and Dr. Yong Fuga Li for their help on peptide detectability. We thank Dr. Smriti R. Ramakrishnan for providing us the protein reference set of yeast dataset used in the experiments. This work was partially supported by the Natural Science Foundation of China under Grant no. 61003176.",
            "id": "computational_biology_and_chemistry/S1476927113000029.txt"
          }
        },
        {
          "text": {
            "text": "Based on 100 experiment replications, the clustering performances of the three clustering algorithms using the three datasets are shown in Tables\u00a05\u20137, respectively. Note that only the features with the top five highest weights are displayed in the last rows of Tables\u00a06 and 7. Based on these tables, we found that the proposed FWAS-K-means is slightly inferior to the w-K-means algorithm using the iris dataset in terms of the Entropy measure, and in the yeast dataset according to the Adjusted Rand index measure. Nevertheless, the performances of the proposed FWAS-K-means are better than the other two algorithms in general, especially when considering the SSE measure. In addition, when comparing to the w-K-means algorithm, the FWAS-K-means algorithm enhances representative features by assigning them more weight, which causes the FWAS-K-means algorithm to generate a better clustering result.",
            "id": "computational_statistics_&_data_analysis/S0167947308001552.txt"
          }
        },
        {
          "text": {
            "text": "In this section, three well-known datasets in pattern recognition literature (i.e.\u00a0iris, wine, and yeast) are used as benchmark datasets. The three datasets were retrieved from the UCI machine learning repository (Newman et\u00a0al., 1998) and their properties are listed in Table\u00a04. The iris dataset contains 3 classes and each class refers to an iris plant type. One class is linearly separable from the other two classes, and the other two are hard to linearly distinguish. The wine dataset is the chemical analysis result of three different wine cultivars grown in Italy. Thirteen constituents are used to identify wine types. The yeast dataset is used to predict 10 yeast protein cellular localization sites. This dataset is considered to be a difficult prediction problem, since most data belong to three classes. The experimental setup used in this section is the same as the one described in Section\u00a04.2.",
            "id": "computational_statistics_&_data_analysis/S0167947308001552.txt"
          }
        },
        {
          "text": {
            "text": "Standard deviation is reported next to each result. In order to test the statistical significance of the results, we applied Student's T-test of difference of means with confidence level of 1%. In the tables, a minus (plus) symbol next to a result indicates that the average is statistically significantly lower (higher) than the average obtained by SMOB\u2013VEon a given dataset. So, for instance, the average MSRobtained by SMOB-\u03b4on the yeast dataset is significantly lower than the average MSRobtained by SMOB\u2013VE.",
            "id": "computers_in_biology_and_medicine/S0010482511002344.txt"
          }
        },
        {
          "text": {
            "text": "Clustering of genes based on expression behavior is a powerful way to uncover unknown functions of genes. By assigning genes with unknown functions to a group of genes whose functions have already been identified, the functions of the unannotated genes can then be inferred based on the similarity of their expression profiles. While the majority of the genes assigned in our clustering results are consistent with the gene annotations, it is important to study the genes that have new assigned functions that were not previously known. Tables 9 and 10 show the number of genes with newly proposed functions for datasets A and B, respectively. These genes have been further investigated. One interesting finding is that the gene YER036C is clustered in the group GO:42254 in our clustering results for both datasets A and B. This suggests that YER036C is involved in ribosome biogenesis and assembly. The GO annotation from SGD which was used to generate the clusters was created in September 2005. From this version of the annotation, YER036C was assigned to biological process unknown (GO:0000004). According to SGD [30], YER036C was assigned a new name ARB1 (ATP-binding cassette protein involved in Ribosome Biogenesis), and was assigned to be involved in ribosome biogenesis on January 5, 2006 based on the published article [37]. The correct assignment of the gene function of ARB1 can be explained by the similarity of the expression values of ARB1/YER036C with the values of the genes that are annotated to be involved in ribosome biogenesis. Similarly, the correctness of the gene function assignment based on the latest GO annotation dated in July 2006 is confirmed for 6 other genes. The genes ALB1/YJL122W [38] and RSA4/YCR072C [39] are assigned to the cluster that corresponds to ribosome biogenesis using datasets A and B, in which the genes are associated with ribosomal large subunit biogenesis (GO:0042273) and ribosomal large subunit assembly and maintenance (GO:0000027), respectively, according to the latest GO annotation. While not as specific, SAE3/YHR079C-A8Even though this gene function was assigned in the Sep 2005 annotation which we used, GO Fuzzy c-means treated this gene as a gene with no annotation due to its name in the yeast dataset. The name of this gene in the annotation files is SAE3/YHR079C-A, while it is called YHR079BC in dataset A.8 [40,41] is assigned to the cluster that corresponds to meiosis (GO:0007126) using dataset A, and TMA19/YKL056C [42], ASC1/YMR116C [43] and ZUO1/YGR285C [44] are assigned to protein biosynthesis (GO:0006412) using dataset B. These genes are summarized in Table 11.",
            "id": "journal_of_biomedical_informatics/S1532046408000798.txt"
          }
        },
        {
          "text": {
            "text": "At the likelihood-ratio-cutoff 600, the KD-BN approach correctly predicted the protein pair YKR085C and YOR150W in the yeast dataset as interacting. In contrast, this protein pair was incorrectly predicted as non-interacting when the NB approach was implemented. Additionally, the NB approach incorrectly predicted 10 protein pairs as interacting (FP), in comparison the KD-BN approach predicted 5 FP.",
            "id": "computers_in_biology_and_medicine/S001048251000003X.txt"
          }
        },
        {
          "text": {
            "text": "RIB probabilistic model based on frequent itemsets is robust to bound data perturbations. Hence, for roughly half of the considered datasets, tuned RIB exploits the same support threshold for most of the tested levels and kinds of noise. In case of complex data distributions (e.g., unbalanced class distributions), tuning the support threshold allows RIB to further adapt to different noise levels (e.g., for the Yeast dataset with class noise injection at p=10%, 52.7% at min_sup=1%, 53.4% at min_sup=0.2%).",
            "id": "knowledge-based_systems/S0950705114003037.txt"
          }
        },
        {
          "text": {
            "text": "The yeast dataset was preprocessed in order to match the procedures documented in the literature. Horton and Nakai [20\u201322] point out that several yeast samples have identical feature vectors. Although unnecessary, Horton and Nakai conducted experiments with the duplicates removed from the dataset. Horton and Nakai reported that the dataset contained 1462 unique samples among the original 1484 samples. We found only 1453 unique samples. The discrepancy of nine samples represents approximately 0.5% of the entire set and does not significantly impact the performance comparison presented in a later section.",
            "id": "applied_soft_computing/S1568494602000601.txt"
          }
        },
        {
          "text": {
            "text": "In this section, we apply the proposed method to discover the transcriptional modules in the form of functional links among genes. Two published gene expression datasets are used to analyze the performance of our method: the yeast dataset [21] and normal human tissue dataset [22].",
            "id": "computers_in_biology_and_medicine/S0010482511001880.txt"
          }
        },
        {
          "text": {
            "text": "Parameter \u03b1 is a harmonic coefficient in integrating dynamic local average connectivity and complex centrality. Table 4 shows the effect of parameter \u03b1 on the performance of CDLC algorithms, where k is the number of top ranked proteins. CDLC can predict the highest number of true essential proteins with \u03b1=0.1. This observation suggests that the complex component is an excellent property for predicting essential proteins while the LAC component only has a little effect. This finding can be attributed to the fact that essentiality is a product of protein complexes rather than the individual protein, and protein complexes have a high collection of essential proteins (Hart et al., 2007). Nevertheless, comparing the result of CDLC with \u03b1=0 and that with \u03b1=0.1, we find that a better result can be achieved by the latter value of \u03b1. In addition, the protein complexes of some species are less completed than yeast. Therefore, deleting the LAC component may significantly reduce prediction precision. Thus, the LAC component is indispensable although it only has a little effect on the results for yeast dataset.",
            "id": "computational_biology_and_chemistry/S1476927114001017.txt"
          }
        },
        {
          "text": {
            "text": "The yeast dataset contains 173 samples collected under several different conditions, which include temperature shocks, hyper- and hypoosmotic shocks, exposure to various agents such as peroxide, menadione, diamide, dithiothreitol, amino acid starvation, nitrogen source depletion and progression into stationary phase, etc. This dataset contains 6152 genes in each sample. For this dataset, p=6152, n=173, and we used KNNimpute [23] to fill in missing values.",
            "id": "computers_in_biology_and_medicine/S0010482511001880.txt"
          }
        },
        {
          "text": {
            "text": "One of the yeast datasets, called GaschYeast, and the Homo sapiens dataset, called Alzheimer, have been downloaded from the supplementary information in papers [21,53], respectively. The GaschYeast dataset is composed of 2993 genes and 173 samples and the Alzheimer dataset comprises 1663 genes and 33 samples. The other yeast dataset has been obtained from the Gene Expression Omnibus (GEO)11http://www.ncbi.nlm.nih.gov/geo/. repository. Specifically, the dataset record GDS1116 reported in a previous study [54] has been used in this work. This dataset is not a time series gene expression data set, and the dimensionality is 7084 genes and 131 samples. The missing values have been preprocessed using the GEPAS22http://www.gepas.org/. tool. This tool basically removes the genes with a large number of missing values and replaces missing values using the mean or median of the row or column values of the gene expression data matrix. In this case, the genes with a percentage of missing values in the expression profile greater than 80% were removed, and the missing values were replaced with the average of the expression profile. After this preprocessing process, approximately 12% of the genes were removed, and the final GDS1116 microarray dataset was composed of 6229 genes and 131 samples.",
            "id": "applied_soft_computing/S1568494615003683.txt"
          }
        },
        {
          "text": {
            "text": "The objective of the yeast dataset is to predict the cellular localization sites of proteins. Proteins from yeast were classified into ten classes: cytoplasmic, including cytoskeletal (CYT); nuclear (NUC); vacuolar (VAC); mitochondrial (MIT); isomal (POX); extracellular, including those localized to the cell wall (EXC); proteins localized to the lumen of the endoplasmic reticulum (ERL); membrane proteins with a cleaved signal (ME1); membrane proteins with an uncleared signal (ME2); and membrane proteins with no N-terminal signM (ME3), where ME1, ME2, and ME3 proteins may be localized to the plasma membrane, the endoplasmic reticulum membrane, or the membrane of a golgi body. There are eight attributes/inputs and ten output classes. It contains 1484 instances.",
            "id": "applied_soft_computing/S156849461100161X.txt"
          }
        },
        {
          "text": {
            "text": "To test the framework, we used the yeast interactions dataset which is collected from database of interacting proteins (DIP) [18]. At the time of doing the experiments, the yeast dataset contained 15,116 interaction pairs, out of which 5522 interactions were validated by small-scale experiment, multiple experiments and/or PVM validation. Hence, these 5522 interactions were extracted for the purpose of experiments. This composes the dataset of true positive interactions for the experiment. Negative dataset (protein pairs which do not interact with each other) is not available since there is no literature that publishes the negative dataset that are experimentally verified. In order to prepare the negative dataset, we reverse the sequence of one interacting protein in the interaction pairs. It has been shown that if a sequence of one interacting partner is reversed or shuffled, then the probability of existence of interaction between these two is negligible [19]. Thus, the negative dataset is prepared by reversing the sequence of the right-side interacting partners. Final dataset contained 11,044 interactions, which is used for cross-validation experiments in our framework.",
            "id": "computers_in_biology_and_medicine/S0010482505000971.txt"
          }
        },
        {
          "text": {
            "text": "The values of \u03b4 used in SMOB-\u03b4on the human and the yeast dataset were taken from [4], while for the other datasets they were established using a procedure suggested in [4]. The values of \u03b4 used in SMOB-\u0394were determined as follows. For each dataset, we first run the experiments with SMOB\u2013VE. We calculated the MSRof all the bicluster found, and then we selected the highest as \u03b4 to be used in SMOB-\u0394. In this way we can test whether the use of \u03b4 prevented SMOB-\u03b4from discovering some interesting biclusters, only because their MSRwas higher than the used \u03b4. Therefore, SMOB-\u0394could obtain similar biclusters to those produced by SMOB\u2013VEguaranteeing in this way a fair comparison.",
            "id": "computers_in_biology_and_medicine/S0010482511002344.txt"
          }
        },
        {
          "text": {
            "text": "The second gene expression dataset is the cold-stressed yeast dataset provided by Sahara et al. [28], which contains gene expression data of about 6000 genes for five time points taken at 0.25, 0.5, 2, 4 and 8h after cold shock to yeast cells. Sahara et al. first extracted 1690 genes whose levels changed more than two-fold during an 8h period. They subsequently performed hierarchical clustering against those gene profiles using the microarray data analysis software, GeneSpring (Silicon Genetics), and finally identified five gene subsets based on the annotation information available in the software i.e., \u201cunclassified proteins\u201d, \u201camino acid biosynthesis and metabolism\u201d, \u201cRNA polymerase I and RNA processing\u201d, \u201cribosomal proteins\u201d and \u201cno annotation\u201d. We removed 81 genes containing at least two missing values from the above genes and consequently used 1609 genes to assess our algorithm.",
            "id": "artificial_intelligence_in_medicine/S0933365705000527.txt"
          }
        },
        {
          "text": {
            "text": "In this case, differences between random biclusters and the other methods are not as significant as they were with the yeast dataset. This was expected since genes from the human lymphoma expression matrix are less understood than those from the yeast dataset: in the yeast dataset, 2601 out of 2879 genes are annotated in the biological process ontology of GO,1010GO release June 2005. meanwhile in the lymphoma dataset, only 2228 out of 4026 genes are annotated in the same ontology. Even considering this lack of knowledge, PSB provides the most functionally enriched biclusters.",
            "id": "computers_in_biology_and_medicine/S0010482507000182.txt"
          }
        },
        {
          "text": {
            "text": "We also used K-means clustering [8,24] and PSMF [7] to cluster the yeast dataset, and the experimental results are listed in Tables 3 and 4. From these two tables we can see that, the two methods are also efficient in clustering the genes. However, compared with them, our method can uncover more significantly enriched modules, e.g. 15 modules were uncovered with corrected P-value<10\u221210, while only 13 and 12 modules by PSMF and K-means, respectively (Tables 1, 3, and 4). In addition, many modules uncovered by the three methods are different. Thus in practice all the three methods can be used to find more reliable result.",
            "id": "computers_in_biology_and_medicine/S0010482511001880.txt"
          }
        }
      ],
      "questions": [
        {
          "answers": [
            "Nakai"
          ],
          "question": "created_by (Yeast Dataset: dataset, ?:author)",
          "candidates": [
            {
              "text": "uci"
            },
            {
              "text": "ppi"
            },
            {
              "text": "distributions"
            },
            {
              "text": "modules"
            },
            {
              "text": "constituents"
            },
            {
              "text": "range"
            },
            {
              "text": "scores"
            },
            {
              "text": "maintenance"
            },
            {
              "text": "possibility"
            },
            {
              "text": "sahara"
            },
            {
              "text": "ovarian cancer"
            },
            {
              "text": "temperature"
            },
            {
              "text": "agents"
            },
            {
              "text": "column"
            },
            {
              "text": "ref"
            },
            {
              "text": "technique"
            },
            {
              "text": "observation"
            },
            {
              "text": "effect"
            },
            {
              "text": "grant"
            },
            {
              "text": "class"
            },
            {
              "text": "number"
            },
            {
              "text": "cells"
            },
            {
              "text": "annotation"
            },
            {
              "text": "classifiers"
            },
            {
              "text": "cluster"
            },
            {
              "text": "statistical significance"
            },
            {
              "text": "microarray"
            },
            {
              "text": "coefficient"
            },
            {
              "text": "phase"
            },
            {
              "text": "symbol"
            },
            {
              "text": "starvation"
            },
            {
              "text": "interactions"
            },
            {
              "text": "missing values"
            },
            {
              "text": "similarity"
            },
            {
              "text": "distance"
            },
            {
              "text": "categories"
            },
            {
              "text": "preprocessing"
            },
            {
              "text": "help"
            },
            {
              "text": "software"
            },
            {
              "text": "random"
            },
            {
              "text": "comparison"
            },
            {
              "text": "constraint"
            },
            {
              "text": "china"
            },
            {
              "text": "clustering"
            },
            {
              "text": "standard"
            },
            {
              "text": "rna"
            },
            {
              "text": "samples"
            },
            {
              "text": "machine"
            },
            {
              "text": "itemsets"
            },
            {
              "text": "finding"
            },
            {
              "text": "clusters"
            },
            {
              "text": "profile"
            },
            {
              "text": "graph"
            },
            {
              "text": "setup"
            },
            {
              "text": "patterns"
            },
            {
              "text": "vectors"
            },
            {
              "text": "fuzzy"
            },
            {
              "text": "extensions"
            },
            {
              "text": "microarray data"
            },
            {
              "text": "cycle"
            },
            {
              "text": "matrix"
            },
            {
              "text": "links"
            },
            {
              "text": "framework"
            },
            {
              "text": "pox"
            },
            {
              "text": "function"
            },
            {
              "text": "value"
            },
            {
              "text": "dimensionality"
            },
            {
              "text": "shocks"
            },
            {
              "text": "use"
            },
            {
              "text": "connectivity"
            },
            {
              "text": "information"
            },
            {
              "text": "conditions"
            },
            {
              "text": "parameter"
            },
            {
              "text": "meaning"
            },
            {
              "text": "kegg"
            },
            {
              "text": "mean"
            },
            {
              "text": "interacting"
            },
            {
              "text": "procedures"
            },
            {
              "text": "sequence"
            },
            {
              "text": "partner"
            },
            {
              "text": "purification"
            },
            {
              "text": "rib"
            },
            {
              "text": "profiles"
            },
            {
              "text": "subsets"
            },
            {
              "text": "ecoc"
            },
            {
              "text": "fig"
            },
            {
              "text": "literature"
            },
            {
              "text": "recognition"
            },
            {
              "text": "probabilistic model"
            },
            {
              "text": "bp"
            },
            {
              "text": "database"
            },
            {
              "text": "r"
            },
            {
              "text": "centrality"
            },
            {
              "text": "bicluster"
            },
            {
              "text": "procedure"
            },
            {
              "text": "natural"
            },
            {
              "text": "ontology"
            },
            {
              "text": "al"
            },
            {
              "text": "amino acid"
            },
            {
              "text": "form"
            },
            {
              "text": "precision"
            },
            {
              "text": "product"
            },
            {
              "text": "authors"
            },
            {
              "text": "mit"
            },
            {
              "text": "threshold"
            },
            {
              "text": "italy"
            },
            {
              "text": "b"
            },
            {
              "text": "shock"
            },
            {
              "text": "structural information"
            },
            {
              "text": "type"
            },
            {
              "text": "transcript"
            },
            {
              "text": "ratio"
            },
            {
              "text": "biosynthesis"
            },
            {
              "text": "feature vectors"
            },
            {
              "text": "novel"
            },
            {
              "text": "values"
            },
            {
              "text": "combinations"
            },
            {
              "text": "algorithm"
            },
            {
              "text": "cross-validation"
            },
            {
              "text": "wine"
            },
            {
              "text": "elements"
            },
            {
              "text": "classification"
            },
            {
              "text": "cycles"
            },
            {
              "text": "majority"
            },
            {
              "text": "protein interaction"
            },
            {
              "text": "section"
            },
            {
              "text": "network"
            },
            {
              "text": "p-value"
            },
            {
              "text": "body"
            },
            {
              "text": "protein"
            },
            {
              "text": "iris"
            },
            {
              "text": "weight"
            },
            {
              "text": "networks"
            },
            {
              "text": "entropy measure"
            },
            {
              "text": "module"
            },
            {
              "text": "final"
            },
            {
              "text": "c2"
            },
            {
              "text": "types"
            },
            {
              "text": "replications"
            },
            {
              "text": "component"
            },
            {
              "text": "result"
            },
            {
              "text": "files"
            },
            {
              "text": "expression"
            },
            {
              "text": "support"
            },
            {
              "text": "source"
            },
            {
              "text": "frequent itemsets"
            },
            {
              "text": "edges"
            },
            {
              "text": "rand"
            },
            {
              "text": "structures"
            },
            {
              "text": "plasma"
            },
            {
              "text": "order"
            },
            {
              "text": "signal"
            },
            {
              "text": "distances"
            },
            {
              "text": "paper"
            },
            {
              "text": "behavior"
            },
            {
              "text": "algorithms"
            },
            {
              "text": "lymphoma"
            },
            {
              "text": "work"
            },
            {
              "text": "signals"
            },
            {
              "text": "harp"
            },
            {
              "text": "science"
            },
            {
              "text": "area"
            },
            {
              "text": "collection"
            },
            {
              "text": "techniques"
            },
            {
              "text": "subunit"
            },
            {
              "text": "addition"
            },
            {
              "text": "hierarchical clustering"
            },
            {
              "text": "<"
            },
            {
              "text": "k"
            },
            {
              "text": "proteins"
            },
            {
              "text": "methodologies"
            },
            {
              "text": "average"
            },
            {
              "text": "levels"
            },
            {
              "text": "rand index"
            },
            {
              "text": "functions"
            },
            {
              "text": "period"
            },
            {
              "text": "fp"
            },
            {
              "text": "means"
            },
            {
              "text": "time"
            },
            {
              "text": "inclusion"
            },
            {
              "text": "weights"
            },
            {
              "text": "discrepancy"
            },
            {
              "text": "classes"
            },
            {
              "text": "noise"
            },
            {
              "text": "go"
            },
            {
              "text": "bar"
            },
            {
              "text": "performance comparison"
            },
            {
              "text": "strategy"
            },
            {
              "text": "alzheimer"
            },
            {
              "text": "complexes"
            },
            {
              "text": "process"
            },
            {
              "text": "sample"
            },
            {
              "text": "experimental results"
            },
            {
              "text": "localization"
            },
            {
              "text": "lumen"
            },
            {
              "text": "c-means"
            },
            {
              "text": "classification accuracy"
            },
            {
              "text": "fdr"
            },
            {
              "text": "graphs"
            },
            {
              "text": "silicon"
            },
            {
              "text": "chemical"
            },
            {
              "text": "superiority"
            },
            {
              "text": "repository"
            },
            {
              "text": "tables"
            },
            {
              "text": "nb"
            },
            {
              "text": "approach"
            },
            {
              "text": "probability"
            },
            {
              "text": "papers"
            },
            {
              "text": "difference"
            },
            {
              "text": "features"
            },
            {
              "text": "group"
            },
            {
              "text": "foundation"
            },
            {
              "text": "frequency"
            },
            {
              "text": "class noise"
            },
            {
              "text": "extension"
            },
            {
              "text": "record"
            },
            {
              "text": "tissue"
            },
            {
              "text": "progression"
            },
            {
              "text": "correlation"
            },
            {
              "text": "reference"
            },
            {
              "text": "membrane"
            },
            {
              "text": "reason"
            },
            {
              "text": "homo"
            },
            {
              "text": "interaction"
            },
            {
              "text": "entropy"
            },
            {
              "text": "errors"
            },
            {
              "text": "chemical analysis"
            },
            {
              "text": "fuzzy c-means"
            },
            {
              "text": "ratios"
            },
            {
              "text": "clustering algorithms"
            },
            {
              "text": "points"
            },
            {
              "text": "model"
            },
            {
              "text": "localizations"
            },
            {
              "text": "accuracy"
            },
            {
              "text": "separation"
            },
            {
              "text": "analysis"
            },
            {
              "text": "practice"
            },
            {
              "text": "problem"
            },
            {
              "text": "assembly"
            },
            {
              "text": "purpose"
            },
            {
              "text": "category"
            },
            {
              "text": "annotations"
            },
            {
              "text": "study"
            },
            {
              "text": "distribution"
            },
            {
              "text": "disjoint"
            },
            {
              "text": "confidence level"
            },
            {
              "text": "objects"
            },
            {
              "text": "saccharomyces cerevisiae"
            },
            {
              "text": "instance"
            },
            {
              "text": "dataset"
            },
            {
              "text": "plant"
            },
            {
              "text": "disparity"
            },
            {
              "text": "correlation analysis"
            },
            {
              "text": "limitations"
            },
            {
              "text": "hypergeometric distribution"
            },
            {
              "text": "microarray data analysis"
            },
            {
              "text": "methods"
            },
            {
              "text": "release"
            },
            {
              "text": "structure"
            },
            {
              "text": "injection"
            },
            {
              "text": "encyclopedia"
            },
            {
              "text": "acid"
            },
            {
              "text": "note"
            },
            {
              "text": "overlap"
            },
            {
              "text": "kyoto"
            },
            {
              "text": "knn"
            },
            {
              "text": "species"
            },
            {
              "text": "set"
            },
            {
              "text": "relationship"
            },
            {
              "text": "tool"
            },
            {
              "text": "t-test"
            },
            {
              "text": "row"
            },
            {
              "text": "differences"
            },
            {
              "text": "rates"
            },
            {
              "text": "cell cycle"
            },
            {
              "text": "discovery"
            },
            {
              "text": "detectability"
            },
            {
              "text": "existence"
            },
            {
              "text": "psb"
            },
            {
              "text": "performance"
            },
            {
              "text": "sse"
            },
            {
              "text": "series"
            },
            {
              "text": "method"
            },
            {
              "text": "experiments"
            },
            {
              "text": "version"
            },
            {
              "text": "complex data"
            },
            {
              "text": "yeast"
            },
            {
              "text": "exposure"
            },
            {
              "text": "error rate"
            },
            {
              "text": "knowledge"
            },
            {
              "text": "wall"
            },
            {
              "text": "significance"
            },
            {
              "text": "lac"
            },
            {
              "text": "sites"
            },
            {
              "text": "pvm"
            },
            {
              "text": "duplicates"
            },
            {
              "text": "results"
            },
            {
              "text": "confidence"
            },
            {
              "text": "prediction"
            },
            {
              "text": "experiment"
            },
            {
              "text": "part"
            },
            {
              "text": "sgd"
            },
            {
              "text": "estimation"
            },
            {
              "text": "index"
            },
            {
              "text": "pattern recognition"
            },
            {
              "text": "feature"
            },
            {
              "text": "metrics"
            },
            {
              "text": "performances"
            },
            {
              "text": "median"
            },
            {
              "text": "genes"
            },
            {
              "text": "standard deviation"
            },
            {
              "text": "svms"
            },
            {
              "text": "student"
            },
            {
              "text": "validation"
            },
            {
              "text": "gene"
            },
            {
              "text": "dip"
            },
            {
              "text": "percentage"
            },
            {
              "text": "bpca"
            },
            {
              "text": "well"
            },
            {
              "text": "predictor"
            },
            {
              "text": "datasets"
            },
            {
              "text": "terms"
            },
            {
              "text": "error"
            },
            {
              "text": "gene expression"
            },
            {
              "text": "error rates"
            },
            {
              "text": "e. coli"
            },
            {
              "text": "similarities"
            },
            {
              "text": "membrane proteins"
            },
            {
              "text": "deviation"
            },
            {
              "text": "array"
            },
            {
              "text": "gene expression data"
            },
            {
              "text": "instances"
            },
            {
              "text": "point"
            },
            {
              "text": "case"
            },
            {
              "text": "enrichment"
            },
            {
              "text": "measure"
            },
            {
              "text": "data"
            },
            {
              "text": "score"
            },
            {
              "text": "nitrogen"
            },
            {
              "text": "time series"
            },
            {
              "text": "assignment"
            },
            {
              "text": "genetics"
            },
            {
              "text": "cancer"
            },
            {
              "text": "imputation"
            },
            {
              "text": "cell"
            },
            {
              "text": "genomes"
            },
            {
              "text": "sets"
            },
            {
              "text": "sep"
            },
            {
              "text": "contrast"
            },
            {
              "text": "list"
            },
            {
              "text": "k-means"
            },
            {
              "text": "perturbations"
            },
            {
              "text": "level"
            },
            {
              "text": "mcl"
            },
            {
              "text": "human"
            },
            {
              "text": "correctness"
            },
            {
              "text": "property"
            },
            {
              "text": "name"
            },
            {
              "text": "objective"
            },
            {
              "text": "properties"
            },
            {
              "text": "rate"
            }
          ]
        }
      ]
    },
    {
      "support": [
        {
          "text": {
            "text": "For purposes of classification, we note that we can use the posteriors given by \u03b1\u03c9i in conjunction with the cluster structure in the MNIST database so as to search for the most similar object in a manner akin to that in [60]. The idea is to find the digit-classes which are most similar to the testing imagery. This can be viewed as a query in which the graph within the database-classes that is most similar to the query is the one that is retrieved. The search process is as follows. First, we compute the set of posteriors for the testing digit for each class in the dataset. The classes with the most similar digits are expected to have larger posteriors in comparison with the others. Thus, we select the first 3 classes, i.e. digit groups, in the training set which correspond to those posteriors \u03b1\u03c9i that are the largest in rank amongst those computed for the testing raster scan. With the candidate classes at hand, we search the training samples in each class so as to find the one that is most similar to the testing digit. The testing raster scan then belongs to the class whose item, i.e. digit, corresponds to the minimum pairwise Euclidean distance. It must be stressed that this can also be viewed as a simple recall strategy which illustrates how the embedding method presented here can be used for purposes of retrieval in a broader sense. The information retrieval literature contains more principled and more efficient alternatives which should be used if the example given here is scaled to very large databases of thousands or tens of thousands of images.",
            "id": "computer_vision_and_image_understanding/S1077314211000737.txt"
          }
        },
        {
          "text": {
            "text": "The WiSARD (Wilkie, Stonham and Aleksander's Recognition Device) weightless neural network model has its functionality based on the collective response of RAM-based neurons. WiSARD's learning phase consists on writing at the RAM neurons\u2019 positions addressed (typically through a pseudo-random mapping) by binary training patterns. By counting the frequency of writing accesses at RAM neuron positions during the learning phase, it is possible to associate the most accessed addresses with the corresponding input field contents that defined them. The idea of associating this process with the formation of \u201cmental\u201d images is explored in the DRASiW model, a WiSARD extension provided with the ability of producing pattern examples, or prototypes, derived from learnt categories. This work demonstrates the equivalence of two ways of generating such prototypes: (i) via frequency counting and filtering and (ii) via formulating fuzzy rules. Moreover, it is shown, through the exploration of the MNIST database of handwritten digits as benchmark, how the process of mental images formation can improve WiSARD's classification skills.",
            "id": "neurocomputing/S0925231210000159.txt"
          }
        },
        {
          "text": {
            "text": "The state of the art approach uses a linear filter for encoding an image into a code and a sparsifying logistic function that the code goes through before being decoded by another linear filter [12]. The learning is achieved by training the encoder to produce codes whose sparse representation accurately reconstruct the original image after decoding. The error rate on the MNIST database reported is 0.39%. Note that we use this dataset to evaluate the performance of our online algorithm and we do not claim improvement over the state of the art performance of an offline algorithm on this specific dataset.",
            "id": "image_and_vision_computing/S0262885609002364.txt"
          }
        },
        {
          "text": {
            "text": "We conduct numerical experiments with 5 different datasets from the UCI Repository [28] as well as the MNIST database [24] to compare the performance of the proposed method to those of the classical solutions presented in Ref. [29] (see Table 1). The Letter and Segment datasets (low dimensionality) use 16 and 19 primitive numerical attributes and represent, respectively, features extracted from images of capital letters in the English alphabet and hand-segmented outdoor images. The Satellite dataset is based on 36 attributes extracted from satellite images in 3\u00d73 pixel neighborhoods in 4 spectral bands. With 114 numerical and 180 logical attributes, respectively, the MNIST and DNA datasets represent features extracted from images of handwritten digits and DNA sequences. These last two datasets are considered high dimensional [24].",
            "id": "applied_soft_computing/S1568494610002553.txt"
          }
        },
        {
          "text": {
            "text": "Now we illustrate the ability of our method to embed graph attributes extracted from a real-world dataset. To this end, we show results on digit classification making use of the MNIST dataset. The MNIST database which contains a training set of 60,000 grey-scale images of handwritten digits from 0 to 9, and a testing set of 10,000 images. Sample handwritten digits in the dataset are shown in Fig. 7. The image sizes are 28\u00d728. We have chosen the dataset due to its size and its widespread use in the literature. We stress in passing that the aim in this section is not to develop an OCR algorithm but rather to provide a quantitative study on a large, real-world dataset comparing our embedding approach with other subspace projection methods. Note that there are a wide variety of OCR techniques that have been applied to the dataset. As these are out of the scope of this paper, for a detailed list we refer the reader to the MNIST website.2http://yann.lecun.com/exdb/mnist/.2",
            "id": "computer_vision_and_image_understanding/S1077314211000737.txt"
          }
        },
        {
          "text": {
            "text": "In the third experiment, we used images from the MNIST database (Mixed National Institute of Standards and Technology database) [28], which is a public image dataset that contains 10,000 images of handwritten digits from the NIST Special Database 1 (SD-1) and Special Database 3 (SD-3), to assess our shape descriptors. The original binary images from SD-1 and SD-3 were size normalized to fit in a pixel box with 20 rows per 20 columns (preserving their aspect ratio) and centered in grayscale images with 28 rows per 28 columns (which were binarized before applying the shape analysis methods tested).",
            "id": "computer_vision_and_image_understanding/S1077314214001374.txt"
          }
        },
        {
          "text": {
            "text": "Since the writers of the digits in the SD-1 and SD-3 databases belong to two very distinct groups of people (in social and cultural terms), the goal of the MNIST database is to mix images from both datasets in order to make the tests with pattern recognition algorithms more complex and realistic, i.e., more independent from the social and economic profile of the writers [28].",
            "id": "computer_vision_and_image_understanding/S1077314214001374.txt"
          }
        },
        {
          "text": {
            "text": "First, we demonstrate the proposed model described in Section\u00a03 using a toy data set with three domains, which is created using handwritten digits from the MNIST database (LeCun, Bottou, Bengio, & Haffner, 1998). The first domain contains original handwritten digits, where each image is downsampled to 16 \u00d7 16 pixels. We synthesize objects for the second and third domains by rotating handwritten digits by 90 and 180 degrees, clockwise, respectively. Thus, we obtain three-domain objects that share a latent space. The number of objects in each domain is 200 for all domains. We would like to match the rotated digits in different domains without information about rotation or correspondence.",
            "id": "information_processing_&_management/S0306457315001508.txt"
          }
        }
      ],
      "questions": [
        {
          "answers": [
            "Database of handwritten digits"
          ],
          "question": "description_of (MNIST Database: dataset, ?:description)",
          "candidates": [
            {
              "text": "process"
            },
            {
              "text": "digit"
            },
            {
              "text": "uci"
            },
            {
              "text": "sample"
            },
            {
              "text": "performance"
            },
            {
              "text": "groups"
            },
            {
              "text": "wisard"
            },
            {
              "text": "benchmark"
            },
            {
              "text": "shape analysis"
            },
            {
              "text": "input"
            },
            {
              "text": "method"
            },
            {
              "text": "reader"
            },
            {
              "text": "field"
            },
            {
              "text": "experiments"
            },
            {
              "text": "aim"
            },
            {
              "text": "projection"
            },
            {
              "text": "error rate"
            },
            {
              "text": "euclidean"
            },
            {
              "text": "ocr"
            },
            {
              "text": "ability"
            },
            {
              "text": "repository"
            },
            {
              "text": "approach"
            },
            {
              "text": "learning"
            },
            {
              "text": "exploration"
            },
            {
              "text": "english"
            },
            {
              "text": "code"
            },
            {
              "text": "ratio"
            },
            {
              "text": "filter"
            },
            {
              "text": "ref"
            },
            {
              "text": "training"
            },
            {
              "text": "digits"
            },
            {
              "text": "results"
            },
            {
              "text": "candidate"
            },
            {
              "text": "addresses"
            },
            {
              "text": "binary images"
            },
            {
              "text": "class"
            },
            {
              "text": "number"
            },
            {
              "text": "features"
            },
            {
              "text": "algorithm"
            },
            {
              "text": "neural network"
            },
            {
              "text": "experiment"
            },
            {
              "text": "equivalence"
            },
            {
              "text": "aspect"
            },
            {
              "text": "projection methods"
            },
            {
              "text": "sequences"
            },
            {
              "text": "aspect ratio"
            },
            {
              "text": "pixels"
            },
            {
              "text": "cluster"
            },
            {
              "text": "frequency"
            },
            {
              "text": "phase"
            },
            {
              "text": "pattern recognition"
            },
            {
              "text": "neighborhoods"
            },
            {
              "text": "neural network model"
            },
            {
              "text": "extension"
            },
            {
              "text": "rules"
            },
            {
              "text": "rank"
            },
            {
              "text": "space"
            },
            {
              "text": "classification"
            },
            {
              "text": "mapping"
            },
            {
              "text": "formation"
            },
            {
              "text": "distance"
            },
            {
              "text": "correspondence"
            },
            {
              "text": "writers"
            },
            {
              "text": "categories"
            },
            {
              "text": "section"
            },
            {
              "text": "network"
            },
            {
              "text": "cluster structure"
            },
            {
              "text": "sense"
            },
            {
              "text": "comparison"
            },
            {
              "text": "image"
            },
            {
              "text": "technology"
            },
            {
              "text": "satellite images"
            },
            {
              "text": "table"
            },
            {
              "text": "object"
            },
            {
              "text": "samples"
            },
            {
              "text": "solutions"
            },
            {
              "text": "pairwise"
            },
            {
              "text": "letter"
            },
            {
              "text": "ram"
            },
            {
              "text": "model"
            },
            {
              "text": "online"
            },
            {
              "text": "online algorithm"
            },
            {
              "text": "columns"
            },
            {
              "text": "satellite"
            },
            {
              "text": "state"
            },
            {
              "text": "response"
            },
            {
              "text": "encoder"
            },
            {
              "text": "outdoor"
            },
            {
              "text": "rotation"
            },
            {
              "text": "prototypes"
            },
            {
              "text": "profile"
            },
            {
              "text": "alternatives"
            },
            {
              "text": "linear filter"
            },
            {
              "text": "analysis"
            },
            {
              "text": "graph"
            },
            {
              "text": "improvement"
            },
            {
              "text": "patterns"
            },
            {
              "text": "testing"
            },
            {
              "text": "datasets"
            },
            {
              "text": "terms"
            },
            {
              "text": "hand"
            },
            {
              "text": "standards"
            },
            {
              "text": "nist"
            },
            {
              "text": "error"
            },
            {
              "text": "shape descriptors"
            },
            {
              "text": "order"
            },
            {
              "text": "drasiw"
            },
            {
              "text": "domains"
            },
            {
              "text": "paper"
            },
            {
              "text": "toy"
            },
            {
              "text": "function"
            },
            {
              "text": "positions"
            },
            {
              "text": "raster"
            },
            {
              "text": "art"
            },
            {
              "text": "information retrieval"
            },
            {
              "text": "dimensionality"
            },
            {
              "text": "study"
            },
            {
              "text": "query"
            },
            {
              "text": "size"
            },
            {
              "text": "work"
            },
            {
              "text": "attributes"
            },
            {
              "text": "use"
            },
            {
              "text": "domain"
            },
            {
              "text": "databases"
            },
            {
              "text": "information"
            },
            {
              "text": "techniques"
            },
            {
              "text": "numerical experiments"
            },
            {
              "text": "subspace"
            },
            {
              "text": "objects"
            },
            {
              "text": "skills"
            },
            {
              "text": "people"
            },
            {
              "text": "conjunction"
            },
            {
              "text": "dataset"
            },
            {
              "text": "box"
            },
            {
              "text": "goal"
            },
            {
              "text": "data"
            },
            {
              "text": "variety"
            },
            {
              "text": "retrieval"
            },
            {
              "text": "imagery"
            },
            {
              "text": "idea"
            },
            {
              "text": "i"
            },
            {
              "text": "dna"
            },
            {
              "text": "large databases"
            },
            {
              "text": "alphabet"
            },
            {
              "text": "images"
            },
            {
              "text": "methods"
            },
            {
              "text": "logistic function"
            },
            {
              "text": "item"
            },
            {
              "text": "uci repository"
            },
            {
              "text": "functionality"
            },
            {
              "text": "structure"
            },
            {
              "text": "euclidean distance"
            },
            {
              "text": "fuzzy rules"
            },
            {
              "text": "clockwise"
            },
            {
              "text": "fig"
            },
            {
              "text": "end"
            },
            {
              "text": "training set"
            },
            {
              "text": "note"
            },
            {
              "text": "literature"
            },
            {
              "text": "device"
            },
            {
              "text": "list"
            },
            {
              "text": "dna sequences"
            },
            {
              "text": "recognition"
            },
            {
              "text": "descriptors"
            },
            {
              "text": "mnist"
            },
            {
              "text": "share"
            },
            {
              "text": "degrees"
            },
            {
              "text": "set"
            },
            {
              "text": "database"
            },
            {
              "text": "pixel"
            },
            {
              "text": "sparse representation"
            },
            {
              "text": "classes"
            },
            {
              "text": "filtering"
            },
            {
              "text": "neurons"
            },
            {
              "text": "segment"
            },
            {
              "text": "mixed"
            },
            {
              "text": "representation"
            },
            {
              "text": "examples"
            },
            {
              "text": "national"
            },
            {
              "text": "tests"
            },
            {
              "text": "rate"
            },
            {
              "text": "recall"
            },
            {
              "text": "strategy"
            },
            {
              "text": "codes"
            },
            {
              "text": "scope"
            },
            {
              "text": "shape"
            },
            {
              "text": "first"
            }
          ]
        }
      ]
    },
    {
      "support": [
        {
          "text": {
            "text": "In terms of the g-mean metric, as shown in Table 4, IMIES is significantly superior to MIES on five datasets (Heart, Ionosphere, Breast, Cardiotocography and Spambase). Even for other three datasets, our method still gives comparable results to MIES. While accompanying low TNRs, interestingly, MIES achieves rather high TPRs on the Breast, the Cardiotocography and the Spambase datasets, from which we can know that the kernel parameters selected by MIES on these datasets produce overfitting decision boundaries. The high test accuracies on the positive samples (i.e., high TPRs) are obtained at the cost of the high test errors on the negative samples (i.e., low TNRs), which finally results in the low g-means. This may be caused by the improper selection of the edge or the interior sample set. For example, regarding the Spambase dataset, if we use the new objective function Eq. (20) but still select the edge samples using EISDTP, then MIES can obtain a kernel parameter value that yields a g-mean of 47.93%, larger than the original g-mean of 19.13% (see Table 4); furthermore, if we use the new objective function and use ESDCP to select the edge samples, namely, using the proposed IMIES, the g-mean further increases to 72.36 % (see Table 4). This is an indication that EISDTP employed in the original MIES has selected the improper edge or interior sample set. On the other hand, IMIES also outperforms DTL on five datasets (Biomed, Heart, Ionosphere, Breast and Cardiotocography) and receives comparable g-means on the Diabetes and the Svmguide1 datasets. DTL gives unstable performance. For example, the performance of DTL can be comparable to that of IMIES on the Diabetes and the Svmguide1 datasets and even better on the Spambase dataset. But for the Heart and the Ionosphere datasets, it results in overfitting models, with high TPRs but low TNRs. The unstable performance is due to the fact that DTL gets a candidate kernel parameter by the bisection method in its iterations [27]; however, the bisection method could easily get stuck in a bad local optimum because of its sensitivity to the initial candidate interval of the kernel parameter. Among these methods, only our method achieves high g-means on the test sets and gets good balances between TPRs and TNRs. In summary, compared with other two methods, the proposed IMIES is able to select more suitable kernel parameters for OCSVM.",
            "id": "knowledge-based_systems/S095070511500355X.txt"
          }
        },
        {
          "text": {
            "text": "We have demonstrated the use abductive machine learning for spam detection and the ranking of 57 spam detection attributes of the spambase dataset according to their predictive value. In general, the proposed approach offers the following advantages: improved classification accuracy, greater insight into the email feature set, data reduction through automatic exclusion of insignificant input features, simpler model development, and reduced training time. Classification accuracies of about 91.7% were achieved with a single model that automatically selects only 10 input attributes, thus achieving data reduction by a ratio of approximately 6:1. Accuracy was further improved up to 92.4% through the use of a 3-member abductive network ensembles with the members trained on different subsets of the training set using all 57 attributes. Abductive modeling was compared with probabilistic neural networks employing a genetic learning algorithm and developed using the NeuroShell Classifier software. Results indicate that the abductive approach gives better classification accuracy and requires much shorter training times. Three methods were adopted for ranking the 57 attributes to determine the most effective spam predictors: (1) The Z-statistic measure of the significance of the difference between the two means of each attribute for the spam and legitimate subsets of the data, (2) A GMDH-based approach of iteratively forcing the synthesis of a simple model and excluding the inputs selected at each stage, and (3) Information provided by the NeuroShell Classifier on the relative importance of inputs to the genetic learning algorithm. The top most effective 12 predictors in each of the above three rankings have 9 attributes in common. The same 9 attributes are automatically selected by the 10-input optimum monolithic abductive network model. These attributes highlight the greater frequency of certain words or characters in either spam or legitimate messages. Performance was compared with that of MLP neural networks and na\u00efve Bayesian classifiers reported in the literature for the same email dataset. The comparison indicates that the abductive network models provide better classification accuracy, spam recall and spam precision with false-positive rates as low as 4.3%. Future work would consider methods for finer ranking within attribute groups in the GMDH-based method to achieve complete ranking of the item set, and the use of ensembles with members trained using different learning paradigms to achieve further improvements in classification performance.",
            "id": "applied_soft_computing/S1568494609002610.txt"
          }
        },
        {
          "text": {
            "text": "To further justify our claim, we present a ROC analysis result with the spambase dataset. This dataset is considered here since it has a moderate imbalance ratio and instance volume. The original spambase has an imbalance ratio of 10; therefore, in this experiments, we test K from 1 to 9. Through experiments in Section 6.3.1, we notice that SMOTE achieves a better performance comparing to other state-of-the-art methods; we hence adopt it as a representative to compare with our proposal. Finally, we depict the ROC curves of the two approaches in Fig. 7. Clearly, EnSVM+ outperforms the other two methods in general.",
            "id": "information_processing_&_management/S030645731000097X.txt"
          }
        },
        {
          "text": {
            "text": "To evaluate and compare the performance of the proposed approach, we used the spambase dataset [42]. This database has been created in June\u2013July 1999 by M. Hopkins, E. Reeber, G. Forman, and J. Suermondt at Hewlett-Packard Labs. It consists of 4601 instances of legitimate and spam email messages with 39.4% being spam. Each instance is characterized by 57 input attributes and is labeled as spam (represented as 1) or legitimate (represented as 0). Table 2 lists the attribute number and name, as well as the average and standard deviation values for each attribute in both the legitimate and spam populations. Attributes 1\u201348 give the percentage of words in the email message for the respective keyword indicated in the attribute name. Attributes 49\u201354 give the percentage of characters in the email message for the respective character indicated in the attribute name. Attributes 55 and 56 give the average and maximum lengths, respectively, of uninterrupted sequences of capital letters in the message. Attribute 57 gives the total number of capital letters in the message. The attribute number will be used as the variable number for models described throughout this paper. Attribute number 58 in the dataset is the true email class. The dataset has no missing attribute values. Table 2 also lists the mean and standard deviation of each attribute in both the legitimate population (\u03bcl and \u03c3l respectively) and spam population (\u03bcs and \u03c3s respectively). The last column in Table 2 gives the absolute value of the Z statistic for each attribute. The z parameter measures the magnitude of the absolute difference between the two means of a given attribute in the legitimate and spam populations relative to variations about the two means. The larger the value of z, the more significant is the difference between the two means and therefore the larger the value discriminatory power of the attribute in classifying the dataset as spam/legitimate. Sorting the list of input attributes in a descending order based on the values of their Z statistic gives the following ranking: {21, 25, 26, 19, 23, 7, 53, 52, 16, 5, 27, 17, 57, 6, 11, 9, 30, 37, 3, 24, 18, 35, 28, 8, 46, 56, 45, 42, 43, 29, 15, 36, 31, 20, 39, 33, 32, 10, 34, 1, 13, 41, 44, 48, 55, 50, 22, 40, 51, 49, 14, 54, 47, 4, 38, 2, 12}. Attributes toward the beginning of the list should prove more effective than attributes toward the end. However, this simple ranking approach does not take into account possible mutual correlation between various attributes when grouped together to classify the dataset. Since using several attributes is more common in discriminating spam from legitimate emails and leads to better results, forming subsets of attributes based on the ranking of individual attributes may not produce compact subsets of predictors and may miss complementary attributes that have low individual ranking.",
            "id": "applied_soft_computing/S1568494609002610.txt"
          }
        },
        {
          "text": {
            "text": "The NeuroShell Classifier software provides an estimate of the relative importance of each input used to train the model, which is updated during training. It would be interesting to compare results using this feature with the ranking based on the Z statistic and the tentative GMDH-based ranking. Unfortunately, the NeuroShell ranking is reliable only for a small number of inputs, e.g. up to 20 inputs [45], and therefore could not be reliably used to give an overall ranking of the full set of 57 attributes of the spambase dataset. Fig. 4 is a bar chart depicting the importance of input attributes, which is produced by the software at the end of training for model 3 in Table 8 on the top 19 attributes. According to Fig. 4, the most important 12 of these attributes are {57, 27, 25, 53, 23, 52, 37, 56, 7, 5, 21, 16}. As shown in Fig. 5, this subset contains 9 out of the 12 attributes at the top of the Z statistic ranking list and 10 out of the 12 attributes at the top of the GMDH-based ranking list. This indicates reasonable agreement between the three ranking schemes considered. In all three ranking lists, attribute 23 occupies the 5th position from the top, while attribute 5 occupies the 10th position. The 9 attributes {5, 7, 16, 21, 23, 25, 27, 52, 53}, which are shaded in Fig. 5, are common to the top 12 attributes in all three ranking lists. It is interesting to note that all these 9 attributes are included in the subset of 10 input attributes selected automatically by the optimum monolithic abductive model with CPM=1 (middle row in Table 3). Rows for these most effective spam detection attributes are shaded in Table 2. All these 9 attributes are characterized by a large value of the Z statistic. Seven attributes indicate a larger presence of the keywords \u201cour\u201d, \u201cremove\u201d, \u201cfree\u201d, \u201cyour\u201d, \u201c000\u201d and the characters \u201c!\u201d and \u201c$\u201d in spam messages. The remaining two attributes indicate a larger presence in legitimate messages of the words \u201cgeorge\u201d and \u201chp\u201d, which refer to the first name and the affiliation of the researcher who developed the spambase dataset.",
            "id": "applied_soft_computing/S1568494609002610.txt"
          }
        },
        {
          "text": {
            "text": "Abductive network models were developed for classifying the spambase dataset into spam or legitimate messages. The dataset was randomly split into a training set of 2844 cases and an evaluation set of 1757 cases. The percentage of spam in the two datasets was 38.4% and 41.1%, respectively. In the first modeling experiment, the full training set was used to develop abductive network models that utilize all available 57 attributes for spam classification. We refer to such models as monolithic models to contrast them with those developed later using the modular approach of abductive network ensembles to be described in Section 5.2. Table 3 shows the abductive model structures synthesized at three levels of model complexity as indicated by the CPM parameter specified prior to training, together with their classification accuracy on both the training and evaluation sets. The variable number indicated at a model input, e.g. Var_i, corresponds to the number of the attribute selected as input to the model during training. Var_58 is the binary classification model output (i.e. spam or legitimate). As the CPM value increased, simpler models are synthesized. In all three models developed with CPM=0.3, 1, and 3; only a small subset of the 57 input attributes is selected automatically during training. The number of selected attributes is 10, 10, and 9, respectively. Eight attributes {7, 16, 21, 23, 25, 46, 52, and 53} are common to all three input subsets. This indicates the effectiveness of such attributes as spam predictors and the robustness of the modeling process. During model evaluation on the evaluation set, computed model output was rounded to 0 (legitimate) or 1 (spam) based on a simple threshold of 0.5. We will later consider operation at other thresholds by presenting the receiver operator characteristics in Section 5.4. Classification performance on the external evaluation set is approximately equal to that on the training set. Best classification performance on the evaluation set is obtained using the optimum model with CPM=1 which gives a classification accuracy of 91.7%. This model will be considered as the optimum monolithic model throughout the paper. The subset of 10 input attributes selected by this model is {5, 7, 16, 21, 23, 25, 27, 46, 52, and 53}. Eight of these 10 attributes are among the top 10 positions of the Z statistic ranking given in Section 4, which confirms their superior explanatory qualities. Table 4(a) shows the resulting confusion matrix and Table 5(a) lists the parameters characterizing the classification performance of this optimum model on the evaluation set, in terms of sensitivity, specificity, \u201cspam\u201d precision, and \u201clegitimate\u201d precision, as well as classification accuracy. The results indicate a minimum value of approximately 88.2% for all performance parameters. A good aspect of this classifier is that specificity is greater than sensitivity. This means fewer false-positive (FP) classification errors where a legitimate email is wrongly classified as spam email, which is a more serious outcome compared to a spam being considered as legitimate i.e. a false negative (FN). Table 4(a) gives the number of FP and FN errors as 61 and 85, respectively, out of the 1757 evaluation cases.",
            "id": "applied_soft_computing/S1568494609002610.txt"
          }
        },
        {
          "text": {
            "text": "The effectiveness of using multilayer perceptron neural networks and na\u00efve Bayesian classifiers has been recently evaluated for spam filtering on the spambase dataset used in this paper [48]. In [48], the dataset was randomly shuffled and then partitioned into five independent subsets using 5-fold cross validation. Five experiments were conducted. In each experiment, four subsets were used for training the classifier and the remaining subset was used for evaluation. The authors of [48] examined various MLP architectures to determine the best classifier model which was then tested for various cutoff threshold values (see Table 6 in [48]). They also tested the na\u00efve Bayesian classifier (see Table 7 in [48]). Our comparisons with their best models are based on classification accuracy (Acc), false-positive rate (FPR), false-negative rate (FNR), spam recall (SR), spam precision (SP) and F-measure (FM). Table 9 summarizes the performance of our four abductive modeling experiments of Table 5 of this paper, and that of the five MLP experiments and five NB experiments reported in [48]. In contrast to their work, we label \u201cspam\u201d as positive and \u201clegitimate\u201d as negative, which is more common in anti-spam literature, and we recalculated the performance measures for their work according to the definitions in Section 3. The results indicate that our proposed approaches yield much better performance compared to the na\u00efve Bayesian classifier in all cases and offer a slight improvement on the MLP models in most of the cases in terms of classification accuracy, spam recall and spam precision. Moreover, unlike the MLP that requires long training times (see Section 7 in [48]), the GMDH-based approach requires much shorter training time as shown in Table 8.",
            "id": "applied_soft_computing/S1568494609002610.txt"
          }
        },
        {
          "text": {
            "text": "Detecting differences between populations (or datasets) is an important research topic in machine learning, yet an common application means of evaluating, such as a new medical product by comparing with an old one. Previous researchers focus on change detection. In this paper, we measure the uncertainty of structural differences, such as mean and distribution function differences, between populations, using a confidence interval (CI), via an empirical likelihood approach. We present a statistically sound method for estimating CIs for differences between non-parametric populations with missing values, which are imputed by using simple random hot deck imputation method. We illustrate the power of CI estimation as a new machine learning technique for, such as, distinguishing spam from non-spam emails in spambase dataset downloaded from UCI.",
            "id": "pattern_recognition_letters/S0167865508000068.txt"
          }
        },
        {
          "text": {
            "text": "Additionally, the execution time of the proposed method has been also compared to those of the wrapper-based feature selection methods on the different datasets and the obtained results are reported in Table 13. The reported results show that in most cases the execution time of the proposed method is lower than the wrapper based methods. For example it can be seen from the results that the GCACO selected the final subset of features for Spambase dataset after 6846ms. While in this case the HGAFS, ACOFS and HGAFS selected the final subset after 98,328, 87,193 and 531,439ms respectively. In this case the results show the proposed method is nearly 88 times faster than the PSOFS method. While the results of Table 10 show that the accuracy of PSOFS method is only 0.5 times higher than GCACO method. Moreover, the results show that for very large datasets such as Arcene the execution time of the HGAFS and ACOFS methods are lower than the proposed method. This is due to the fact that these methods are executed in two steps. In the first step they applied a filter based method (such as Information gain or Gini-index) to rank the features independently and then a small size subset (i.e. a subset with only 100 top ranked features) are selected to be incorporated a wrapper based method in the second step. While in this case the proposed method is applied over all of the original features. On the other hand Table 10 results show that in this case the classification accuracy of the proposed method is much higher than the HGAFS and ACOFS methods.",
            "id": "knowledge-based_systems/S0950705115001458.txt"
          }
        },
        {
          "text": {
            "text": "The rest of this paper is organized as follows: Section 2 describes the GMDH-based learning methods used for building optimal network models that capture the input\u2013output relationships in the training set. Section 3 defines the performance measures used for evaluating and comparing the effectiveness of the proposed methods. Section 4 describes the spambase dataset used. Section 5 describes the various experiments conducted using abductive machine learning. Section 6 compares the performance of the proposed methods with three other existing techniques. Finally, Section 7 concludes the paper and proposes directions for future work.",
            "id": "applied_soft_computing/S1568494609002610.txt"
          }
        }
      ],
      "questions": [
        {
          "answers": [
            "Spam detection",
            "classification"
          ],
          "question": "tasks_annotated_for (Spambase Dataset: dataset, ?:task)",
          "candidates": [
            {
              "text": "network models"
            },
            {
              "text": "process"
            },
            {
              "text": "imbalance"
            },
            {
              "text": "emails"
            },
            {
              "text": "cost"
            },
            {
              "text": "dtl"
            },
            {
              "text": "precision"
            },
            {
              "text": "sample"
            },
            {
              "text": "smote"
            },
            {
              "text": "performance"
            },
            {
              "text": "product"
            },
            {
              "text": "uci"
            },
            {
              "text": "groups"
            },
            {
              "text": "authors"
            },
            {
              "text": "local optimum"
            },
            {
              "text": "data reduction"
            },
            {
              "text": "cpm"
            },
            {
              "text": "importance"
            },
            {
              "text": "cutoff"
            },
            {
              "text": "threshold"
            },
            {
              "text": "g-means"
            },
            {
              "text": "method"
            },
            {
              "text": "input"
            },
            {
              "text": "classification accuracy"
            },
            {
              "text": "attribute"
            },
            {
              "text": "experiments"
            },
            {
              "text": "sensitivity"
            },
            {
              "text": "measures"
            },
            {
              "text": "uncertainty"
            },
            {
              "text": "sr"
            },
            {
              "text": "change"
            },
            {
              "text": "development"
            },
            {
              "text": "schemes"
            },
            {
              "text": "volume"
            },
            {
              "text": "mlp"
            },
            {
              "text": "ensembles"
            },
            {
              "text": "significance"
            },
            {
              "text": "approach"
            },
            {
              "text": "keywords"
            },
            {
              "text": "model evaluation"
            },
            {
              "text": "outcome"
            },
            {
              "text": "nb"
            },
            {
              "text": "stage"
            },
            {
              "text": "roc curves"
            },
            {
              "text": "learning"
            },
            {
              "text": "ratio"
            },
            {
              "text": "filter"
            },
            {
              "text": "statistic"
            },
            {
              "text": "column"
            },
            {
              "text": "bisection method"
            },
            {
              "text": "technique"
            },
            {
              "text": "training"
            },
            {
              "text": "results"
            },
            {
              "text": "effectiveness"
            },
            {
              "text": "research"
            },
            {
              "text": "values"
            },
            {
              "text": "representative"
            },
            {
              "text": "agreement"
            },
            {
              "text": "candidate"
            },
            {
              "text": "performance measures"
            },
            {
              "text": "exclusion"
            },
            {
              "text": "fpr"
            },
            {
              "text": "keyword"
            },
            {
              "text": "techniques"
            },
            {
              "text": "difference"
            },
            {
              "text": "test sets"
            },
            {
              "text": "class"
            },
            {
              "text": "features"
            },
            {
              "text": "email"
            },
            {
              "text": "number"
            },
            {
              "text": "algorithm"
            },
            {
              "text": "confidence"
            },
            {
              "text": "classifiers"
            },
            {
              "text": "experiment"
            },
            {
              "text": "roc"
            },
            {
              "text": "labs"
            },
            {
              "text": "aspect"
            },
            {
              "text": "model development"
            },
            {
              "text": "sequences"
            },
            {
              "text": "population"
            },
            {
              "text": "predictors"
            },
            {
              "text": "frequency"
            },
            {
              "text": "information gain"
            },
            {
              "text": "estimation"
            },
            {
              "text": "selection"
            },
            {
              "text": "classification performance"
            },
            {
              "text": "learning algorithm"
            },
            {
              "text": "feature"
            },
            {
              "text": "classification"
            },
            {
              "text": "bisection"
            },
            {
              "text": "interval"
            },
            {
              "text": "roc analysis"
            },
            {
              "text": "missing values"
            },
            {
              "text": "improvements"
            },
            {
              "text": "learning methods"
            },
            {
              "text": "models"
            },
            {
              "text": "synthesis"
            },
            {
              "text": "correlation"
            },
            {
              "text": "operator"
            },
            {
              "text": "section"
            },
            {
              "text": "magnitude"
            },
            {
              "text": "spam"
            },
            {
              "text": "software"
            },
            {
              "text": "network"
            },
            {
              "text": "sp"
            },
            {
              "text": "message"
            },
            {
              "text": "comparison"
            },
            {
              "text": "insight"
            },
            {
              "text": "definitions"
            },
            {
              "text": "simpler"
            },
            {
              "text": "empirical likelihood"
            },
            {
              "text": "change detection"
            },
            {
              "text": "standard deviation"
            },
            {
              "text": "topic"
            },
            {
              "text": "networks"
            },
            {
              "text": "samples"
            },
            {
              "text": "binary classification"
            },
            {
              "text": "receiver"
            },
            {
              "text": "errors"
            },
            {
              "text": "breast"
            },
            {
              "text": "cis"
            },
            {
              "text": "parameters"
            },
            {
              "text": "proposal"
            },
            {
              "text": "validation"
            },
            {
              "text": "balances"
            },
            {
              "text": "machine"
            },
            {
              "text": "eq"
            },
            {
              "text": "model"
            },
            {
              "text": "application"
            },
            {
              "text": "f-measure"
            },
            {
              "text": "percentage"
            },
            {
              "text": "estimate"
            },
            {
              "text": "ranking"
            },
            {
              "text": "diabetes"
            },
            {
              "text": "accuracy"
            },
            {
              "text": "result"
            },
            {
              "text": "paradigms"
            },
            {
              "text": "wrapper"
            },
            {
              "text": "ionosphere"
            },
            {
              "text": "likelihood"
            },
            {
              "text": "objective function"
            },
            {
              "text": "test"
            },
            {
              "text": "analysis"
            },
            {
              "text": "neural networks"
            },
            {
              "text": "kernel"
            },
            {
              "text": "researcher"
            },
            {
              "text": "improvement"
            },
            {
              "text": "fm"
            },
            {
              "text": "terms"
            },
            {
              "text": "datasets"
            },
            {
              "text": "hand"
            },
            {
              "text": "character"
            },
            {
              "text": "power"
            },
            {
              "text": "structures"
            },
            {
              "text": "order"
            },
            {
              "text": "ocsvm"
            },
            {
              "text": "researchers"
            },
            {
              "text": "abductive network"
            },
            {
              "text": "matrix"
            },
            {
              "text": "rankings"
            },
            {
              "text": "paper"
            },
            {
              "text": "characteristics"
            },
            {
              "text": "function"
            },
            {
              "text": "value"
            },
            {
              "text": "positions"
            },
            {
              "text": "distribution function"
            },
            {
              "text": "claim"
            },
            {
              "text": "execution"
            },
            {
              "text": "probabilistic neural networks"
            },
            {
              "text": "deviation"
            },
            {
              "text": "size"
            },
            {
              "text": "work"
            },
            {
              "text": "attributes"
            },
            {
              "text": "use"
            },
            {
              "text": "model complexity"
            },
            {
              "text": "step"
            },
            {
              "text": "edge"
            },
            {
              "text": "detection"
            },
            {
              "text": "perceptron"
            },
            {
              "text": "distribution"
            },
            {
              "text": "thresholds"
            },
            {
              "text": "information"
            },
            {
              "text": "cardiotocography"
            },
            {
              "text": "instances"
            },
            {
              "text": "position"
            },
            {
              "text": "presence"
            },
            {
              "text": "parameter"
            },
            {
              "text": "architectures"
            },
            {
              "text": "feature selection"
            },
            {
              "text": "mean"
            },
            {
              "text": "instance"
            },
            {
              "text": "populations"
            },
            {
              "text": "cross validation"
            },
            {
              "text": "dataset"
            },
            {
              "text": "k"
            },
            {
              "text": "case"
            },
            {
              "text": "measure"
            },
            {
              "text": "data"
            },
            {
              "text": "iterations"
            },
            {
              "text": "words"
            },
            {
              "text": "variations"
            },
            {
              "text": "gain"
            },
            {
              "text": "na\u00efve bayesian classifier"
            },
            {
              "text": "average"
            },
            {
              "text": "levels"
            },
            {
              "text": "optimum"
            },
            {
              "text": "negative"
            },
            {
              "text": "imputation"
            },
            {
              "text": "robustness"
            },
            {
              "text": "subset"
            },
            {
              "text": "methods"
            },
            {
              "text": "multilayer perceptron"
            },
            {
              "text": "item"
            },
            {
              "text": "relationships"
            },
            {
              "text": "chart"
            },
            {
              "text": "affiliation"
            },
            {
              "text": "sets"
            },
            {
              "text": "confusion"
            },
            {
              "text": "execution time"
            },
            {
              "text": "fn"
            },
            {
              "text": "subsets"
            },
            {
              "text": "boundaries"
            },
            {
              "text": "confidence interval"
            },
            {
              "text": "characters"
            },
            {
              "text": "fig"
            },
            {
              "text": "means"
            },
            {
              "text": "fp"
            },
            {
              "text": "time"
            },
            {
              "text": "end"
            },
            {
              "text": "training set"
            },
            {
              "text": "contrast"
            },
            {
              "text": "literature"
            },
            {
              "text": "comparisons"
            },
            {
              "text": "indication"
            },
            {
              "text": "list"
            },
            {
              "text": "heart"
            },
            {
              "text": "modeling"
            },
            {
              "text": "acc"
            },
            {
              "text": "fnr"
            },
            {
              "text": "set"
            },
            {
              "text": "database"
            },
            {
              "text": "reduction"
            },
            {
              "text": "complexity"
            },
            {
              "text": "differences"
            },
            {
              "text": "operation"
            },
            {
              "text": "confusion matrix"
            },
            {
              "text": "machine learning"
            },
            {
              "text": "filtering"
            },
            {
              "text": "large datasets"
            },
            {
              "text": "relative importance"
            },
            {
              "text": "messages"
            },
            {
              "text": "name"
            },
            {
              "text": "rest"
            },
            {
              "text": "row"
            },
            {
              "text": "classifier"
            },
            {
              "text": "account"
            },
            {
              "text": "lists"
            },
            {
              "text": "curves"
            },
            {
              "text": "decision"
            },
            {
              "text": "spam detection"
            },
            {
              "text": "rates"
            },
            {
              "text": "bar"
            },
            {
              "text": "recall"
            },
            {
              "text": "evaluation"
            },
            {
              "text": "specificity"
            },
            {
              "text": "spam filtering"
            },
            {
              "text": "rate"
            },
            {
              "text": "z"
            }
          ]
        }
      ]
    }
  ],
  "meta": "scienceQA.json"
}