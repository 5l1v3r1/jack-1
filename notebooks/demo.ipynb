{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jack the Reader: A Machine Reading Framework\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Note:** these commands need to be run in terminal from the root of Jack.\n",
    "```\n",
    "sh data/GloVe/download.sh\n",
    "wget -O fastqa.zip https://www.dropbox.com/s/lftgh01zi60r9jv/fastqa.zip?dl=1\n",
    "wget http://data.neuralnoise.com/jack-demo/esim.tar.gz\n",
    "wget http://data.neuralnoise.com/jack-demo/bidaf.tar.gz\n",
    "wget http://data.neuralnoise.com/jack2/nli/dam.tar.gz ###\n",
    "unzip fastqa.zip\n",
    "tar -xvzf bidaf.tar.gz\n",
    "tar -xvzf esim.tar.gz\n",
    "tar -xvzf dam.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir('..')    # change dir to Jack root\n",
    "from jack import readers\n",
    "from jack.core import QASetting\n",
    "from jack.io.load import load_jack\n",
    "from notebooks.prettyprint import QAPrettyPrint, print_nli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering (QA)\n",
    "\n",
    "Loading ready-to-use pretrained extractive QA models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Restoring parameters from ./bidaf/model_module\n",
      "INFO:tensorflow:Restoring parameters from ./fastqa/model_module\n"
     ]
    }
   ],
   "source": [
    "bidaf_reader = readers.reader_from_file(\"./bidaf\")\n",
    "fastqa_reader = readers.reader_from_file(\"./fastqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. \n",
    "At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\"\"\n",
    "question = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Tom was visiting his family in the Rocky Mountains with his two little dogs. They had a welcome BBQ and ate all of the chocolates he brought. Tom has a friend whose family lives in Vermont.\"\"\"\n",
    "question = \"Where do Tom's parents live?\"\n",
    "#question = \"What did the family eat for desert?\"\n",
    "#question = \"How many dogs does Tom have?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"France won against Croatia 4 : 2 in yesterday's World Cup final. \"\n",
    "question = \"What was the final score of the World Cup final match?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Format: the `QASetting` data structure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_setting = QASetting(question=question, support=[paragraph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling the Reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the final score of the World Cup final match?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "France won against <span style=\"background-color: #ff00ff; color: white\">Croatia 4 : 2</span> in yesterday's World Cup final. "
      ],
      "text/plain": [
       "<notebooks.prettyprint.QAPrettyPrint at 0x2d5b90ef0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = bidaf_reader([qa_setting])\n",
    "print(question)\n",
    "QAPrettyPrint(paragraph, answers[0][0].span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top-k Predictions with, Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Answer score: 0.94161 \t Saint Bernadette Soubirous\n",
      "Top 2 Answer score: 0.94161 \t Saint Bernadette Soubirous\n",
      "Top 3 Answer score: 0.05784 \t Bernadette Soubirous\n",
      "Top 4 Answer score: 0.00042 \t Soubirous\n",
      "Top 5 Answer score: 0.00009 \t Saint Bernadette\n"
     ]
    }
   ],
   "source": [
    "fastqa_reader.model_module.set_topk(k=5)\n",
    "answers = fastqa_reader([qa_setting])\n",
    "for i, a in enumerate(answers[0]):\n",
    "    print(\"Top %d Answer score: %.5f \\t %s\" % (i+1, a.score, a.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Inference (NLI)\n",
    "\n",
    "Loading ready-to-use pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./esim/model_module\n"
     ]
    }
   ],
   "source": [
    "esim_reader = readers.reader_from_file(\"./esim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"A wedding party is taking pictures.\"\n",
    "hypothesis1 = \"A group of people is celebrating.\"\n",
    "hypothesis2 = \"A rock band is giving a concert.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"France lost to Croatia 0:5 in yesterday's World Cup final.\"\n",
    "hypothesis1 = \"Croatia won against France in the World Cup finals.\"\n",
    "hypothesis2 = \"France won against Croatia in the World Cup finals\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same `QASetting` input data structure is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_setting1 = QASetting(question=hypothesis1, support=[premise])\n",
    "snli_setting2 = QASetting(question=hypothesis2, support=[premise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate predictions by calling the reader with these inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wedding party is taking pictures.\t--(entailment)-->\tA group of people is celebrating.\n",
      "A wedding party is taking pictures.\t--(contradiction)-->\tA rock band is giving a concert.\n"
     ]
    }
   ],
   "source": [
    "prediction = esim_reader([snli_setting1])\n",
    "print_nli(premise, hypothesis1, prediction[0][0].text)\n",
    "\n",
    "prediction = esim_reader([snli_setting2])\n",
    "print_nli(premise, hypothesis2, prediction[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NLI case, the answer is a label among {_\"entailment\"_, _\"neutral\"_, _\"contradiction\"_}.\n",
    "\n",
    "##### Prediction score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988599\n"
     ]
    }
   ],
   "source": [
    "print(prediction[0][0].score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
