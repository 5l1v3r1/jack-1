{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from quebap.projects.modelF.structs import FrozenIdentifier\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def tok(string):\n",
    "    return string.split(\" \")\n",
    "\n",
    "statements = [tok(w) for w in [\n",
    "    \"eagle can fly\",\n",
    "    \"eagle is a bird\",\n",
    "    \"bird can fly\",\n",
    "    \"duck can fly\"\n",
    "]]\n",
    "\n",
    "questions = [tok(w) for w in [\n",
    "    \"duck can _\"\n",
    "]]\n",
    "\n",
    "WHITESPACE = '[WS]'\n",
    "words = { word for statement in statements + questions for word in statement }\n",
    "words.add(WHITESPACE)\n",
    "vocab = FrozenIdentifier(words)\n",
    "embeddings = tf.diag(tf.ones(len(vocab)))\n",
    "whitespace_repr = tf.gather(embeddings, vocab[WHITESPACE]) # [repr_dim]\n",
    "cost_for_remainder = 1 - tf.gather(embeddings, vocab['_']) - whitespace_repr\n",
    "\n",
    "def match_and_extract(statement, question):\n",
    "    # statement: [batch_size, max_length, repr_dim]\n",
    "    # questions: [batch_size, max_length, repr_dim]\n",
    "    # result: [batch_size, max_length, repr_dim], [batch_size]\n",
    "    # for each batch, for each token, check if both tokens are identical. If so use WHITESPACE, else keep only statement\n",
    "    logits = tf.reduce_sum(statement * question, 2) #[batch_size, max_length]\n",
    "    match_scores = tf.maximum(tf.minimum(logits,1.0),0.0) # alternatively: sigmoids, look for distance etc.\n",
    "    \n",
    "    expanded_match_scores = tf.expand_dims(match_scores, 2)\n",
    "    whitespace_where_match = whitespace_repr * expanded_match_scores # [batch_size, max_length, repr_dim]\n",
    "    statement_elsewhere = (1.0 - expanded_match_scores) * statement\n",
    "    extraction = whitespace_where_match + statement_elsewhere\n",
    "    question_elsewhere = (1.0 - expanded_match_scores) * question #[batch_size, max_length, repr_dim]\n",
    "    question_leftover = whitespace_where_match + question_elsewhere\n",
    "    costs = tf.reduce_sum(question_leftover * cost_for_remainder, [1,2])\n",
    "    return extraction, costs,question_leftover\n",
    "#     return statement_elsewhere\n",
    "\n",
    "def repr_text_batch(texts):\n",
    "    max_length = max([len(text) for text in texts])\n",
    "    result = [[vocab[text[i]] if i < len(text) else vocab[WHITESPACE] for i in range(0, max_length)] for text in texts]\n",
    "    return result\n",
    "\n",
    "def embed_statements(statements):\n",
    "    return tf.gather(embeddings, statements)\n",
    "        \n",
    "statement_placeholder = tf.placeholder(tf.int32,(None,None))\n",
    "question_placeholder = tf.placeholder(tf.int32, (None,None))\n",
    "\n",
    "extract_result = match_and_extract(embed_statements(statement_placeholder), embed_statements(question_placeholder))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(whitespace_repr)\n",
    "\n",
    "def to_feed_dict(statements, questions):    \n",
    "    text_repr = repr_text_batch(statements + questions)\n",
    "    statement_repr, question_repr = text_repr[:len(statements)], text_repr[len(statements):]\n",
    "    return {statement_placeholder:statement_repr, question_placeholder: question_repr}\n",
    "\n",
    "sess.run((extract_result,whitespace_repr,cost_for_remainder), \n",
    "         feed_dict=to_feed_dict(statements[3:4], questions[:1]))\n",
    "\n",
    "sess.run(embeddings[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.5,  0. , -1. ,  0. ,  2. , -1. ],\n",
       "        [ 0. ,  4. , -1. ,  0. ,  0. ,  0. ]], dtype=float32),\n",
       " array([[ 1.,  0.],\n",
       "        [ 0.,  1.]], dtype=float32))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import numpy as np\n",
    "\n",
    "class CompactifyCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \n",
    "    def __init__(self, max_compact_length, input_dim, zero_result=None):\n",
    "        # zero_result: [batch_size, max_compact_length, input_dim]\n",
    "        self._max_compact_length = max_compact_length\n",
    "        self._input_dim = input_dim\n",
    "        self._shift_matrix = np.zeros((max_compact_length,max_compact_length))\n",
    "        for i in range(0, max_compact_length):\n",
    "            self._shift_matrix[i,(i+1)%max_compact_length] = 1.0\n",
    "        \n",
    "        self._zero_result = zero_result\n",
    "        \n",
    "    \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        result, counter = state\n",
    "        result_matrix = tf.reshape(result, (-1, self._max_compact_length,self._input_dim))\n",
    "        # result matrix # [batch_size, input_dim, max_compact_length]\n",
    "        # counter [batch_size, max_compact_length]\n",
    "        # inputs [batch_size, input_dim + 1]\n",
    "        # mask = [batch_size, 1]\n",
    "        mask = inputs[:,0:1] # [batch_size, 1] \n",
    "        input_tokens = inputs[:,1:] # [batch_size, input_dim]\n",
    "        \n",
    "        input_at_counter = tf.expand_dims(input_tokens, 1) * tf.expand_dims(counter, 2) \n",
    "        # [batch_size, max_compact_length, input_dim]\n",
    "        expanded_mask = tf.expand_dims(mask,1)\n",
    "        \n",
    "#         new_result_matrix = mask * (result_matrix + input_at_counter) + (1.0 - mask) * result_matrix \n",
    "        new_result_matrix = expanded_mask * (result_matrix + input_at_counter) + (1.0 - expanded_mask) * result_matrix\n",
    "        new_counter = mask * tf.matmul(counter,tf.constant(self._shift_matrix,dtype=tf.float32))+ (1.0 - mask) * counter\n",
    "#         new_counter = tf.matmul(counter,tf.constant(self._shift_matrix,dtype=tf.float32)) \n",
    "#         new_counter = counter\n",
    "        \n",
    "        new_result = tf.reshape(new_result_matrix, tf.shape(result))\n",
    "#         new_result = tf.reshape(input_at_counter, tf.shape(result))\n",
    "        \n",
    "        return new_result, (new_result, new_counter)\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        zero_result = tf.zeros((batch_size,self._input_dim * self._max_compact_length)) \\\n",
    "            if self._zero_result is None else tf.reshape(self._zero_result, (batch_size,self._input_dim * self._max_compact_length))\n",
    "        zero_counter = tf.concat(1, [tf.ones((batch_size,1)), tf.zeros((batch_size,self._max_compact_length-1))])\n",
    "        return (zero_result, zero_counter)\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (self._input_dim * self._max_compact_length, self._max_compact_length)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._input_dim * self._max_compact_length\n",
    "    \n",
    "cell = CompactifyCell(2,3)\n",
    "\n",
    "inputs = tf.constant([\n",
    "    [[1., 0.5, 0, -1], [1., 0, 2, -1]],\n",
    "    [[0., 3, 0, -1], [1., 0, 4, -1]]\n",
    "])\n",
    "\n",
    "outputs, (result,counter) = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=(2,2),\n",
    "        inputs=inputs)\n",
    "\n",
    "sess.run((result,counter))\n",
    "# sess.run(inputs[:,0,0:1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
