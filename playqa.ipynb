{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from quebap.projects.modelF.structs import FrozenIdentifier\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a asd'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def tok(string):\n",
    "    return string.split(\" \")\n",
    "\n",
    "statements = [tok(w) for w in [\n",
    "    \"eagle can fly\",\n",
    "    \"eagle => bird\",\n",
    "    \"bird can fly\",\n",
    "    \"duck can fly\"\n",
    "]]\n",
    "\n",
    "questions = [tok(w) for w in [\n",
    "    \"duck can _\"\n",
    "]]\n",
    "\n",
    "WHITESPACE = '[WS]'\n",
    "words = { word for statement in statements + questions for word in statement }\n",
    "words.add(WHITESPACE)\n",
    "vocab = FrozenIdentifier(words)\n",
    "embeddings = tf.diag(tf.ones(len(vocab)))\n",
    "whitespace_repr = tf.gather(embeddings, vocab[WHITESPACE]) # [repr_dim]\n",
    "cost_for_remainder = 1 - tf.gather(embeddings, vocab['_']) - whitespace_repr\n",
    "\n",
    "def match_and_extract(statement, question):\n",
    "    # statement: [batch_size, max_length, repr_dim]\n",
    "    # questions: [batch_size, max_length, repr_dim]\n",
    "    # result: [batch_size, max_length, repr_dim], [batch_size]\n",
    "    # for each batch, for each token, check if both tokens are identical. If so use WHITESPACE, else keep only statement\n",
    "    logits = tf.reduce_sum(statement * question, 2) #[batch_size, max_length]\n",
    "    match_scores = tf.maximum(tf.minimum(logits,1.0),0.0) # alternatively: sigmoids, look for distance etc.\n",
    "    \n",
    "    expanded_match_scores = tf.expand_dims(match_scores, 2)\n",
    "    whitespace_where_match = whitespace_repr * expanded_match_scores # [batch_size, max_length, repr_dim]\n",
    "    statement_elsewhere = (1.0 - expanded_match_scores) * statement\n",
    "    extraction = whitespace_where_match + statement_elsewhere\n",
    "    question_elsewhere = (1.0 - expanded_match_scores) * question #[batch_size, max_length, repr_dim]\n",
    "    question_leftover = whitespace_where_match + question_elsewhere\n",
    "    costs = tf.reduce_sum(question_leftover * cost_for_remainder, [1,2])\n",
    "    return extraction, costs,question_leftover\n",
    "#     return statement_elsewhere\n",
    "\n",
    "def repr_text_batch(texts):\n",
    "    max_length = max([len(text) for text in texts])\n",
    "    result = [[vocab[text[i]] if i < len(text) else vocab[WHITESPACE] for i in range(0, max_length)] for text in texts]\n",
    "    return result\n",
    "\n",
    "def embed_statements(statements):\n",
    "    return tf.gather(embeddings, statements)\n",
    "        \n",
    "statement_placeholder = tf.placeholder(tf.int32,(None,None))\n",
    "question_placeholder = tf.placeholder(tf.int32, (None,None))\n",
    "\n",
    "extract_result = match_and_extract(embed_statements(statement_placeholder), embed_statements(question_placeholder))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(whitespace_repr)\n",
    "\n",
    "def to_feed_dict(statements, questions):    \n",
    "    text_repr = repr_text_batch(statements + questions)\n",
    "    statement_repr, question_repr = text_repr[:len(statements)], text_repr[len(statements):]\n",
    "    return {statement_placeholder:statement_repr, question_placeholder: question_repr}\n",
    "\n",
    "sess.run((extract_result,whitespace_repr,cost_for_remainder), \n",
    "         feed_dict=to_feed_dict(statements[3:4], questions[:1]))\n",
    "\n",
    "sess.run(embeddings[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['bird', 'can', '_']],\n",
       " array([ 5.], dtype=float32),\n",
       " array([ 3.], dtype=float32))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple extract_or_translate unit\n",
    "import quebap.projects.playqa.model as model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def tok(string):\n",
    "    return string.split(\" \")\n",
    "\n",
    "statements = [tok(w) for w in [\n",
    "    \"eagle can fly\",\n",
    "    \"eagle isa bird\",\n",
    "    \"bird can fly\",\n",
    "    \"duck can fly\"\n",
    "]]\n",
    "\n",
    "questions = [tok(w) for w in [\n",
    "    \"eagle can _\"\n",
    "]]\n",
    "\n",
    "WHITESPACE = '[WS]'\n",
    "words = { word for statement in statements + questions for word in statement }\n",
    "words.add(WHITESPACE)\n",
    "vocab = FrozenIdentifier(words)\n",
    "embeddings = tf.diag(tf.ones(len(vocab)))\n",
    "whitespace_repr = tf.gather(embeddings, vocab[WHITESPACE]) # [repr_dim]\n",
    "cost_for_remainder = 1 - tf.gather(embeddings, vocab['_']) - whitespace_repr\n",
    "\n",
    "wh_token = tf.gather(embeddings, vocab['_']) # [repr_dim]\n",
    "translate_token = tf.gather(embeddings, vocab['isa']) # [repr_dim]\n",
    "\n",
    "softmax_slope = 2.0\n",
    "\n",
    "def simple_extract_or_translate(questions, statements):\n",
    "    # questions: [batch_size, length, repr_dim]\n",
    "    # statements: [batch_size, length, repr_dim]\n",
    "    # return: extraction result,extraction score, translation result, translation score\n",
    "    # calculate total token-by-token match score\n",
    "    match_score = tf.reduce_sum(questions * statements, [1,2]) # [batch_size]\n",
    "    \n",
    "    # find WH token\n",
    "    wh_scores = tf.reduce_sum(questions * wh_token,2) # [batch_size, length]\n",
    "    wh_probs = tf.nn.softmax(wh_scores) # [batch_size, length]\n",
    "    \n",
    "    # extract answer token\n",
    "    answer_token = tf.reduce_sum(statements * tf.expand_dims(wh_probs, 2), 1) # [batch_size, repr_dim]\n",
    "    \n",
    "    # answer should be a sequence, append zeros\n",
    "    padding = tf.zeros(tf.shape(statements) - [0, 1, 0])\n",
    "    padding_ws = tf.tile(tf.expand_dims(tf.expand_dims(whitespace_repr,0),0), \n",
    "                         tf.shape(statements) * [1,1,0] + [0, -1, 1])\n",
    "    answer = tf.concat(1, [tf.expand_dims(answer_token,1), padding_ws])\n",
    "    \n",
    "    # check for TR token at index 1\n",
    "    tr_match_score = tf.reduce_sum(statements[:,1,:] * translate_token,1) # [batch_size]\n",
    "    # A => Y\n",
    "    lhs = statements[:,0:1,:] # [batch_size, 1, repr_dim]\n",
    "    rhs = statements[:,2:3,:] \n",
    "    \n",
    "    # find best match with rhs in question\n",
    "    lhs_match_scores = tf.reduce_sum(questions * lhs, 2) # [batch_size, length]\n",
    "    lhs_prob = tf.nn.softmax(lhs_match_scores) # [batch_size, length]\n",
    "    lhs_prob_expanded = tf.expand_dims(lhs_prob, 2) # [batch_size, length, 1]\n",
    "    \n",
    "    # then replace with lhs\n",
    "    replacement = rhs * lhs_prob_expanded \n",
    "    to_remove = lhs * lhs_prob_expanded\n",
    "    new_questions = questions - to_remove + replacement\n",
    "    \n",
    "    # translation score\n",
    "    tr_score = tf.reduce_sum(lhs_match_scores, 1) + tr_match_score\n",
    "    \n",
    "    # X Y  _ & \n",
    "    # A => Y \n",
    "    # X A  _ \n",
    "    # find tokens in statement that are not the TR token and aren't matched in question\n",
    "    # A * *\n",
    "    # find tokens in questions that are matched with the statement (assume there is a single one so use softmax)\n",
    "    # * Y * \n",
    "    # remove found token from question and replace with token from statement\n",
    "    # X A _\n",
    "    \n",
    "    return answer, match_score, new_questions, tr_score\n",
    "\n",
    "def repr_text_batch(texts):\n",
    "    max_length = max([len(text) for text in texts])\n",
    "    result = [[vocab[text[i]] if i < len(text) else vocab[WHITESPACE] for i in range(0, max_length)] for text in texts]\n",
    "    return result\n",
    "\n",
    "def embed_statements(statements):\n",
    "    return tf.gather(embeddings, statements)\n",
    "\n",
    "def to_feed_dict(statements, questions):    \n",
    "    text_repr = repr_text_batch(statements + questions)\n",
    "    statement_repr, question_repr = text_repr[:len(statements)], text_repr[len(statements):]\n",
    "    return {statement_placeholder:statement_repr, question_placeholder: question_repr}\n",
    "\n",
    "def match_all(kb, questions):\n",
    "    # kb: [kb_size, max_length, repr_dim]\n",
    "    # questions [batch_size, max_length, repr_dim]\n",
    "    # turn kb into [batch_size, kb_size, max_length, repr_dim]\n",
    "    expanded_kb = tf.expand_dims(kb,0) \n",
    "    expanded_questions = tf.expand_dims(questions, 1)\n",
    "    tiled_kb = tf.tile(expanded_kb, tf.shape(expanded_questions) * [1, 0, 0, 0] + [0, 1, 1, 1])\n",
    "    tiled_questions = tf.tile(expanded_questions, tf.shape(expanded_kb) * [0, 1, 0, 0] + [1, 0, 1, 1])\n",
    "    \n",
    "    # now flatten \n",
    "    new_dim = tf.shape(kb)[0:1] * tf.shape(questions)[0:1]\n",
    "    new_shape = tf.concat(0, [new_dim, tf.shape(kb)[1:]])\n",
    "    batch_kb_shape = tf.shape(tiled_kb)[0:2]\n",
    "    flat_kb = tf.reshape(tiled_kb, new_shape)\n",
    "    flat_questions = tf.reshape(tiled_questions, new_shape)\n",
    "    \n",
    "    answers, match_scores, new_questions, tr_scores = simple_extract_or_translate(flat_questions, flat_kb)\n",
    "    \n",
    "    # extraction\n",
    "    def aggregate_and_score(answers, match_scores):\n",
    "        answers_reshaped = tf.reshape(answers, tf.shape(tiled_kb)) # [batch_size, kb_size, max_length, repr_dim]\n",
    "        match_scores_reshaped = tf.reshape(match_scores, batch_kb_shape) # [batch_size, kb_size]\n",
    "        match_probs = tf.nn.softmax(match_scores_reshaped * 2.0)\n",
    "        match_probs_expanded = tf.expand_dims(tf.expand_dims(match_probs,2), 3) # [batch_size, kb_size, 1, 1]\n",
    "        weighted_answer = tf.reduce_sum(match_probs_expanded * answers_reshaped, 1)\n",
    "        global_match_score = tf.reduce_sum(match_scores_reshaped, 1)\n",
    "        return weighted_answer, global_match_score\n",
    "        \n",
    "    \n",
    "    global_answer, global_match_score = aggregate_and_score(answers, match_scores)\n",
    "    global_translation, global_translation_score = aggregate_and_score(new_questions, tr_scores)\n",
    "        \n",
    "    return global_answer, global_match_score, global_translation, global_translation_score\n",
    "\n",
    "def inference_steps(kb, questions, num_steps=1):\n",
    "    current_questions = questions\n",
    "    current_result = questions\n",
    "    current_match_score = None\n",
    "    current_translation_score = None\n",
    "    steps = []\n",
    "    for step in range(0, num_steps):\n",
    "        current_result, current_match_score, global_translation, current_translation_score = match_all(kb, questions)\n",
    "        # if global_match_score >> global_translation_score we should never change the question again\n",
    "        prob_translate = tf.sigmoid(1.0 * (current_translation_score - current_match_score + 10.))\n",
    "        current_questions = prob_translate * global_translation + (1.0 - prob_translate) * current_questions\n",
    "#         current_questions = global_translation \n",
    "        steps.append((current_result, current_match_score, current_questions, current_translation_score))\n",
    "        \n",
    "    return steps\n",
    "        \n",
    "\n",
    "def decode(statements):\n",
    "    compare_all = tf.reduce_sum(tf.expand_dims(statements,2) * embeddings,3)\n",
    "    top_k = tf.nn.top_k(compare_all)\n",
    "\n",
    "    values, indices = sess.run(top_k)\n",
    "\n",
    "    results = []\n",
    "    for seq in indices:\n",
    "        sentence = []\n",
    "        for token in seq:\n",
    "            word = vocab.key_by_id(token[0])\n",
    "            if word != WHITESPACE:\n",
    "                sentence.append(vocab.key_by_id(token[0]))\n",
    "        results.append(sentence)\n",
    "    return results\n",
    "        \n",
    "statement_placeholder = tf.placeholder(tf.int32,(None,None))\n",
    "question_placeholder = tf.placeholder(tf.int32, (None,None))\n",
    "\n",
    "result = simple_extract_or_translate(embed_statements(question_placeholder), \n",
    "                                           embed_statements(statement_placeholder))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "e_result, e_score, t_result, t_score = sess.run(result, feed_dict=to_feed_dict(statements[1:2],questions[:1]))\n",
    "\n",
    "decode(t_result)\n",
    "# e_score\n",
    "# t_result\n",
    "\n",
    "all_answer, all_match_score, all_translation, all_translation_score = sess.run(\n",
    "    inference_steps(embed_statements(statement_placeholder), \n",
    "                   embed_statements(question_placeholder))[-1],\n",
    "        feed_dict=to_feed_dict(statements[0:4],questions[:1]))\n",
    "   \n",
    "decode(all_translation), all_match_score, all_translation_score\n",
    "# all_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fly']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.gather(embeddings, vocab['[WS]'])) # [repr_dim]\n",
    "\n",
    "def decode(statements):\n",
    "    compare_all = tf.reduce_sum(tf.expand_dims(statements,2) * embeddings,3)\n",
    "    top_k = tf.nn.top_k(compare_all)\n",
    "\n",
    "    values, indices = sess.run(top_k)\n",
    "\n",
    "    results = []\n",
    "    for seq in indices:\n",
    "        sentence = []\n",
    "        for token in seq:\n",
    "            word = vocab.key_by_id(token[0])\n",
    "            if word != WHITESPACE:\n",
    "                sentence.append(vocab.key_by_id(token[0]))\n",
    "        results.append(sentence)\n",
    "    return results\n",
    "\n",
    "decode(e_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.5,  0. , -1. ,  0. ,  2. , -1. ],\n",
       "        [ 0. ,  4. , -1. ,  0. ,  0. ,  0. ]], dtype=float32),\n",
       " array([[ 1.,  0.],\n",
       "        [ 0.,  1.]], dtype=float32))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import numpy as np\n",
    "import quebap.projects.playqa.util as pqutil\n",
    "    \n",
    "cell = pqutil.CompactifyCell(2,3)\n",
    "\n",
    "inputs = tf.constant([\n",
    "    [[0.5, 0, -1], [0, 2, -1]],\n",
    "    [[3, 0, -1], [0, 4, -1]]\n",
    "])\n",
    "\n",
    "masks = tf.constant([[1.,1],[0,1]])\n",
    "\n",
    "outputs, (result,counter) = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        dtype=tf.float32,\n",
    "        sequence_length=(2,2),\n",
    "        inputs=playqa.to_inputs(inputs,masks))\n",
    "\n",
    "sess.run((result,counter))\n",
    "# sess.run(inputs[:,0,0:1])\n",
    "# mask = tf.expand_dims(tf.constant([[1.,1],[1, 0]]),2) # [batch,size, max_length]\n",
    "# sess.run(tf.concat(2, [mask, inputs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}