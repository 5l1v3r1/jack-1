description: >
  Our optimized Jack NLI reader which tries to keep up with SotA while being as resource friendly as possible. This
  model is a bit more lightweight than jack_qa.

parent_config: './conf/nli/modular_nli.yaml'

name: 'jack_nli_reader'

# fixed experiment seed
seed: 1337

# where to store the reader
save_dir: './jack_nli_reader'

with_char_embeddings: True


# To be fast we have to restrict the use of RNNs as much as possible and use convolutions instead
model:
  encoder_layer:
  # Shared Embedding Processing
  # Support
  - input: ['hypothesis', 'char_hypothesis']
    output: 'hypothesis'
    module: 'concat'
  - input: 'hypothesis'
    name: 'embedding_highway'
    module: 'highway'
  - input: 'hypothesis'
    output: 'emb_hypothesis'
    name: 'embedding_projection'
    module: 'dense'
    activation: 'tanh'
    dropout: True
  # Question
  - input: ['premise', 'char_premise']
    output: 'premise'
    module: 'concat'
  - input: 'premise'
    name: 'embedding_highway'  # use same network as hypothesis
    module: 'highway'
  - input: 'premise'
    output: 'emb_premise'
    name: 'embedding_projection'  # use same network as hypothesis
    module: 'dense'
    activation: 'tanh'
    dropout: True

    # Shared Contextual Encoding
  - input: 'emb_hypothesis'
    output: 'hypothesis'
    module: 'conv_glu'
    conv_width: 5
    num_layers: 2
    name: 'contextual_encoding'
    dropout: True
  - input: 'emb_premise'
    output: 'premise'
    module: 'conv_glu'
    conv_width: 5
    num_layers: 2
    name: 'contextual_encoding'  # use same network as hypothesis
    dropout: True

  - input: ['emb_premise', 'premise']
    output: 'enc_premise'
    module: 'concat'
  - input: ['emb_hypothesis', 'hypothesis']
    output: 'enc_hypothesis'
    module: 'concat'

  # Attention
  - input: 'enc_hypothesis'
    dependent: 'enc_premise'
    output: 'hypothesis'
    module: 'attention_matching'
    name: 'attn'
    attn_type: 'diagonal_bilinear'
    with_sentinel: True  # we gate the attention with an additional scalar sentinel because what we retrieve might actually not be what we were looking for because (softmax) attn always retrieves something
    scaled: True

  - input: 'hypothesis'
    output: 'hypothesis'
    module: 'dense'
    activation: 'relu'

  # BiLSTM
  - input: 'hypothesis'
    module: 'lstm'  # the only application of a RNN
    with_projection: True
    activation: 'relu'
    dropout: True

  prediction_layer:
    module: 'max_mlp_hypothesis'
