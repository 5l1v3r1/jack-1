<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>quebap.projects.autoread.wikireading package &#8212; quebap  documentation</title>
    
    <link rel="stylesheet" href="static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="static/jquery.js"></script>
    <script type="text/javascript" src="static/underscore.js"></script>
    <script type="text/javascript" src="static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="quebap.projects.clozecompose package" href="quebap.projects.clozecompose.html" />
    <link rel="prev" title="quebap.projects.autoread.wikipedia package" href="quebap.projects.autoread.wikipedia.html" />
   
  <link rel="stylesheet" href="static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="quebap-projects-autoread-wikireading-package">
<h1>quebap.projects.autoread.wikireading package<a class="headerlink" href="#quebap-projects-autoread-wikireading-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-quebap.projects.autoread.wikireading.my_seq2seq">
<span id="quebap-projects-autoread-wikireading-my-seq2seq-module"></span><h2>quebap.projects.autoread.wikireading.my_seq2seq module<a class="headerlink" href="#module-quebap.projects.autoread.wikireading.my_seq2seq" title="Permalink to this headline">¶</a></h2>
<p>Library for creating sequence-to-sequence models in TensorFlow.</p>
<p>Sequence-to-sequence recurrent neural networks can learn complex functions
that map input sequences to output sequences. These models yield very good
results on a number of tasks, such as speech recognition, parsing, machine
translation, or even constructing automated replies to emails.</p>
<ul class="simple">
<li>Full sequence-to-sequence models.<ul>
<li>embedding_rnn_seq2seq: The basic model with input embedding.</li>
<li><dl class="first docutils">
<dt>embedding_attention_seq2seq: Advanced model with input embedding and</dt>
<dd>the neural attention mechanism; recommended for complex tasks.</dd>
</dl>
</li>
</ul>
</li>
<li>Decoders
- rnn_decoder: The basic decoder based on a pure RNN.
- attention_decoder: A decoder that uses the attention mechanism.</li>
<li>Losses.
- sequence_loss: Loss for a sequence model returning average log-perplexity.
- sequence_loss_by_example: As above, but not averaging over all examples.</li>
<li><dl class="first docutils">
<dt>model_with_buckets: A convenience function to create models with bucketing</dt>
<dd>(see the tutorial above for an explanation of why and how to use it).</dd>
</dl>
</li>
</ul>
<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.attention_decoder">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">attention_decoder</code><span class="sig-paren">(</span><em>decoder_inputs</em>, <em>initial_state</em>, <em>attention_states</em>, <em>cell</em>, <em>output_size=None</em>, <em>num_heads=1</em>, <em>loop_function=None</em>, <em>dtype=tf.float32</em>, <em>scope=None</em>, <em>output_projection=None</em>, <em>initial_state_attention=False</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.attention_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>RNN decoder with attention for the sequence-to-sequence model.</p>
<p>In this context &#8220;attention&#8221; means that, during decoding, the RNN can look up
information in the additional tensor attention_states, and it does this by
focusing on a few entries from the tensor. This model has proven to yield
especially good results in a number of sequence-to-sequence tasks. This
implementation is based on <a class="reference external" href="http://arxiv.org/abs/1412.7449">http://arxiv.org/abs/1412.7449</a> (see below for
details). It is recommended for complex sequence-to-sequence tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>decoder_inputs</strong> &#8211; A list of 2D Tensors [batch_size x input_size].</li>
<li><strong>initial_state</strong> &#8211; 2D Tensor [batch_size x cell.state_size].</li>
<li><strong>attention_states</strong> &#8211; 3D Tensor [batch_size x attn_length x attn_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function and size.</li>
<li><strong>output_size</strong> &#8211; Size of the output vectors; if None, we use cell.output_size.</li>
<li><strong>num_heads</strong> &#8211; Number of attention heads that read from attention_states.</li>
<li><strong>loop_function</strong> &#8211; <p>If not None, this function will be applied to i-th output
in order to generate i+1-th input, and decoder_inputs will be ignored,
except for the first element (&#8220;GO&#8221; symbol). This can be used for decoding,
but also for training to emulate <a class="reference external" href="http://arxiv.org/abs/1506.03099">http://arxiv.org/abs/1506.03099</a>.
Signature &#8211; loop_function(prev, i) = next</p>
<blockquote>
<div><ul>
<li>prev is a 2D Tensor of shape [batch_size x output_size],</li>
<li>i is an integer, the step number (when advanced control is needed),</li>
<li>next is a 2D Tensor of shape [batch_size x input_size].</li>
</ul>
</div></blockquote>
</li>
<li><strong>dtype</strong> &#8211; The dtype to use for the RNN initial state (default: tf.float32).</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; default: &#8220;attention_decoder&#8221;.</li>
<li><strong>initial_state_attention</strong> &#8211; If False (default), initial attentions are zero.
If True, initialize the attentions from the initial state and attention
states &#8211; useful when we wish to resume decoding from a previously
stored decoder state and attention states.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors of</dt>
<dd><p class="first">shape [batch_size x output_size]. These represent the generated outputs.
Output i is computed from input i (which is either the i-th element
of decoder_inputs or loop_function(output {i-1}, i)) as follows.
First, we run the cell on a combination of the input and previous
attention masks:</p>
<blockquote>
<div><p>cell_output, new_state = cell(linear(input, prev_attn), prev_state).</p>
</div></blockquote>
<dl class="last docutils">
<dt>Then, we calculate new attention masks:</dt>
<dd><p class="first last">new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))</p>
</dd>
<dt>and then we calculate the output:</dt>
<dd><p class="first last">output = linear(cell_output, new_attn).</p>
</dd>
</dl>
</dd>
<dt>state: The state of each decoder cell the final time-step.</dt>
<dd><p class="first last">It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A tuple of the form (outputs, state), where</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; when num_heads is not positive, there are no inputs, shapes
of attention_states are not set, or input size cannot be inferred
from the input.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.beam_attention_decoder">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">beam_attention_decoder</code><span class="sig-paren">(</span><em>decoder_inputs</em>, <em>initial_state</em>, <em>attention_states</em>, <em>cell</em>, <em>output_size=None</em>, <em>num_heads=1</em>, <em>loop_function=None</em>, <em>dtype=tf.float32</em>, <em>scope=None</em>, <em>initial_state_attention=False</em>, <em>output_projection=None</em>, <em>beam_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.beam_attention_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>RNN decoder with attention for the sequence-to-sequence model.</p>
<p>In this context &#8220;attention&#8221; means that, during decoding, the RNN can look up
information in the additional tensor attention_states, and it does this by
focusing on a few entries from the tensor. This model has proven to yield
especially good results in a number of sequence-to-sequence tasks. This
implementation is based on <a class="reference external" href="http://arxiv.org/abs/1412.7449">http://arxiv.org/abs/1412.7449</a> (see below for
details). It is recommended for complex sequence-to-sequence tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>decoder_inputs</strong> &#8211; A list of 2D Tensors [batch_size x input_size].</li>
<li><strong>initial_state</strong> &#8211; 2D Tensor [batch_size x cell.state_size].</li>
<li><strong>attention_states</strong> &#8211; 3D Tensor [batch_size x attn_length x attn_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function and size.</li>
<li><strong>output_size</strong> &#8211; Size of the output vectors; if None, we use cell.output_size.</li>
<li><strong>num_heads</strong> &#8211; Number of attention heads that read from attention_states.</li>
<li><strong>loop_function</strong> &#8211; <p>If not None, this function will be applied to i-th output
in order to generate i+1-th input, and decoder_inputs will be ignored,
except for the first element (&#8220;GO&#8221; symbol). This can be used for decoding,
but also for training to emulate <a class="reference external" href="http://arxiv.org/abs/1506.03099">http://arxiv.org/abs/1506.03099</a>.
Signature &#8211; loop_function(prev, i) = next</p>
<blockquote>
<div><ul>
<li>prev is a 2D Tensor of shape [batch_size x output_size],</li>
<li>i is an integer, the step number (when advanced control is needed),</li>
<li>next is a 2D Tensor of shape [batch_size x input_size].</li>
</ul>
</div></blockquote>
</li>
<li><strong>dtype</strong> &#8211; The dtype to use for the RNN initial state (default: tf.float32).</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; default: &#8220;attention_decoder&#8221;.</li>
<li><strong>initial_state_attention</strong> &#8211; If False (default), initial attentions are zero.
If True, initialize the attentions from the initial state and attention
states &#8211; useful when we wish to resume decoding from a previously
stored decoder state and attention states.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors of</dt>
<dd><p class="first">shape [batch_size x output_size]. These represent the generated outputs.
Output i is computed from input i (which is either the i-th element
of decoder_inputs or loop_function(output {i-1}, i)) as follows.
First, we run the cell on a combination of the input and previous
attention masks:</p>
<blockquote>
<div><p>cell_output, new_state = cell(linear(input, prev_attn), prev_state).</p>
</div></blockquote>
<dl class="last docutils">
<dt>Then, we calculate new attention masks:</dt>
<dd><p class="first last">new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))</p>
</dd>
<dt>and then we calculate the output:</dt>
<dd><p class="first last">output = linear(cell_output, new_attn).</p>
</dd>
</dl>
</dd>
<dt>state: The state of each decoder cell the final time-step.</dt>
<dd><p class="first last">It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A tuple of the form (outputs, state), where</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; when num_heads is not positive, there are no inputs, shapes
of attention_states are not set, or input size cannot be inferred
from the input.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.beam_rnn_decoder">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">beam_rnn_decoder</code><span class="sig-paren">(</span><em>decoder_inputs</em>, <em>initial_state</em>, <em>cell</em>, <em>loop_function=None</em>, <em>scope=None</em>, <em>output_projection=None</em>, <em>beam_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.beam_rnn_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>RNN decoder for the sequence-to-sequence model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>decoder_inputs</strong> &#8211; A list of 2D Tensors [batch_size x input_size].</li>
<li><strong>initial_state</strong> &#8211; 2D Tensor with shape [batch_size x cell.state_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function and size.</li>
<li><strong>loop_function</strong> &#8211; <p>If not None, this function will be applied to the i-th output
in order to generate the i+1-st input, and decoder_inputs will be ignored,
except for the first element (&#8220;GO&#8221; symbol). This can be used for decoding,
but also for training to emulate <a class="reference external" href="http://arxiv.org/abs/1506.03099">http://arxiv.org/abs/1506.03099</a>.
Signature &#8211; loop_function(prev, i) = next</p>
<blockquote>
<div><ul>
<li>prev is a 2D Tensor of shape [batch_size x output_size],</li>
<li>i is an integer, the step number (when advanced control is needed),</li>
<li>next is a 2D Tensor of shape [batch_size x input_size].</li>
</ul>
</div></blockquote>
</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; defaults to &#8220;rnn_decoder&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors with</dt>
<dd><p class="first last">shape [batch_size x output_size] containing generated outputs.</p>
</dd>
<dt>state: The state of each cell at the final time-step.</dt>
<dd><p class="first">It is a 2D Tensor of shape [batch_size x cell.state_size].
(Note that in some cases, like basic RNN cell or GRU cell, outputs and</p>
<blockquote class="last">
<div><p>states can be the same. They are different for LSTM cells though.)</p>
</div></blockquote>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tuple of the form (outputs, state), where</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.decode_model_with_buckets">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">decode_model_with_buckets</code><span class="sig-paren">(</span><em>encoder_inputs</em>, <em>decoder_inputs</em>, <em>targets</em>, <em>weights</em>, <em>buckets</em>, <em>seq2seq</em>, <em>softmax_loss_function=None</em>, <em>per_example_loss=False</em>, <em>name=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.decode_model_with_buckets" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a sequence-to-sequence model with support for bucketing.</p>
<p>The seq2seq argument is a function that defines a sequence-to-sequence model,
e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(x, y, rnn_cell.GRUCell(24))</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>encoder_inputs</strong> &#8211; A list of Tensors to feed the encoder; first seq2seq input.</li>
<li><strong>decoder_inputs</strong> &#8211; A list of Tensors to feed the decoder; second seq2seq input.</li>
<li><strong>targets</strong> &#8211; A list of 1D batch-sized int32 Tensors (desired output sequence).</li>
<li><strong>weights</strong> &#8211; List of 1D batch-sized float-Tensors to weight the targets.</li>
<li><strong>buckets</strong> &#8211; A list of pairs of (input size, output size) for each bucket.</li>
<li><strong>seq2seq</strong> &#8211; A sequence-to-sequence model function; it takes 2 input that
agree with encoder_inputs and decoder_inputs, and returns a pair
consisting of outputs and states (as, e.g., basic_rnn_seq2seq).</li>
<li><strong>softmax_loss_function</strong> &#8211; Function (inputs-batch, labels-batch) -&gt; loss-batch
to be used instead of the standard softmax (the default if this is None).</li>
<li><strong>per_example_loss</strong> &#8211; Boolean. If set, the returned loss will be a batch-sized
tensor of losses for each sequence in the batch. If unset, it will be
a scalar with the averaged loss from all examples.</li>
<li><strong>name</strong> &#8211; Optional name for this operation, defaults to &#8220;model_with_buckets&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: The outputs for each bucket. Its j&#8217;th element consists of a list</dt>
<dd><p class="first last">of 2D Tensors of shape [batch_size x num_decoder_symbols] (jth outputs).</p>
</dd>
<dt>losses: List of scalar Tensors, representing losses for each bucket, or,</dt>
<dd><p class="first last">if per_example_loss is set, a list of 1D batch-sized float Tensors.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A tuple of the form (outputs, losses), where</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; If length of encoder_inputsut, targets, or weights is smaller
than the largest (last) bucket.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.embedding_attention_decoder">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">embedding_attention_decoder</code><span class="sig-paren">(</span><em>decoder_inputs</em>, <em>initial_state</em>, <em>attention_states</em>, <em>cell</em>, <em>num_symbols</em>, <em>embedding_size</em>, <em>num_heads=1</em>, <em>output_size=None</em>, <em>output_projection=None</em>, <em>feed_previous=False</em>, <em>embedding=None</em>, <em>update_embedding_for_previous=True</em>, <em>dtype=tf.float32</em>, <em>scope=None</em>, <em>initial_state_attention=False</em>, <em>beam_search=True</em>, <em>beam_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.embedding_attention_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>RNN decoder with embedding and attention and a pure-decoding option.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>decoder_inputs</strong> &#8211; A list of 1D batch-sized int32 Tensors (decoder inputs).</li>
<li><strong>initial_state</strong> &#8211; 2D Tensor [batch_size x cell.state_size].</li>
<li><strong>attention_states</strong> &#8211; 3D Tensor [batch_size x attn_length x attn_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function.</li>
<li><strong>num_symbols</strong> &#8211; Integer, how many symbols come into the embedding.</li>
<li><strong>embedding_size</strong> &#8211; Integer, the length of the embedding vector for each symbol.</li>
<li><strong>num_heads</strong> &#8211; Number of attention heads that read from attention_states.</li>
<li><strong>output_size</strong> &#8211; Size of the output vectors; if None, use output_size.</li>
<li><strong>output_projection</strong> &#8211; None or a pair (W, B) of output projection weights and
biases; W has shape [output_size x num_symbols] and B has shape
[num_symbols]; if provided and feed_previous=True, each fed previous
output will first be multiplied by W and added B.</li>
<li><strong>feed_previous</strong> &#8211; <p>Boolean; if True, only the first of decoder_inputs will be
used (the &#8220;GO&#8221; symbol), and all other decoder inputs will be generated by:</p>
<blockquote>
<div>next = embedding_lookup(embedding, argmax(previous_output)),</div></blockquote>
<p>In effect, this implements a greedy decoder. It can also be used
during training to emulate <a class="reference external" href="http://arxiv.org/abs/1506.03099">http://arxiv.org/abs/1506.03099</a>.
If False, decoder_inputs are used as given (the standard decoder case).</p>
</li>
<li><strong>update_embedding_for_previous</strong> &#8211; Boolean; if False and feed_previous=True,
only the embedding for the first symbol of decoder_inputs (the &#8220;GO&#8221;
symbol) will be updated by back propagation. Embeddings for the symbols
generated from the decoder itself remain unchanged. This parameter has
no effect if feed_previous=False.</li>
<li><strong>dtype</strong> &#8211; The dtype to use for the RNN initial states (default: tf.float32).</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; defaults to
&#8220;embedding_attention_decoder&#8221;.</li>
<li><strong>initial_state_attention</strong> &#8211; If False (default), initial attentions are zero.
If True, initialize the attentions from the initial state and attention
states &#8211; useful when we wish to resume decoding from a previously
stored decoder state and attention states.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors with</dt>
<dd><p class="first last">shape [batch_size x output_size] containing the generated outputs.</p>
</dd>
<dt>state: The state of each decoder cell at the final time-step.</dt>
<dd><p class="first last">It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A tuple of the form (outputs, state), where</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; When output_projection has the wrong shape.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.embedding_attention_seq2seq">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">embedding_attention_seq2seq</code><span class="sig-paren">(</span><em>encoder_inputs</em>, <em>decoder_inputs</em>, <em>cell</em>, <em>num_encoder_symbols</em>, <em>num_decoder_symbols</em>, <em>embedding_size</em>, <em>num_heads=1</em>, <em>output_projection=None</em>, <em>feed_previous=False</em>, <em>dtype=tf.float32</em>, <em>scope=None</em>, <em>initial_state_attention=False</em>, <em>beam_search=True</em>, <em>beam_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.embedding_attention_seq2seq" title="Permalink to this definition">¶</a></dt>
<dd><p>Embedding sequence-to-sequence model with attention.</p>
<p>This model first embeds encoder_inputs by a newly created embedding (of shape
[num_encoder_symbols x input_size]). Then it runs an RNN to encode
embedded encoder_inputs into a state vector. It keeps the outputs of this
RNN at every step to use for attention later. Next, it embeds decoder_inputs
by another newly created embedding (of shape [num_decoder_symbols x
input_size]). Then it runs attention decoder, initialized with the last
encoder state, on embedded decoder_inputs and attending to encoder outputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>encoder_inputs</strong> &#8211; A list of 1D int32 Tensors of shape [batch_size].</li>
<li><strong>decoder_inputs</strong> &#8211; A list of 1D int32 Tensors of shape [batch_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function and size.</li>
<li><strong>num_encoder_symbols</strong> &#8211; Integer; number of symbols on the encoder side.</li>
<li><strong>num_decoder_symbols</strong> &#8211; Integer; number of symbols on the decoder side.</li>
<li><strong>embedding_size</strong> &#8211; Integer, the length of the embedding vector for each symbol.</li>
<li><strong>num_heads</strong> &#8211; Number of attention heads that read from attention_states.</li>
<li><strong>output_projection</strong> &#8211; None or a pair (W, B) of output projection weights and
biases; W has shape [output_size x num_decoder_symbols] and B has
shape [num_decoder_symbols]; if provided and feed_previous=True, each
fed previous output will first be multiplied by W and added B.</li>
<li><strong>feed_previous</strong> &#8211; Boolean or scalar Boolean Tensor; if True, only the first
of decoder_inputs will be used (the &#8220;GO&#8221; symbol), and all other decoder
inputs will be taken from previous outputs (as in embedding_rnn_decoder).
If False, decoder_inputs are used as given (the standard decoder case).</li>
<li><strong>dtype</strong> &#8211; The dtype of the initial RNN state (default: tf.float32).</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; defaults to
&#8220;embedding_attention_seq2seq&#8221;.</li>
<li><strong>initial_state_attention</strong> &#8211; If False (default), initial attentions are zero.
If True, initialize the attentions from the initial state and attention
states.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors with</dt>
<dd><p class="first last">shape [batch_size x num_decoder_symbols] containing the generated
outputs.</p>
</dd>
<dt>state: The state of each decoder cell at the final time-step.</dt>
<dd><p class="first last">It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tuple of the form (outputs, state), where</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.embedding_rnn_decoder">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">embedding_rnn_decoder</code><span class="sig-paren">(</span><em>decoder_inputs</em>, <em>initial_state</em>, <em>cell</em>, <em>num_symbols</em>, <em>embedding_size</em>, <em>embedding=None</em>, <em>output_projection=None</em>, <em>feed_previous=False</em>, <em>update_embedding_for_previous=True</em>, <em>scope=None</em>, <em>beam_search=True</em>, <em>beam_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.embedding_rnn_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>RNN decoder with embedding and a pure-decoding option.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>decoder_inputs</strong> &#8211; A list of 1D batch-sized int32 Tensors (decoder inputs).</li>
<li><strong>initial_state</strong> &#8211; 2D Tensor [batch_size x cell.state_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function.</li>
<li><strong>num_symbols</strong> &#8211; Integer, how many symbols come into the embedding.</li>
<li><strong>embedding_size</strong> &#8211; Integer, the length of the embedding vector for each symbol.</li>
<li><strong>output_projection</strong> &#8211; None or a pair (W, B) of output projection weights and
biases; W has shape [output_size x num_symbols] and B has
shape [num_symbols]; if provided and feed_previous=True, each fed
previous output will first be multiplied by W and added B.</li>
<li><strong>feed_previous</strong> &#8211; <p>Boolean; if True, only the first of decoder_inputs will be
used (the &#8220;GO&#8221; symbol), and all other decoder inputs will be generated by:</p>
<blockquote>
<div>next = embedding_lookup(embedding, argmax(previous_output)),</div></blockquote>
<p>In effect, this implements a greedy decoder. It can also be used
during training to emulate <a class="reference external" href="http://arxiv.org/abs/1506.03099">http://arxiv.org/abs/1506.03099</a>.
If False, decoder_inputs are used as given (the standard decoder case).</p>
</li>
<li><strong>update_embedding_for_previous</strong> &#8211; Boolean; if False and feed_previous=True,
only the embedding for the first symbol of decoder_inputs (the &#8220;GO&#8221;
symbol) will be updated by back propagation. Embeddings for the symbols
generated from the decoder itself remain unchanged. This parameter has
no effect if feed_previous=False.</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; defaults to
&#8220;embedding_rnn_decoder&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors with</dt>
<dd><p class="first last">shape [batch_size x output_size] containing the generated outputs.</p>
</dd>
<dt>state: The state of each decoder cell in each time-step. This is a list</dt>
<dd><p class="first last">with length len(decoder_inputs) &#8211; one item for each time-step.
It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A tuple of the form (outputs, state), where</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; When output_projection has the wrong shape.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.embedding_rnn_seq2seq">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">embedding_rnn_seq2seq</code><span class="sig-paren">(</span><em>encoder_inputs</em>, <em>decoder_inputs</em>, <em>cell</em>, <em>num_encoder_symbols</em>, <em>num_decoder_symbols</em>, <em>embedding_size</em>, <em>output_projection=None</em>, <em>feed_previous=False</em>, <em>dtype=tf.float32</em>, <em>scope=None</em>, <em>beam_search=True</em>, <em>beam_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.embedding_rnn_seq2seq" title="Permalink to this definition">¶</a></dt>
<dd><p>Embedding RNN sequence-to-sequence model.</p>
<p>This model first embeds encoder_inputs by a newly created embedding (of shape
[num_encoder_symbols x input_size]). Then it runs an RNN to encode
embedded encoder_inputs into a state vector. Next, it embeds decoder_inputs
by another newly created embedding (of shape [num_decoder_symbols x
input_size]). Then it runs RNN decoder, initialized with the last
encoder state, on embedded decoder_inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>encoder_inputs</strong> &#8211; A list of 1D int32 Tensors of shape [batch_size].</li>
<li><strong>decoder_inputs</strong> &#8211; A list of 1D int32 Tensors of shape [batch_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function and size.</li>
<li><strong>num_encoder_symbols</strong> &#8211; Integer; number of symbols on the encoder side.</li>
<li><strong>num_decoder_symbols</strong> &#8211; Integer; number of symbols on the decoder side.</li>
<li><strong>embedding_size</strong> &#8211; Integer, the length of the embedding vector for each symbol.</li>
<li><strong>output_projection</strong> &#8211; None or a pair (W, B) of output projection weights and
biases; W has shape [output_size x num_decoder_symbols] and B has
shape [num_decoder_symbols]; if provided and feed_previous=True, each
fed previous output will first be multiplied by W and added B.</li>
<li><strong>feed_previous</strong> &#8211; Boolean or scalar Boolean Tensor; if True, only the first
of decoder_inputs will be used (the &#8220;GO&#8221; symbol), and all other decoder
inputs will be taken from previous outputs (as in embedding_rnn_decoder).
If False, decoder_inputs are used as given (the standard decoder case).</li>
<li><strong>dtype</strong> &#8211; The dtype of the initial state for both the encoder and encoder
rnn cells (default: tf.float32).</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; defaults to
&#8220;embedding_rnn_seq2seq&#8221;</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors with</dt>
<dd><p class="first last">shape [batch_size x num_decoder_symbols] containing the generated
outputs.</p>
</dd>
<dt>state: The state of each decoder cell in each time-step. This is a list</dt>
<dd><p class="first last">with length len(decoder_inputs) &#8211; one item for each time-step.
It is a 2D Tensor of shape [batch_size x cell.state_size].</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tuple of the form (outputs, state), where</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.model_with_buckets">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">model_with_buckets</code><span class="sig-paren">(</span><em>encoder_inputs</em>, <em>decoder_inputs</em>, <em>targets</em>, <em>weights</em>, <em>buckets</em>, <em>seq2seq</em>, <em>softmax_loss_function=None</em>, <em>per_example_loss=False</em>, <em>name=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.model_with_buckets" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a sequence-to-sequence model with support for bucketing.</p>
<p>The seq2seq argument is a function that defines a sequence-to-sequence model,
e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(x, y, rnn_cell.GRUCell(24))</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>encoder_inputs</strong> &#8211; A list of Tensors to feed the encoder; first seq2seq input.</li>
<li><strong>decoder_inputs</strong> &#8211; A list of Tensors to feed the decoder; second seq2seq input.</li>
<li><strong>targets</strong> &#8211; A list of 1D batch-sized int32 Tensors (desired output sequence).</li>
<li><strong>weights</strong> &#8211; List of 1D batch-sized float-Tensors to weight the targets.</li>
<li><strong>buckets</strong> &#8211; A list of pairs of (input size, output size) for each bucket.</li>
<li><strong>seq2seq</strong> &#8211; A sequence-to-sequence model function; it takes 2 input that
agree with encoder_inputs and decoder_inputs, and returns a pair
consisting of outputs and states (as, e.g., basic_rnn_seq2seq).</li>
<li><strong>softmax_loss_function</strong> &#8211; Function (inputs-batch, labels-batch) -&gt; loss-batch
to be used instead of the standard softmax (the default if this is None).</li>
<li><strong>per_example_loss</strong> &#8211; Boolean. If set, the returned loss will be a batch-sized
tensor of losses for each sequence in the batch. If unset, it will be
a scalar with the averaged loss from all examples.</li>
<li><strong>name</strong> &#8211; Optional name for this operation, defaults to &#8220;model_with_buckets&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: The outputs for each bucket. Its j&#8217;th element consists of a list</dt>
<dd><p class="first last">of 2D Tensors of shape [batch_size x num_decoder_symbols] (jth outputs).</p>
</dd>
<dt>losses: List of scalar Tensors, representing losses for each bucket, or,</dt>
<dd><p class="first last">if per_example_loss is set, a list of 1D batch-sized float Tensors.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A tuple of the form (outputs, losses), where</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; If length of encoder_inputsut, targets, or weights is smaller
than the largest (last) bucket.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.rnn_decoder">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">rnn_decoder</code><span class="sig-paren">(</span><em>decoder_inputs</em>, <em>initial_state</em>, <em>cell</em>, <em>output_projection=None</em>, <em>loop_function=None</em>, <em>scope=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.rnn_decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>RNN decoder for the sequence-to-sequence model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>decoder_inputs</strong> &#8211; A list of 2D Tensors [batch_size x input_size].</li>
<li><strong>initial_state</strong> &#8211; 2D Tensor with shape [batch_size x cell.state_size].</li>
<li><strong>cell</strong> &#8211; rnn_cell.RNNCell defining the cell function and size.</li>
<li><strong>loop_function</strong> &#8211; <p>If not None, this function will be applied to the i-th output
in order to generate the i+1-st input, and decoder_inputs will be ignored,
except for the first element (&#8220;GO&#8221; symbol). This can be used for decoding,
but also for training to emulate <a class="reference external" href="http://arxiv.org/abs/1506.03099">http://arxiv.org/abs/1506.03099</a>.
Signature &#8211; loop_function(prev, i) = next</p>
<blockquote>
<div><ul>
<li>prev is a 2D Tensor of shape [batch_size x output_size],</li>
<li>i is an integer, the step number (when advanced control is needed),</li>
<li>next is a 2D Tensor of shape [batch_size x input_size].</li>
</ul>
</div></blockquote>
</li>
<li><strong>scope</strong> &#8211; VariableScope for the created subgraph; defaults to &#8220;rnn_decoder&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>outputs: A list of the same length as decoder_inputs of 2D Tensors with</dt>
<dd><p class="first last">shape [batch_size x output_size] containing generated outputs.</p>
</dd>
<dt>state: The state of each cell at the final time-step.</dt>
<dd><p class="first">It is a 2D Tensor of shape [batch_size x cell.state_size].
(Note that in some cases, like basic RNN cell or GRU cell, outputs and</p>
<blockquote class="last">
<div><p>states can be the same. They are different for LSTM cells though.)</p>
</div></blockquote>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">A tuple of the form (outputs, state), where</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.sequence_loss">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">sequence_loss</code><span class="sig-paren">(</span><em>logits</em>, <em>targets</em>, <em>weights</em>, <em>average_across_timesteps=True</em>, <em>average_across_batch=True</em>, <em>softmax_loss_function=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.sequence_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Weighted cross-entropy loss for a sequence of logits, batch-collapsed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>logits</strong> &#8211; List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>
<li><strong>targets</strong> &#8211; List of 1D batch-sized int32 Tensors of the same length as logits.</li>
<li><strong>weights</strong> &#8211; List of 1D batch-sized float-Tensors of the same length as logits.</li>
<li><strong>average_across_timesteps</strong> &#8211; If set, divide the returned cost by the total
label weight.</li>
<li><strong>average_across_batch</strong> &#8211; If set, divide the returned cost by the batch size.</li>
<li><strong>softmax_loss_function</strong> &#8211; Function (inputs-batch, labels-batch) -&gt; loss-batch
to be used instead of the standard softmax (the default if this is None).</li>
<li><strong>name</strong> &#8211; Optional name for this operation, defaults to &#8220;sequence_loss&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The average log-perplexity per symbol (weighted).</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">A scalar float Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; If len(logits) is different from len(targets) or len(weights).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="quebap.projects.autoread.wikireading.my_seq2seq.sequence_loss_by_example">
<code class="descclassname">quebap.projects.autoread.wikireading.my_seq2seq.</code><code class="descname">sequence_loss_by_example</code><span class="sig-paren">(</span><em>logits</em>, <em>targets</em>, <em>weights</em>, <em>average_across_timesteps=True</em>, <em>softmax_loss_function=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.my_seq2seq.sequence_loss_by_example" title="Permalink to this definition">¶</a></dt>
<dd><p>Weighted cross-entropy loss for a sequence of logits (per example).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>logits</strong> &#8211; List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>
<li><strong>targets</strong> &#8211; List of 1D batch-sized int32 Tensors of the same length as logits.</li>
<li><strong>weights</strong> &#8211; List of 1D batch-sized float-Tensors of the same length as logits.</li>
<li><strong>average_across_timesteps</strong> &#8211; If set, divide the returned cost by the total
label weight.</li>
<li><strong>softmax_loss_function</strong> &#8211; Function (inputs-batch, labels-batch) -&gt; loss-batch
to be used instead of the standard softmax (the default if this is None).</li>
<li><strong>name</strong> &#8211; Optional name for this operation, default: &#8220;sequence_loss_by_example&#8221;.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The log-perplexity for each sequence.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">1D batch-sized float Tensor</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> &#8211; If len(logits) is different from len(targets) or len(weights).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-quebap.projects.autoread.wikireading.qa">
<span id="quebap-projects-autoread-wikireading-qa-module"></span><h2>quebap.projects.autoread.wikireading.qa module<a class="headerlink" href="#module-quebap.projects.autoread.wikireading.qa" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="quebap.projects.autoread.wikireading.qa.QASetting">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.qa.</code><code class="descname">QASetting</code><span class="sig-paren">(</span><em>question</em>, <em>answers</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.qa.QASetting" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-quebap.projects.autoread.wikireading.sampler">
<span id="quebap-projects-autoread-wikireading-sampler-module"></span><h2>quebap.projects.autoread.wikireading.sampler module<a class="headerlink" href="#module-quebap.projects.autoread.wikireading.sampler" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="quebap.projects.autoread.wikireading.sampler.BatchSampler">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.sampler.</code><code class="descname">BatchSampler</code><span class="sig-paren">(</span><em>sess</em>, <em>dir</em>, <em>filenames</em>, <em>batch_size</em>, <em>max_length</em>, <em>max_vocab</em>, <em>max_answer_vocab</em>, <em>vocab</em>, <em>batches_per_epoch=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.BatchSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="quebap.projects.autoread.wikireading.sampler.BatchSampler.close">
<code class="descname">close</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.BatchSampler.close" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="quebap.projects.autoread.wikireading.sampler.BatchSampler.get_batch">
<code class="descname">get_batch</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.BatchSampler.get_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="quebap.projects.autoread.wikireading.sampler.ContextBatchSampler">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.sampler.</code><code class="descname">ContextBatchSampler</code><span class="sig-paren">(</span><em>sess</em>, <em>dir</em>, <em>filenames</em>, <em>batch_size</em>, <em>max_length</em>, <em>max_vocab</em>, <em>vocab</em>, <em>batches_per_epoch=None</em>, <em>word_freq={}</em>, <em>beta=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.ContextBatchSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#quebap.projects.autoread.wikireading.sampler.BatchSampler" title="quebap.projects.autoread.wikireading.sampler.BatchSampler"><code class="xref py py-class docutils literal"><span class="pre">quebap.projects.autoread.wikireading.sampler.BatchSampler</span></code></a></p>
<dl class="method">
<dt id="quebap.projects.autoread.wikireading.sampler.ContextBatchSampler.get_batch">
<code class="descname">get_batch</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.ContextBatchSampler.get_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="quebap.projects.autoread.wikireading.sampler.TextBatchSampler">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.sampler.</code><code class="descname">TextBatchSampler</code><span class="sig-paren">(</span><em>sess</em>, <em>directory</em>, <em>batch_size</em>, <em>max_length</em>, <em>max_vocab</em>, <em>max_answer_vocab</em>, <em>vocab</em>, <em>epoch_batches=None</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.TextBatchSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="quebap.projects.autoread.wikireading.sampler.TextBatchSampler.get_batch">
<code class="descname">get_batch</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.sampler.TextBatchSampler.get_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-quebap.projects.autoread.wikireading.seq2seq_models">
<span id="quebap-projects-autoread-wikireading-seq2seq-models-module"></span><h2>quebap.projects.autoread.wikireading.seq2seq_models module<a class="headerlink" href="#module-quebap.projects.autoread.wikireading.seq2seq_models" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.AttentiveAnswerSeq2SeqModel">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.seq2seq_models.</code><code class="descname">AttentiveAnswerSeq2SeqModel</code><span class="sig-paren">(</span><em>size</em>, <em>vocab_size</em>, <em>answer_vocab_size</em>, <em>max_context_length</em>, <em>max_question_length</em>, <em>max_answer_length</em>, <em>beam_size=1</em>, <em>is_train=True</em>, <em>learning_rate=0.01</em>, <em>keep_prob=1.0</em>, <em>composition='GRU'</em>, <em>devices=None</em>, <em>name='GatedSeq2SeqModel'</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.AttentiveAnswerSeq2SeqModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel" title="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel"><code class="xref py py-class docutils literal"><span class="pre">quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel</span></code></a></p>
<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.AttentiveAnswerSeq2SeqModel.encoder">
<code class="descname">encoder</code><span class="sig-paren">(</span><em>embedded_question</em>, <em>embedded_context</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.AttentiveAnswerSeq2SeqModel.encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.AttentiveSeq2SeqModel">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.seq2seq_models.</code><code class="descname">AttentiveSeq2SeqModel</code><span class="sig-paren">(</span><em>size</em>, <em>vocab_size</em>, <em>answer_vocab_size</em>, <em>max_context_length</em>, <em>max_question_length</em>, <em>max_answer_length</em>, <em>beam_size=1</em>, <em>is_train=True</em>, <em>learning_rate=0.01</em>, <em>keep_prob=1.0</em>, <em>composition='GRU'</em>, <em>devices=None</em>, <em>name='GatedSeq2SeqModel'</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.AttentiveSeq2SeqModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel" title="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel"><code class="xref py py-class docutils literal"><span class="pre">quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel</span></code></a></p>
<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.AttentiveSeq2SeqModel.encoder">
<code class="descname">encoder</code><span class="sig-paren">(</span><em>embedded_question</em>, <em>embedded_context</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.AttentiveSeq2SeqModel.encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.ProjectionWrapper">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.seq2seq_models.</code><code class="descname">ProjectionWrapper</code><span class="sig-paren">(</span><em>cell</em>, <em>output_size</em>, <em>w</em>, <em>b</em>, <em>activation_fn=&lt;function relu&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.ProjectionWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">tensorflow.python.ops.rnn_cell.RNNCell</span></code></p>
<dl class="attribute">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.ProjectionWrapper.output_size">
<code class="descname">output_size</code><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.ProjectionWrapper.output_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.ProjectionWrapper.state_size">
<code class="descname">state_size</code><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.ProjectionWrapper.state_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.seq2seq_models.</code><code class="descname">QASeq2SeqModel</code><span class="sig-paren">(</span><em>size</em>, <em>vocab_size</em>, <em>answer_vocab_size</em>, <em>max_context_length</em>, <em>max_question_length</em>, <em>max_answer_length</em>, <em>beam_size=1</em>, <em>is_train=True</em>, <em>learning_rate=0.01</em>, <em>keep_prob=1.0</em>, <em>composition='GRU'</em>, <em>devices=None</em>, <em>name='GatedSeq2SeqModel'</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.decoder">
<code class="descname">decoder</code><span class="sig-paren">(</span><em>start_state</em>, <em>encoder_outputs</em>, <em>embedded_answers</em>, <em>with_attention=False</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.encoder">
<code class="descname">encoder</code><span class="sig-paren">(</span><em>embedded_question</em>, <em>embedded_context</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.run">
<code class="descname">run</code><span class="sig-paren">(</span><em>sess</em>, <em>goal</em>, <em>qa_settings</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.run" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.set_eval">
<code class="descname">set_eval</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.set_eval" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.set_train">
<code class="descname">set_train</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel.set_train" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QConditionedSeq2SeqModel">
<em class="property">class </em><code class="descclassname">quebap.projects.autoread.wikireading.seq2seq_models.</code><code class="descname">QConditionedSeq2SeqModel</code><span class="sig-paren">(</span><em>size</em>, <em>vocab_size</em>, <em>answer_vocab_size</em>, <em>max_context_length</em>, <em>max_question_length</em>, <em>max_answer_length</em>, <em>beam_size=1</em>, <em>is_train=True</em>, <em>learning_rate=0.01</em>, <em>keep_prob=1.0</em>, <em>composition='GRU'</em>, <em>devices=None</em>, <em>name='GatedSeq2SeqModel'</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QConditionedSeq2SeqModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel" title="quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel"><code class="xref py py-class docutils literal"><span class="pre">quebap.projects.autoread.wikireading.seq2seq_models.QASeq2SeqModel</span></code></a></p>
<dl class="method">
<dt id="quebap.projects.autoread.wikireading.seq2seq_models.QConditionedSeq2SeqModel.encoder">
<code class="descname">encoder</code><span class="sig-paren">(</span><em>embedded_question</em>, <em>embedded_context</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.seq2seq_models.QConditionedSeq2SeqModel.encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-quebap.projects.autoread.wikireading">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-quebap.projects.autoread.wikireading" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="quebap.projects.autoread.wikireading.load_vocab">
<code class="descclassname">quebap.projects.autoread.wikireading.</code><code class="descname">load_vocab</code><span class="sig-paren">(</span><em>fn</em><span class="sig-paren">)</span><a class="headerlink" href="#quebap.projects.autoread.wikireading.load_vocab" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="data">
<dt id="quebap.projects.autoread.wikireading.tfrecord_features">
<code class="descclassname">quebap.projects.autoread.wikireading.</code><code class="descname">tfrecord_features</code><em class="property"> = {'raw_answers': VarLenFeature(dtype=tf.string), 'raw_answer_ids': VarLenFeature(dtype=tf.int64), 'answer_location': VarLenFeature(dtype=tf.int64), 'document_sequence': VarLenFeature(dtype=tf.int64), 'question_string_sequence': VarLenFeature(dtype=tf.string), 'answer_breaks': VarLenFeature(dtype=tf.int64), 'answer_sequence': VarLenFeature(dtype=tf.int64), 'sentence_breaks': VarLenFeature(dtype=tf.int64), 'answer_string_sequence': VarLenFeature(dtype=tf.string), 'paragraph_breaks': VarLenFeature(dtype=tf.int64), 'break_levels': VarLenFeature(dtype=tf.int64), 'full_match_answer_location': VarLenFeature(dtype=tf.int64), 'answer_ids': VarLenFeature(dtype=tf.int64), 'string_sequence': VarLenFeature(dtype=tf.string), 'question_sequence': VarLenFeature(dtype=tf.int64), 'type_sequence': VarLenFeature(dtype=tf.int64)}</em><a class="headerlink" href="#quebap.projects.autoread.wikireading.tfrecord_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Largest answer
A History of the Clan MacLean from Its First Settlement at Duard Castle, in the Isle of Mull, to the Present Period: Including a Genealogical Account of Some of the Principal Families Together with Their Heraldry, Legends, Superstitions, etc.</p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">quebap.projects.autoread.wikireading package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-quebap.projects.autoread.wikireading.my_seq2seq">quebap.projects.autoread.wikireading.my_seq2seq module</a></li>
<li><a class="reference internal" href="#module-quebap.projects.autoread.wikireading.qa">quebap.projects.autoread.wikireading.qa module</a></li>
<li><a class="reference internal" href="#module-quebap.projects.autoread.wikireading.sampler">quebap.projects.autoread.wikireading.sampler module</a></li>
<li><a class="reference internal" href="#module-quebap.projects.autoread.wikireading.seq2seq_models">quebap.projects.autoread.wikireading.seq2seq_models module</a></li>
<li><a class="reference internal" href="#module-quebap.projects.autoread.wikireading">Module contents</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="modules.html">quebap</a><ul>
  <li><a href="quebap.html">quebap package</a><ul>
  <li><a href="quebap.projects.html">quebap.projects package</a><ul>
  <li><a href="quebap.projects.autoread.html">quebap.projects.autoread package</a><ul>
      <li>Previous: <a href="quebap.projects.autoread.wikipedia.html" title="previous chapter">quebap.projects.autoread.wikipedia package</a></li>
      <li>Next: <a href="quebap.projects.clozecompose.html" title="next chapter">quebap.projects.clozecompose package</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="sources/quebap.projects.autoread.wikireading.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, UCL Machine Reading Group.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="sources/quebap.projects.autoread.wikireading.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>