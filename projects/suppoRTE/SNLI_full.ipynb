{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for debugging full SNLI experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "#import matplotlib\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline  \n",
    "\n",
    "\n",
    "#general\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger(os.path.basename(sys.argv[0]))\n",
    "\n",
    "#jack\n",
    "os.chdir('../..') #First change dir to Jack parent\n",
    "import jtr.jack.readers as readers\n",
    "from jtr.load.embeddings.embeddings import load_embeddings, Embeddings\n",
    "\n",
    "#info \n",
    "print('current working directory: ', os.getcwd())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "logger.info('available devices:')\n",
    "for l in device_lib.list_local_devices():\n",
    "    logger.info('device info: ' + str(l).replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"./data/SNLI/snli_1.0/snli_1.0_train_jtr_v1.json\"\n",
    "dev_file = \"./data/SNLI/snli_1.0/snli_1.0_dev_jtr_v1.json\"\n",
    "test_file = \"./data/SNLI/snli_1.0/snli_1.0_test_jtr_v1.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment mode:\n",
    "debug = True #only use small part ofdata; train=test=dev\n",
    "debug_examples = 500  #train set size in case debug\n",
    "\n",
    "#tensorboard folder\n",
    "#tensorboard_folder = './.tb/'\n",
    "\n",
    "#config for preprocessing\n",
    "lowercase = True\n",
    "#vocab_sep = True #Should there be separate vocabularies for questions and supports, vs. candidates and answers. This needs to be set to True for candidate-based methods.\n",
    "\n",
    "#config of Vocab and NeuralVocab\n",
    "vocab_max_size = sys.maxsize\n",
    "vocab_min_freq = 2\n",
    "pretrain = True\n",
    "train_pretrained = False\n",
    "#normalize_pretrain = True\n",
    "repr_dim_input = 50 if debug else 300\n",
    "#repr_dim_input_trf = 100\n",
    "#repr_dim_output = 100\n",
    "\n",
    "#model -- choices=sorted(reader_models.keys())\n",
    "#model = bicond_singlesupport_reader \n",
    "\n",
    "#model hyperparams\n",
    "hidden_dim = 100\n",
    "#dim_rnn_in = 100\n",
    "#dim_rnn_out = 100\n",
    "\n",
    "\n",
    "#training\n",
    "batch_size = 32 if debug else 256\n",
    "eval_batch_size = 256\n",
    "learning_rate = 0.001\n",
    "#l2 = 0.0\n",
    "#clip_value = 0.0\n",
    "dropout = 0.0\n",
    "epochs = 30\n",
    "#buckets = 1\n",
    "\n",
    "#misc\n",
    "seed = 1337\n",
    "#logfile = ''\n",
    "#write_metrics_to = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': batch_size,\n",
    "    'eval_batch_size': eval_batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'vocab_min_freq': vocab_min_freq,\n",
    "    'vocab_max_size': vocab_max_size,\n",
    "    'lowercase': lowercase,\n",
    "    'repr_dim_input': repr_dim_input,\n",
    "    'repr_dim': hidden_dim,\n",
    "    'train_pretrained': train_pretrained,\n",
    "    'dropout': dropout    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jtr.jack.data_structures import load_labelled_data\n",
    "\n",
    "snli_data = []\n",
    "splits = [train_file, dev_file, test_file]\n",
    "max_count = debug_examples if debug else None\n",
    "\n",
    "train_set, dev_set, test_set = [load_labelled_data(f, max_count) for f in splits]\n",
    "\n",
    "for s,l in zip([train_set, dev_set, test_set],['train', 'dev', 'test']):\n",
    "    print('loaded %d %s instances'%(len(s), l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embeddings and create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jtr.preprocess.vocab import Vocab\n",
    "\n",
    "#load pre-trained embeddings\n",
    "embeddings = None\n",
    "if pretrain:\n",
    "    if debug:\n",
    "        emb_file = 'glove.6B.50d.txt'\n",
    "        embeddings = load_embeddings(os.path.join('jtr', 'data', 'GloVe', emb_file), 'glove')\n",
    "    else:\n",
    "        emb_file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "        embeddings = load_embeddings(os.path.join('jtr', 'data', 'SG_GoogleNews', emb_file, 'word2vec'))    \n",
    "    logger.info('loaded pre-trained embeddings ({})'.format(emb_file))\n",
    "\n",
    "#create Vocab object\n",
    "vocab = Vocab(emb=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create example reader with a basic config\n",
    "reader = readers.readers[\"snli_reader\"](vocab, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create hooks which keep track of the metrics such as the loss\n",
    "# We also create a classification metric monitoring hook for our model\n",
    "from jtr.jack.train.hooks import LossHook\n",
    "hooks = [LossHook(reader, iter_interval=100),\n",
    "         readers.eval_hooks['snli_reader'](reader, train_set, iter_interval=100, info='train'),\n",
    "         readers.eval_hooks['snli_reader'](reader, dev_set, iter_interval=100, info='dev'),\n",
    "         readers.eval_hooks['snli_reader'](reader, test_set, epoch_interval=epochs, info='test')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we initialise our optimiser\n",
    "# we choose Adam with standard momentum values\n",
    "optim = tf.train.AdamOptimizer(config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's train the reader on the CPU for 2 epochs\n",
    "reader.train(optim, train_set,\n",
    "             hooks=hooks, max_epochs=epochs,\n",
    "             device='/cpu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot loss\n",
    "hooks[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot train accuracy and F1\n",
    "hooks[1].plot(ylim=[0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot dev accuracy and F1\n",
    "hooks[2].plot(ylim=[0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can analyse what our model is doing by using the MisclassificationAnalyzerOutputModule.\n",
    "#from jtr.jack.tasks.mcqa.simple_mcqa import MisclassificationOutputModule   \n",
    "\n",
    "# We want misclassifications where the model predicted a probability between 0 and 0.2, \n",
    "# and we want to print 10 examples\n",
    "#reader.output_module = MisclassificationOutputModule(interval=[0.0, 0.20], limit=10)                                                                                                                     \n",
    "#reader.process_outputs(test_set) # run the output module on the test set\n",
    "# From the output below we can see that our model still has problems to distinguishe between neutral and\n",
    "# entailment between premise and hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
