{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import NamedTuple, List, Tuple, Sequence,Mapping\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def to_tensors(data:Sequence[Mapping[str,float]], targets:Mapping[str,float]):\n",
    "    \"\"\"\n",
    "    Turns dictionaries of instance features and target features into numpy arrays.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    def to_vector(instance:Mapping[str,float]):\n",
    "        vector = np.ndarray(len(vocab))\n",
    "        for key, value in instance.items():\n",
    "            vector[vocab[key]] = value\n",
    "        return vector\n",
    "        \n",
    "    for instance in data:\n",
    "        for key,value in instance.items():\n",
    "            if key not in vocab:\n",
    "                vocab[key] = len(vocab)\n",
    "    data_vectors = []\n",
    "    for instance in data:\n",
    "        data_vectors.append(to_vector(instance))\n",
    "        \n",
    "    data_matrix = np.stack(data_vectors)\n",
    "    \n",
    "    target_vector = to_vector(targets)\n",
    "\n",
    "    print(data_matrix)\n",
    "    \n",
    "    return data_matrix, target_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_correction_weights(data:Sequence[Mapping[str,float]], targets:Mapping[str,float], \n",
    "                                reg_lambda = 0.0, debug=False):\n",
    "    \"\"\"\n",
    "    Calculates a sequence of instance weights such that the total sum of their features\n",
    "    equals/is close to the target vector.\n",
    "    Args:\n",
    "        data: list of feature vectors in sparse dictionary format\n",
    "        targets: feature vector with target total counts\n",
    "    Returns:\n",
    "        `instance_weights`, `total` where instance_weights is a list of weights corresponding\n",
    "        to the instances in `data` and `total` is the total population vector approximation.\n",
    "    \"\"\"\n",
    "    data_matrix, target_vector = to_tensors(data,targets)\n",
    "    \n",
    "    instance_weights = tf.Variable(initial_value=tf.zeros([len(data)]))\n",
    "    data_placeholder = tf.placeholder(tf.float32, shape=data_matrix.shape)\n",
    "    target_placeholder = tf.placeholder(tf.float32, shape=target_vector.shape)\n",
    "    total = tf.einsum(\"ij,j -> i\", data_placeholder, instance_weights)\n",
    "    target_loss = tf.nn.l2_loss(total - target_placeholder)\n",
    "    regularizer = tf.nn.l2_loss(instance_weights - 1)\n",
    "    total_loss = target_loss + reg_lambda * regularizer\n",
    "\n",
    "    sess = tf.Session()\n",
    "    optimizer = tf.train.AdamOptimizer(0.1)\n",
    "    opt_op = optimizer.minimize(total_loss)\n",
    "    project_nonnegative = tf.assign(instance_weights, tf.maximum(0.0,instance_weights))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(0,100):\n",
    "        feed_dict = {data_placeholder:data_matrix, \n",
    "              target_placeholder:target_vector}\n",
    "        sess.run(opt_op,feed_dict)\n",
    "        sess.run(project_nonnegative)\n",
    "        result = sess.run({'total':total,\n",
    "              'loss':total_loss, \n",
    "              'regularizer':regularizer, \n",
    "              'weights':instance_weights,\n",
    "              'target':target_placeholder},feed_dict)\n",
    "        if debug:\n",
    "            print(result['loss'])\n",
    "            print(result['weights'])\n",
    "            print(result['total'])\n",
    "    return result['weights'],result['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 0.  1.]]\n",
      "0.806\n",
      "[ 0.09999997  0.09999999]\n",
      "[ 0.19999996  0.09999999]\n",
      "0.565831\n",
      "[ 0.19890024  0.19926944]\n",
      "[ 0.3981697   0.19926944]\n",
      "0.379527\n",
      "[ 0.29551122  0.29710737]\n",
      "[ 0.59261858  0.29710737]\n",
      "0.245664\n",
      "[ 0.38814718  0.39260525]\n",
      "[ 0.78075242  0.39260525]\n",
      "0.160732\n",
      "[ 0.47452283  0.48461699]\n",
      "[ 0.95913982  0.48461699]\n",
      "0.118539\n",
      "[ 0.55177951  0.57176262]\n",
      "[ 1.12354207  0.57176262]\n",
      "0.110016\n",
      "[ 0.61678004  0.65248877]\n",
      "[ 1.26926875  0.65248877]\n",
      "0.123891\n",
      "[ 0.66672748  0.72520626]\n",
      "[ 1.39193368  0.72520626]\n",
      "0.148367\n",
      "[ 0.69988877  0.7884959 ]\n",
      "[ 1.48838472  0.7884959 ]\n",
      "0.17318\n",
      "[ 0.71598929  0.84132487]\n",
      "[ 1.55731416  0.84132487]\n",
      "0.191089\n",
      "[ 0.71605974  0.88319659]\n",
      "[ 1.59925628  0.88319659]\n",
      "0.1983\n",
      "[ 0.70194262  0.91418767]\n",
      "[ 1.61613035  0.91418767]\n",
      "0.194053\n",
      "[ 0.6758008   0.93488008]\n",
      "[ 1.61068082  0.93488008]\n",
      "0.179799\n",
      "[ 0.63980412  0.94623655]\n",
      "[ 1.58604074  0.94623655]\n",
      "0.158329\n",
      "[ 0.59599143  0.94946963]\n",
      "[ 1.54546106  0.94946963]\n",
      "0.133022\n",
      "[ 0.54624265  0.94593447]\n",
      "[ 1.49217713  0.94593447]\n",
      "0.10724\n",
      "[ 0.49230176  0.93705273]\n",
      "[ 1.42935443  0.93705273]\n",
      "0.0838978\n",
      "[ 0.43581405  0.9242627 ]\n",
      "[ 1.36007679  0.9242627 ]\n",
      "0.0651609\n",
      "[ 0.37835768  0.90898478]\n",
      "[ 1.28734243  0.90898478]\n",
      "0.0522749\n",
      "[ 0.32145911  0.89259213]\n",
      "[ 1.21405125  0.89259213]\n",
      "0.0455195\n",
      "[ 0.26658642  0.87637943]\n",
      "[ 1.14296579  0.87637943]\n",
      "0.0442854\n",
      "[ 0.21511942  0.86152631]\n",
      "[ 1.07664573  0.86152631]\n",
      "0.0472681\n",
      "[ 0.16829936  0.84905642]\n",
      "[ 1.0173558   0.84905642]\n",
      "0.0527532\n",
      "[ 0.12716804  0.83979714]\n",
      "[ 0.9669652   0.83979714]\n",
      "0.0589447\n",
      "[ 0.09250882  0.83434641]\n",
      "[ 0.92685521  0.83434641]\n",
      "0.0642754\n",
      "[ 0.06480402  0.83305305]\n",
      "[ 0.89785707  0.83305305]\n",
      "0.0676385\n",
      "[ 0.04421799  0.83601373]\n",
      "[ 0.88023174  0.83601373]\n",
      "0.0685046\n",
      "[ 0.03060794  0.84308642]\n",
      "[ 0.87369436  0.84308642]\n",
      "0.0669155\n",
      "[ 0.02355771  0.85391641]\n",
      "[ 0.87747413  0.85391641]\n",
      "0.0633765\n",
      "[ 0.02242555  0.86797082]\n",
      "[ 0.89039636  0.86797082]\n",
      "0.0586852\n",
      "[ 0.02639745  0.8845771 ]\n",
      "[ 0.91097456  0.8845771 ]\n",
      "0.0537376\n",
      "[ 0.0345398   0.90296239]\n",
      "[ 0.93750221  0.90296239]\n",
      "0.0493488\n",
      "[ 0.04584868  0.92229337]\n",
      "[ 0.96814203  0.92229337]\n",
      "0.0461151\n",
      "[ 0.05929558  0.94171596]\n",
      "[ 1.00101149  0.94171596]\n",
      "0.0443356\n",
      "[ 0.07387014  0.96039492]\n",
      "[ 1.03426504  0.96039492]\n",
      "0.0439972\n",
      "[ 0.08862078  0.97755301]\n",
      "[ 1.06617379  0.97755301]\n",
      "0.0448205\n",
      "[ 0.10269269  0.99250776]\n",
      "[ 1.09520042  0.99250776]\n",
      "0.0463493\n",
      "[ 0.1153619   1.00470424]\n",
      "[ 1.12006617  1.00470424]\n",
      "0.0480646\n",
      "[ 0.12606287  1.01374018]\n",
      "[ 1.13980305  1.01374018]\n",
      "0.0494948\n",
      "[ 0.13440718  1.01938236]\n",
      "[ 1.15378952  1.01938236]\n",
      "0.050303\n",
      "[ 0.14019156  1.02157104]\n",
      "[ 1.1617626   1.02157104]\n",
      "0.0503346\n",
      "[ 0.14339474  1.02041483]\n",
      "[ 1.16380954  1.02041483]\n",
      "0.0496209\n",
      "[ 0.14416373  1.01617503]\n",
      "[ 1.16033876  1.01617503]\n",
      "0.0483446\n",
      "[ 0.14279141  1.00924325]\n",
      "[ 1.15203464  1.00924325]\n",
      "0.046779\n",
      "[ 0.13968746  1.00011349]\n",
      "[ 1.13980091  1.00011349]\n",
      "0.0452183\n",
      "[ 0.1353451   0.98935068]\n",
      "[ 1.12469578  0.98935068]\n",
      "0.0439127\n",
      "[ 0.13030536  0.97755831]\n",
      "[ 1.10786366  0.97755831]\n",
      "0.0430233\n",
      "[ 0.12512106  0.96534544]\n",
      "[ 1.0904665   0.96534544]\n",
      "0.0426012\n",
      "[ 0.12032187  0.95329523]\n",
      "[ 1.0736171   0.95329523]\n",
      "0.0425938\n",
      "[ 0.11638251  0.94193602]\n",
      "[ 1.0583185   0.94193602]\n",
      "0.0428724\n",
      "[ 0.11369555  0.9317165 ]\n",
      "[ 1.04541206  0.9317165 ]\n",
      "0.0432719\n",
      "[ 0.11255055  0.92298627]\n",
      "[ 1.03553677  0.92298627]\n",
      "0.0436336\n",
      "[ 0.1131207   0.91598314]\n",
      "[ 1.02910388  0.91598314]\n",
      "0.0438397\n",
      "[ 0.11545759  0.91082752]\n",
      "[ 1.02628517  0.91082752]\n",
      "0.043833\n",
      "[ 0.11949403  0.90752441]\n",
      "[ 1.02701843  0.90752441]\n",
      "0.0436205\n",
      "[ 0.12505433  0.90597218]\n",
      "[ 1.03102648  0.90597218]\n",
      "0.0432608\n",
      "[ 0.13187085  0.90597743]\n",
      "[ 1.03784823  0.90597743]\n",
      "0.0428417\n",
      "[ 0.1396053   0.90727425]\n",
      "[ 1.04687953  0.90727425]\n",
      "0.0424545\n",
      "[ 0.14787327  0.90954655]\n",
      "[ 1.05741978  0.90954655]\n",
      "0.0421709\n",
      "[ 0.15627027  0.91245204]\n",
      "[ 1.06872225  0.91245204]\n",
      "0.0420286\n",
      "[ 0.16439782  0.91564608]\n",
      "[ 1.08004391  0.91564608]\n",
      "0.042027\n",
      "[ 0.17188789  0.91880405]\n",
      "[ 1.09069192  0.91880405]\n",
      "0.0421329\n",
      "[ 0.17842421  0.92164111]\n",
      "[ 1.10006535  0.92164111]\n",
      "0.0422935\n",
      "[ 0.18375939  0.92392778]\n",
      "[ 1.10768723  0.92392778]\n",
      "0.0424522\n",
      "[ 0.18772666  0.92550111]\n",
      "[ 1.11322773  0.92550111]\n",
      "0.0425629\n",
      "[ 0.1902457   0.92627037]\n",
      "[ 1.11651611  0.92627037]\n",
      "0.0425999\n",
      "[ 0.19132251  0.92621762]\n",
      "[ 1.11754012  0.92621762]\n",
      "0.0425607\n",
      "[ 0.19104356  0.9253931 ]\n",
      "[ 1.11643672  0.9253931 ]\n",
      "0.0424627\n",
      "[ 0.18956506  0.92390627]\n",
      "[ 1.11347127  0.92390627]\n",
      "0.0423359\n",
      "[ 0.18709828  0.92191327]\n",
      "[ 1.10901153  0.92191327]\n",
      "0.0422122\n",
      "[ 0.18389222  0.91960198]\n",
      "[ 1.10349417  0.91960198]\n",
      "0.0421178\n",
      "[ 0.18021493  0.91717583]\n",
      "[ 1.09739077  0.91717583]\n",
      "0.0420664\n",
      "[ 0.1763349   0.91483766]\n",
      "[ 1.09117258  0.91483766]\n",
      "0.0420582\n",
      "[ 0.17250364  0.91277432]\n",
      "[ 1.08527792  0.91277432]\n",
      "0.0420822\n",
      "[ 0.16894056  0.91114402]\n",
      "[ 1.08008456  0.91114402]\n",
      "0.0421206\n",
      "[ 0.16582125  0.91006577]\n",
      "[ 1.07588696  0.91006577]\n",
      "0.0421552\n",
      "[ 0.16326958  0.90961313]\n",
      "[ 1.07288265  0.90961313]\n",
      "0.0421723\n",
      "[ 0.16135396  0.90981108]\n",
      "[ 1.07116508  0.90981108]\n",
      "0.0421658\n",
      "[ 0.16008796  0.91063702]\n",
      "[ 1.07072496  0.91063702]\n",
      "0.0421375\n",
      "[ 0.15943471  0.91202521]\n",
      "[ 1.07145989  0.91202521]\n",
      "0.0420956\n",
      "[ 0.15931469  0.91387391]\n",
      "[ 1.07318854  0.91387391]\n",
      "0.042051\n",
      "[ 0.15961599  0.9160549 ]\n",
      "[ 1.07567096  0.9160549 ]\n",
      "0.0420141\n",
      "[ 0.1602062   0.91842449]\n",
      "[ 1.07863069  0.91842449]\n",
      "0.0419916\n",
      "[ 0.1609448   0.92083472]\n",
      "[ 1.08177948  0.92083472]\n",
      "0.0419854\n",
      "[ 0.16169527  0.92314428]\n",
      "[ 1.08483958  0.92314428]\n",
      "0.0419927\n",
      "[ 0.16233584  0.92522842]\n",
      "[ 1.08756423  0.92522842]\n",
      "0.0420078\n",
      "[ 0.16276824  0.92698663]\n",
      "[ 1.08975482  0.92698663]\n",
      "0.0420238\n",
      "[ 0.16292392  0.92834824]\n",
      "[ 1.09127212  0.92834824]\n",
      "0.0420349\n",
      "[ 0.16276738  0.92927539]\n",
      "[ 1.0920428   0.92927539]\n",
      "0.0420381\n",
      "[ 0.16229658  0.9297632 ]\n",
      "[ 1.09205973  0.9297632 ]\n",
      "0.0420332\n",
      "[ 0.16154063  0.92983747]\n",
      "[ 1.09137809  0.92983747]\n",
      "0.0420226\n",
      "[ 0.16055501  0.92955029]\n",
      "[ 1.0901053   0.92955029]\n",
      "0.0420101\n",
      "[ 0.15941501  0.92897385]\n",
      "[ 1.08838892  0.92897385]\n",
      "0.0419992\n",
      "[ 0.15820803  0.92819315]\n",
      "[ 1.08640122  0.92819315]\n",
      "0.0419926\n",
      "[ 0.15702529  0.92729849]\n",
      "[ 1.08432376  0.92729849]\n",
      "0.0419911\n",
      "[ 0.15595391  0.92637777]\n",
      "[ 1.08233166  0.92637777]\n",
      "0.0419937\n",
      "[ 0.15506969  0.92551005]\n",
      "[ 1.08057976  0.92551005]\n",
      "0.0419985\n",
      "[ 0.15443145  0.92475992]\n",
      "[ 1.07919133  0.92475992]\n",
      "0.0420032\n",
      "[ 0.15407705  0.92417389]\n",
      "[ 1.07825089  0.92417389]\n",
      "0.0420057\n",
      "[ 0.15402147  0.92377818]\n",
      "[ 1.07779968  0.92377818]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.15402147,  0.92377818], dtype=float32),\n",
       " array([ 1.07779968,  0.92377818], dtype=float32))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{\"ent_premise_China\":1, \"ent_hyp_China\":1},{\"ent_premise_China\":1, \"ent_hyp_China\":0}]\n",
    "targets = {\"ent_premise_China\":1, \"ent_hyp_China\":1}\n",
    "\n",
    "estimate_correction_weights(data, targets, reg_lambda=0.1, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_vector(instance:Mapping[str,float],vocab):\n",
    "    vector = np.ndarray(len(vocab))\n",
    "    for key, value in instance.items():\n",
    "        vector[vocab[key]] = value\n",
    "    return vector\n",
    "\n",
    "def to_sparse_tensors(data:Sequence[Mapping[str,float]], targets:Mapping[str,float]):\n",
    "    \"\"\"\n",
    "    Turns dictionaries of instance features and target features into numpy arrays.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "\n",
    "        \n",
    "    for instance in data:\n",
    "        for key,value in instance.items():\n",
    "            if key not in vocab:\n",
    "                vocab[key] = len(vocab)\n",
    "                \n",
    "    data_vectors = []\n",
    "    data_indices = []\n",
    "    data_values = []\n",
    "    for instance_nr,instance in enumerate(data):\n",
    "        for key, value in instance.items():\n",
    "            data_indices.append((instance_nr,vocab[key]))\n",
    "            data_values.append(value)\n",
    "        \n",
    "    data_matrix = tf.SparseTensorValue(data_indices, data_values, [len(data),len(vocab)])\n",
    "    \n",
    "    target_vector = to_vector(targets,vocab)\n",
    "\n",
    "    print(data_matrix)\n",
    "    \n",
    "    return data_matrix, target_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_correction_weights_sparse(data:Sequence[Mapping[str,float]], \n",
    "                                       targets:Mapping[str,float], \n",
    "                                       reg_lambda = 0.0, debug=False,\n",
    "                                       max_iterations=100):\n",
    "    \"\"\"\n",
    "    Calculates a sequence of instance weights such that the total sum of their features\n",
    "    equals/is close to the target vector.\n",
    "    Args:\n",
    "        data: list of feature vectors in sparse dictionary format\n",
    "        targets: feature vector with target total counts\n",
    "    Returns:\n",
    "        `instance_weights`, `total` where instance_weights is a list of weights corresponding\n",
    "        to the instances in `data` and `total` is the total population vector approximation.\n",
    "    \"\"\"\n",
    "    data_matrix, target_vector = to_sparse_tensors(data,targets)\n",
    "    \n",
    "    instance_weights = tf.Variable(initial_value=tf.zeros([len(data),1]))\n",
    "    data_placeholder = tf.sparse_placeholder(tf.float32)\n",
    "    target_placeholder = tf.placeholder(tf.float32, shape=target_vector.shape)\n",
    "    total = tf.sparse_tensor_dense_matmul(data_placeholder,instance_weights,adjoint_a=True)\n",
    "    target_loss = tf.nn.l2_loss(total - target_placeholder)\n",
    "    regularizer = tf.nn.l2_loss(instance_weights - 1)\n",
    "    total_loss = target_loss + reg_lambda * regularizer\n",
    "\n",
    "    sess = tf.Session()\n",
    "    optimizer = tf.train.AdamOptimizer(0.1)\n",
    "    opt_op = optimizer.minimize(total_loss)\n",
    "    project_nonnegative = tf.assign(instance_weights, tf.maximum(0.0,instance_weights))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(0,max_iterations):\n",
    "        feed_dict = {data_placeholder:data_matrix, \n",
    "              target_placeholder:target_vector}\n",
    "        sess.run(opt_op,feed_dict)\n",
    "        sess.run(project_nonnegative)\n",
    "        result = sess.run({'total':total,\n",
    "              'loss':total_loss, \n",
    "              'regularizer':regularizer, \n",
    "              'weights':instance_weights,\n",
    "              'target':target_placeholder},feed_dict)\n",
    "        if debug:\n",
    "            print(result['loss'])\n",
    "            print(result['weights'])\n",
    "            print(result['total'])\n",
    "    return result['weights'][:,0],result['total'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensorValue(indices=[(0, 0), (0, 1), (1, 0), (1, 1)], values=[1.0, 1.0, 1.0, 1.0], dense_shape=[2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.5,  0.5], dtype=float32), array([ 1.,  1.], dtype=float32))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{\"ent_premise_China\":1.0, \"ent_hyp_China\":1.0},\n",
    "        {\"ent_premise_China\":1.0, \"ent_hyp_China\":1.0}]\n",
    "targets = {\"ent_premise_China\":1.0, \"ent_hyp_China\":1.0}\n",
    "\n",
    "estimate_correction_weights_sparse(data, targets, reg_lambda=0.0,debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
